{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from sqlalchemy.orm import Session\n",
    "from typing import List\n",
    "from model import models, schemas\n",
    "from utils import database\n",
    "import math\n",
    "\n",
    "# Database dependency\n",
    "def get_db():\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "\n",
    "def join_restaurant_avis_date(db: Session) -> pd.DataFrame:\n",
    "    # Join the DimRestaurant, FactAvis, and DimDate and select the fields\n",
    "    query = db.query(models.FaitAvis, models.DimRestaurant, models.DimDate)\\\n",
    "              .join(models.DimRestaurant, models.FaitAvis.id_restaurant == models.DimRestaurant.id_restaurant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La distance entre Paris et Londres est d'environ 343.56 km.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calcule la distance entre deux points géographiques en utilisant la formule de Haversine.\n",
    "\n",
    "    :param lat1: Latitude du premier point (en degrés).\n",
    "    :param lon1: Longitude du premier point (en degrés).\n",
    "    :param lat2: Latitude du deuxième point (en degrés).\n",
    "    :param lon2: Longitude du deuxième point (en degrés).\n",
    "    :return: Distance entre les deux points (en kilomètres).\n",
    "    \"\"\"\n",
    "    # Rayon de la Terre en kilomètres\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convertir les degrés en radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "\n",
    "    # Différences de latitude et de longitude\n",
    "    delta_lat = lat2_rad - lat1_rad\n",
    "    delta_lon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Formule de Haversine\n",
    "    a = math.sin(delta_lat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "# Exemple d'utilisation\n",
    "lat1, lon1 = 48.8566, 2.3522  # Paris\n",
    "lat2, lon2 = 51.5074, -0.1278  # Londres\n",
    "\n",
    "distance = haversine(lat1, lon1, lat2, lon2)\n",
    "print(f\"La distance entre Paris et Londres est d'environ {distance:.2f} km.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similarité entre les deux textes est : 0.88\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Charger un modèle pour générer des embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Petit modèle rapide et efficace\n",
    "\n",
    "def calculate_similarity(text1: str, text2: str) -> float:\n",
    "    # Générer les embeddings pour les deux textes\n",
    "    embedding1 = model.encode(text1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(text2, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculer la similarité cosinus\n",
    "    similarity = util.cos_sim(embedding1, embedding2)\n",
    "    \n",
    "    return similarity.item()  # Retourne un score de similarité (0 à 1)\n",
    "\n",
    "# Champs texte\n",
    "# text1 = input(\"Entrez le premier texte : \")\n",
    "# text2 = input(\"Entrez le deuxième texte : \")\n",
    "text1 = \"Bonjour, je suis un texte de test je veux  1  franx\"\n",
    "text2 = \"Bonjour, je suis un autre texte de test\"\n",
    "# Calcul de la similarité\n",
    "similarity_score = calculate_similarity(text1, text2)\n",
    "\n",
    "print(f\"La similarité entre les deux textes est : {similarity_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from sqlalchemy.orm import Session\n",
    "from typing import List\n",
    "from model import models, schemas\n",
    "from utils import database\n",
    "import math\n",
    "\n",
    "# Database dependency\n",
    "def get_db():\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "\n",
    "def join_restaurant_avis_date(db: Session) -> pd.DataFrame:\n",
    "    # Join the DimRestaurant, FactAvis, and DimDate and select the fields\n",
    "    query = db.query(models.FaitAvis, models.DimRestaurant, models.DimDate)\\\n",
    "              .join(models.DimRestaurant, models.FaitAvis.id_restaurant == models.DimRestaurant.id_restaurant)\n",
    "              \n",
    "\n",
    "    # Execute the query\n",
    "    results = query.all()\n",
    "    \n",
    "    # Create a DataFrame with the results\n",
    "    df = pd.DataFrame([{\n",
    "        \"restaurant\": restaurant.nom,\n",
    "        \"type_cuisines\": restaurant.type_cuisines,\n",
    "        \"fonctionnalites\": restaurant.fonctionnalites,\n",
    "        \"infos_pratiques\": restaurant.infos_pratiques,\n",
    "        \"classement\": restaurant.classement,\n",
    "        \n",
    "    } for avis, restaurant, date in results])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "# List of stopwords\n",
    "mots_vides = stopwords.words(\"french\")\n",
    "\n",
    "# List of punctuations and digits\n",
    "ponctuations = string.punctuation\n",
    "chiffres = string.digits\n",
    "\n",
    "# Function to clean and tokenize document\n",
    "def nettoyage_doc(doc_param):\n",
    "    # Convert to lowercase\n",
    "    doc = doc_param.lower()\n",
    "    # Remove punctuations\n",
    "    doc = \"\".join([w for w in list(doc) if not w in ponctuations])\n",
    "    # Remove digits\n",
    "    # doc = \"\".join([w for w in list(doc) if not w in chiffres])\n",
    "    # Tokenize the document\n",
    "    doc = word_tokenize(doc)\n",
    "    # Lemmatize each term\n",
    "    doc = [lem.lemmatize(terme) for terme in doc]\n",
    "    # Remove stopwords\n",
    "    doc = [w for w in doc if not w in mots_vides]\n",
    "    # Remove terms with less than 3 characters if they are not numeric\n",
    "    doc = [w for w in doc if len(w) >= 3 or w.isnumeric()]\n",
    "    return doc\n",
    "\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calcule la distance entre deux points géographiques en utilisant la formule de Haversine.\n",
    "\n",
    "    :param lat1: Latitude du premier point (en degrés).\n",
    "    :param lon1: Longitude du premier point (en degrés).\n",
    "    :param lat2: Latitude du deuxième point (en degrés).\n",
    "    :param lon2: Longitude du deuxième point (en degrés).\n",
    "    :return: Distance entre les deux points (en kilomètres).\n",
    "    \"\"\"\n",
    "    # Rayon de la Terre en kilomètres\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convertir les degrés en radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "\n",
    "    # Différences de latitude et de longitude\n",
    "    delta_lat = lat2_rad - lat1_rad\n",
    "    delta_lon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Formule de Haversine\n",
    "    a = math.sin(delta_lat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "def calculate_similarity_notes(note1: float, note2: float) -> float:\n",
    "    # Calculate the absolute difference between the two notes\n",
    "    diff = abs(note1 - note2)\n",
    "    \n",
    "    # Normalize the difference to a similarity score (0 to 1)\n",
    "    similarity = 1 / (1 + diff)\n",
    "    return similarity\n",
    "\n",
    "def calculate_similarity_texte(text1: str, text2: str) -> float:\n",
    "    # Clean and tokenize the texts\n",
    "    doc1 = nettoyage_doc(text1)\n",
    "    doc2 = nettoyage_doc(text2)\n",
    "    \n",
    "    # Join tokens back to strings\n",
    "    text1 = \" \".join(doc1)\n",
    "    text2 = \" \".join(doc2)\n",
    "    \n",
    "    # print(f\"Texte 1 nettoyé : {text1}\")\n",
    "    # print(f\"Texte 2 nettoyé : {text2}\")\n",
    "\n",
    "    # Create TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    return similarity[0][0]  # Return the similarity score (0 to 1)\n",
    "\n",
    "\n",
    "\n",
    "def build_similarity_matrix(data , method) -> pd.DataFrame:\n",
    "    # Initialize an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through all pairs of texts\n",
    "    for i in range(len(data)):\n",
    "        for j in range(i+1, len(data)):\n",
    "            # Calculate similarity between the two texts\n",
    "            if method == \"texte\":\n",
    "                similarity = calculate_similarity_texte(data[i], data[j])\n",
    "            elif method == \"notes\":\n",
    "                similarity = calculate_similarity_notes(data[i], data[j])\n",
    "            elif method == \"geographique\":\n",
    "                similarity = haversine(data[i][0], data[i][1], data[j][0], data[j][1])\n",
    "            # Add the similarity to the DataFrame\n",
    "            df = df.append({\n",
    "                \"restaurant1\": data[i],\n",
    "                \"restaurant2\": data[j],\n",
    "                \"similarity\": similarity\n",
    "            }, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# def construireMesMatrix(db):\n",
    "#     data = join_restaurant_avis_date( db)\n",
    "#     # print(data)\n",
    "#     return data\n",
    "\n",
    "# #cconstruction  de la matrice de similarité\n",
    "# def build_similarity_matrix(texts: List[str]) -> pd.DataFrame:\n",
    "#     # Initialize an empty DataFrame\n",
    "#     df = pd.DataFrame()\n",
    "    \n",
    "#     # Loop through all pairs of texts\n",
    "#     for i in range(len(texts)):\n",
    "#         for j in range(i+1, len(texts)):\n",
    "#             # Calculate similarity between the two texts\n",
    "#             similarity = calculate_similarity(texts[i], texts[j])\n",
    "            \n",
    "#             # Add the similarity to the DataFrame\n",
    "#             df = df.append({\n",
    "#                 \"text1\": texts[i],\n",
    "#                 \"text2\": texts[j],\n",
    "#                 \"similarity\": similarity\n",
    "#             }, ignore_index=True)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# Example texts\n",
    "# text1 = \"Bonjour, je suis un texte de test\"\n",
    "# text2 = \"Bonjour, je suis un autre texte de test\"\n",
    "\n",
    "\n",
    "\n",
    "# Calculate similarity\n",
    "# similarity_score = calculate_similarity(text1, text2)\n",
    "# print(f\"La similarité entre les deux textes est : {similarity_score:.2f}\")\n",
    "# db = next(get_db())\n",
    "# contruireDF = join_restaurant_avis_date(db)\n",
    "# print(contruireDF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte 1 nettoyé : bonjour texte test\n",
      "Texte 2 nettoyé : bonjour autre texte test\n",
      "La similarité entre les deux textes est : 0.78\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from sqlalchemy.orm import Session\n",
    "from typing import List\n",
    "from model import models, schemas\n",
    "from utils import database\n",
    "\n",
    "\n",
    "# Database dependency\n",
    "def get_db():\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "def join_restaurant_avis_date(db: Session) -> pd.DataFrame:\n",
    "    # Join the DimRestaurant and FactAvis and data and select the fields\n",
    "    query = db.query(models.FactAvis, models.DimRestaurant).join(models.DimRestaurant)\n",
    "\n",
    "    # Execute the query\n",
    "    \n",
    "    \n",
    "    results = query.all()\n",
    "    \n",
    "    # Create a DataFrame with the results\n",
    "    df = pd.DataFrame([{\n",
    "        \"avis\": avis.commentaire,\n",
    "        \"date\": avis.date,\n",
    "        \"restaurant\": restaurant.nom\n",
    "    } for avis, restaurant in results])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "# List of stopwords\n",
    "mots_vides = stopwords.words(\"french\")\n",
    "\n",
    "# List of punctuations and digits\n",
    "ponctuations = string.punctuation\n",
    "chiffres = string.digits\n",
    "\n",
    "# Function to clean and tokenize document\n",
    "def nettoyage_doc(doc_param):\n",
    "    # Convert to lowercase\n",
    "    doc = doc_param.lower()\n",
    "    # Remove punctuations\n",
    "    doc = \"\".join([w for w in list(doc) if not w in ponctuations])\n",
    "    # Remove digits\n",
    "    # doc = \"\".join([w for w in list(doc) if not w in chiffres])\n",
    "    # Tokenize the document\n",
    "    doc = word_tokenize(doc)\n",
    "    # Lemmatize each term\n",
    "    doc = [lem.lemmatize(terme) for terme in doc]\n",
    "    # Remove stopwords\n",
    "    doc = [w for w in doc if not w in mots_vides]\n",
    "    # Remove terms with less than 3 characters if they are not numeric\n",
    "    doc = [w for w in doc if len(w) >= 3 or w.isnumeric()]\n",
    "    return doc\n",
    "\n",
    "def calculate_similarity(text1: str, text2: str) -> float:\n",
    "    # Clean and tokenize the texts\n",
    "    doc1 = nettoyage_doc(text1)\n",
    "    doc2 = nettoyage_doc(text2)\n",
    "    \n",
    "    # Join tokens back to strings\n",
    "    text1 = \" \".join(doc1)\n",
    "    text2 = \" \".join(doc2)\n",
    "    \n",
    "    print(f\"Texte 1 nettoyé : {text1}\")\n",
    "    print(f\"Texte 2 nettoyé : {text2}\")\n",
    "\n",
    "    # Create TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    return similarity[0][0]  # Return the similarity score (0 to 1)\n",
    "\n",
    "# Example texts\n",
    "text1 = \"Bonjour, je suis un texte de test\"\n",
    "text2 = \"Bonjour, je suis un autre texte de test\"\n",
    "\n",
    "# Calculate similarity\n",
    "similarity_score = calculate_similarity(text1, text2)\n",
    "\n",
    "print(f\"La similarité entre les deux textes est : {similarity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
