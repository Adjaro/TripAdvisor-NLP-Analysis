{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import fitz\n",
    "import chromadb\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from chromadb.config import Settings\n",
    "\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "\n",
    "class BDDChunks:\n",
    "    \"\"\"\n",
    "    A class to handle operations related to chunking text data, embedding, and storing in a ChromaDB instance.\n",
    "\n",
    "    This class provides methods to:\n",
    "    - Read text from PDF files.\n",
    "    - Split the text into smaller chunks for processing.\n",
    "    - Create a ChromaDB collection with embeddings for the chunks.\n",
    "    - Add these chunks and their embeddings to the ChromaDB collection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nom: str, embedding_model: str, db:):\n",
    "        \"\"\"\n",
    "        Initialize a BDDChunks instance.\n",
    "\n",
    "        Args:\n",
    "            embedding_model (str): The name of the embedding model to use for generating embeddings.\n",
    "            path (str): The file path to the PDF or dataset to process.\n",
    "        \"\"\"\n",
    "        self.nom\n",
    "        self.path = path\n",
    "        self.chunks: list[str] | None = None\n",
    "        self.client = chromadb.PersistentClient(\n",
    "            path=\"./ChromaDB\", settings=Settings(anonymized_telemetry=False)\n",
    "        )\n",
    "        self.embedding_name = embedding_model\n",
    "        self.embeddings = SentenceTransformerEmbeddingFunction(\n",
    "            model_name=embedding_model\n",
    "        )\n",
    "        self.chroma_db = None\n",
    "\n",
    "    def _create_collection(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Create a new ChromaDB collection for storing embeddings.\n",
    "\n",
    "        Args:\n",
    "            path (str): The name of the collection to create in ChromaDB.\n",
    "        \"\"\"\n",
    "        # Tester qu'en changeant de path, on accède pas au reste\n",
    "        file_name = \"a\" + os.path.basename(path)[0:50].strip() + \"a\"\n",
    "        file_name = re.sub(r\"\\s+\", \"-\", file_name)\n",
    "        # Expected collection name that (1) contains 3-63 characters, (2) starts and ends with an alphanumeric character, (3) otherwise contains only alphanumeric characters, underscores or hyphens (-), (4) contains no two consecutive periods (..)\n",
    "        self.chroma_db = self.client.get_or_create_collection(name=file_name, embedding_function=self.embeddings, metadata={\"hnsw:space\": \"cosine\"})  # type: ignore\n",
    "\n",
    "    def read_pdf(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Reads the content of a PDF file, excluding the specified number of pages from the start and end.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the PDF file.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted text from the specified pages of the PDF.\n",
    "        \"\"\"\n",
    "        doc = fitz.open(file_path)\n",
    "        text = str()\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            text += page.get_text()  # type: ignore\n",
    "        return text  # type: ignore\n",
    "\n",
    "    def split_text_into_chunks(self, corpus: str, chunk_size: int = 500) -> list[str]:\n",
    "        \"\"\"\n",
    "        Splits a given text corpus into chunks of a specified size.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): The input text corpus to be split into chunks.\n",
    "            chunk_size (int, optional): The size of each chunk. Defaults to 500.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of text chunks.\n",
    "        \"\"\"\n",
    "        tokenized_corpus = enc.encode(corpus)\n",
    "        chunks = [\n",
    "            \"\".join(enc.decode(tokenized_corpus[i : i + chunk_size]))\n",
    "            for i in tqdm(range(0, len(tokenized_corpus), chunk_size))\n",
    "        ]\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def add_embeddings(self, list_chunks: list[str], batch_size: int = 100) -> None:\n",
    "        \"\"\"\n",
    "        Add embeddings for text chunks to the ChromaDB collection.\n",
    "\n",
    "        Args:\n",
    "            list_chunks (list[str]): A list of text chunks to embed and add to the collection.\n",
    "            batch_size (int, optional): The batch size for adding documents to the collection. Defaults to 100.\n",
    "\n",
    "        Note:\n",
    "            ChromaDB supports a maximum of 166 documents per batch.\n",
    "        \"\"\"\n",
    "        if len(list_chunks) < batch_size:\n",
    "            batch_size_for_chromadb = len(list_chunks)\n",
    "        else:\n",
    "            batch_size_for_chromadb = batch_size\n",
    "\n",
    "        document_ids: list[str] = []\n",
    "\n",
    "        for i in tqdm(\n",
    "            range(0, len(list_chunks), batch_size_for_chromadb)\n",
    "        ):  # On met en place une stratégie d'ajout par batch car ChromaDB ne supporte pas plus de 166 documents d'un coup.\n",
    "            batch_documents = list_chunks[i : i + batch_size_for_chromadb]\n",
    "            list_ids = [\n",
    "                str(id_chunk) for id_chunk in list(range(i, i + len(batch_documents)))\n",
    "            ]\n",
    "            list_id_doc = [str(uuid.uuid4()) for x in list_ids]\n",
    "            self.chroma_db.add(documents=batch_documents, ids=list_id_doc)  # type: ignore\n",
    "            document_ids.extend(list_ids)\n",
    "\n",
    "    def __call__(self) -> None:\n",
    "        \"\"\"\n",
    "        Execute the entire process of reading, chunking, creating a collection, and adding embeddings.\n",
    "\n",
    "        This method:\n",
    "        1. Reads the text from the specified PDF file.\n",
    "        2. Splits the text into chunks.\n",
    "        3. Creates a ChromaDB collection for storing the embeddings.\n",
    "        4. Adds the text chunks and their embeddings to the ChromaDB collection.\n",
    "        \"\"\"\n",
    "        corpus = self.read_pdf(file_path=self.path)\n",
    "        chunks = self.split_text_into_chunks(corpus=corpus)\n",
    "        self._create_collection(path=self.path)\n",
    "        self.add_embeddings(list_chunks=chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from sqlalchemy.orm import Session\n",
    "from typing import List\n",
    "from model import models, schemas\n",
    "from utils import database\n",
    "import math\n",
    "\n",
    "# Database dependency\n",
    "def get_db():\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "\n",
    "def join_restaurant_avis_date(db: Session) -> pd.DataFrame:\n",
    "    # Join the DimRestaurant, FactAvis, and DimDate and select the fields\n",
    "    query = db.query(models.FaitAvis, models.DimRestaurant, models.DimDate)\\\n",
    "              .join(models.DimRestaurant, models.FaitAvis.id_restaurant == models.DimRestaurant.id_restaurant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La distance entre Paris et Londres est d'environ 343.56 km.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calcule la distance entre deux points géographiques en utilisant la formule de Haversine.\n",
    "\n",
    "    :param lat1: Latitude du premier point (en degrés).\n",
    "    :param lon1: Longitude du premier point (en degrés).\n",
    "    :param lat2: Latitude du deuxième point (en degrés).\n",
    "    :param lon2: Longitude du deuxième point (en degrés).\n",
    "    :return: Distance entre les deux points (en kilomètres).\n",
    "    \"\"\"\n",
    "    # Rayon de la Terre en kilomètres\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convertir les degrés en radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "\n",
    "    # Différences de latitude et de longitude\n",
    "    delta_lat = lat2_rad - lat1_rad\n",
    "    delta_lon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Formule de Haversine\n",
    "    a = math.sin(delta_lat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "# Exemple d'utilisation\n",
    "lat1, lon1 = 48.8566, 2.3522  # Paris\n",
    "lat2, lon2 = 51.5074, -0.1278  # Londres\n",
    "\n",
    "distance = haversine(lat1, lon1, lat2, lon2)\n",
    "print(f\"La distance entre Paris et Londres est d'environ {distance:.2f} km.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similarité entre les deux textes est : 0.88\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Charger un modèle pour générer des embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Petit modèle rapide et efficace\n",
    "\n",
    "def calculate_similarity(text1: str, text2: str) -> float:\n",
    "    # Générer les embeddings pour les deux textes\n",
    "    embedding1 = model.encode(text1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(text2, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculer la similarité cosinus\n",
    "    similarity = util.cos_sim(embedding1, embedding2)\n",
    "    \n",
    "    return similarity.item()  # Retourne un score de similarité (0 à 1)\n",
    "\n",
    "# Champs texte\n",
    "# text1 = input(\"Entrez le premier texte : \")\n",
    "# text2 = input(\"Entrez le deuxième texte : \")\n",
    "text1 = \"Bonjour, je suis un texte de test je veux  1  franx\"\n",
    "text2 = \"Bonjour, je suis un autre texte de test\"\n",
    "# Calcul de la similarité\n",
    "similarity_score = calculate_similarity(text1, text2)\n",
    "\n",
    "print(f\"La similarité entre les deux textes est : {similarity_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from sqlalchemy.orm import Session\n",
    "from typing import List\n",
    "from model import models, schemas\n",
    "from utils import database\n",
    "import math\n",
    "\n",
    "# Database dependency\n",
    "def get_db():\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "\n",
    "def join_restaurant_avis_date(db: Session) -> pd.DataFrame:\n",
    "    # Join the DimRestaurant, FactAvis, and DimDate and select the fields\n",
    "    query = db.query(models.FaitAvis, models.DimRestaurant, models.DimDate)\\\n",
    "              .join(models.DimRestaurant, models.FaitAvis.id_restaurant == models.DimRestaurant.id_restaurant)\n",
    "              \n",
    "\n",
    "    # Execute the query\n",
    "    results = query.all()\n",
    "    \n",
    "    # Create a DataFrame with the results\n",
    "    df = pd.DataFrame([{\n",
    "        \"restaurant\": restaurant.nom,\n",
    "        \"type_cuisines\": restaurant.type_cuisines,\n",
    "        \"fonctionnalites\": restaurant.fonctionnalites,\n",
    "        \"infos_pratiques\": restaurant.infos_pratiques,\n",
    "        \"classement\": restaurant.classement,\n",
    "        \n",
    "    } for avis, restaurant, date in results])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "# List of stopwords\n",
    "mots_vides = stopwords.words(\"french\")\n",
    "\n",
    "# List of punctuations and digits\n",
    "ponctuations = string.punctuation\n",
    "chiffres = string.digits\n",
    "\n",
    "# Function to clean and tokenize document\n",
    "def nettoyage_doc(doc_param):\n",
    "    # Convert to lowercase\n",
    "    doc = doc_param.lower()\n",
    "    # Remove punctuations\n",
    "    doc = \"\".join([w for w in list(doc) if not w in ponctuations])\n",
    "    # Remove digits\n",
    "    # doc = \"\".join([w for w in list(doc) if not w in chiffres])\n",
    "    # Tokenize the document\n",
    "    doc = word_tokenize(doc)\n",
    "    # Lemmatize each term\n",
    "    doc = [lem.lemmatize(terme) for terme in doc]\n",
    "    # Remove stopwords\n",
    "    doc = [w for w in doc if not w in mots_vides]\n",
    "    # Remove terms with less than 3 characters if they are not numeric\n",
    "    doc = [w for w in doc if len(w) >= 3 or w.isnumeric()]\n",
    "    return doc\n",
    "\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calcule la distance entre deux points géographiques en utilisant la formule de Haversine.\n",
    "\n",
    "    :param lat1: Latitude du premier point (en degrés).\n",
    "    :param lon1: Longitude du premier point (en degrés).\n",
    "    :param lat2: Latitude du deuxième point (en degrés).\n",
    "    :param lon2: Longitude du deuxième point (en degrés).\n",
    "    :return: Distance entre les deux points (en kilomètres).\n",
    "    \"\"\"\n",
    "    # Rayon de la Terre en kilomètres\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convertir les degrés en radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "\n",
    "    # Différences de latitude et de longitude\n",
    "    delta_lat = lat2_rad - lat1_rad\n",
    "    delta_lon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Formule de Haversine\n",
    "    a = math.sin(delta_lat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "def calculate_similarity_notes(note1: float, note2: float) -> float:\n",
    "    # Calculate the absolute difference between the two notes\n",
    "    diff = abs(note1 - note2)\n",
    "    \n",
    "    # Normalize the difference to a similarity score (0 to 1)\n",
    "    similarity = 1 / (1 + diff)\n",
    "    return similarity\n",
    "\n",
    "def calculate_similarity_texte(text1: str, text2: str) -> float:\n",
    "    # Clean and tokenize the texts\n",
    "    doc1 = nettoyage_doc(text1)\n",
    "    doc2 = nettoyage_doc(text2)\n",
    "    \n",
    "    # Join tokens back to strings\n",
    "    text1 = \" \".join(doc1)\n",
    "    text2 = \" \".join(doc2)\n",
    "    \n",
    "    # print(f\"Texte 1 nettoyé : {text1}\")\n",
    "    # print(f\"Texte 2 nettoyé : {text2}\")\n",
    "\n",
    "    # Create TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    return similarity[0][0]  # Return the similarity score (0 to 1)\n",
    "\n",
    "\n",
    "\n",
    "def build_similarity_matrix(data , method) -> pd.DataFrame:\n",
    "    # Initialize an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through all pairs of texts\n",
    "    for i in range(len(data)):\n",
    "        for j in range(i+1, len(data)):\n",
    "            # Calculate similarity between the two texts\n",
    "            if method == \"texte\":\n",
    "                similarity = calculate_similarity_texte(data[i], data[j])\n",
    "            elif method == \"notes\":\n",
    "                similarity = calculate_similarity_notes(data[i], data[j])\n",
    "            elif method == \"geographique\":\n",
    "                similarity = haversine(data[i][0], data[i][1], data[j][0], data[j][1])\n",
    "            # Add the similarity to the DataFrame\n",
    "            df = df.append({\n",
    "                \"restaurant1\": data[i],\n",
    "                \"restaurant2\": data[j],\n",
    "                \"similarity\": similarity\n",
    "            }, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# def construireMesMatrix(db):\n",
    "#     data = join_restaurant_avis_date( db)\n",
    "#     # print(data)\n",
    "#     return data\n",
    "\n",
    "# #cconstruction  de la matrice de similarité\n",
    "# def build_similarity_matrix(texts: List[str]) -> pd.DataFrame:\n",
    "#     # Initialize an empty DataFrame\n",
    "#     df = pd.DataFrame()\n",
    "    \n",
    "#     # Loop through all pairs of texts\n",
    "#     for i in range(len(texts)):\n",
    "#         for j in range(i+1, len(texts)):\n",
    "#             # Calculate similarity between the two texts\n",
    "#             similarity = calculate_similarity(texts[i], texts[j])\n",
    "            \n",
    "#             # Add the similarity to the DataFrame\n",
    "#             df = df.append({\n",
    "#                 \"text1\": texts[i],\n",
    "#                 \"text2\": texts[j],\n",
    "#                 \"similarity\": similarity\n",
    "#             }, ignore_index=True)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# Example texts\n",
    "# text1 = \"Bonjour, je suis un texte de test\"\n",
    "# text2 = \"Bonjour, je suis un autre texte de test\"\n",
    "\n",
    "\n",
    "\n",
    "# Calculate similarity\n",
    "# similarity_score = calculate_similarity(text1, text2)\n",
    "# print(f\"La similarité entre les deux textes est : {similarity_score:.2f}\")\n",
    "# db = next(get_db())\n",
    "# contruireDF = join_restaurant_avis_date(db)\n",
    "# print(contruireDF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte 1 nettoyé : bonjour texte test\n",
      "Texte 2 nettoyé : bonjour autre texte test\n",
      "La similarité entre les deux textes est : 0.78\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from sqlalchemy.orm import Session\n",
    "from typing import List\n",
    "from model import models, schemas\n",
    "from utils import database\n",
    "\n",
    "\n",
    "# Database dependency\n",
    "def get_db():\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "def join_restaurant_avis_date(db: Session) -> pd.DataFrame:\n",
    "    # Join the DimRestaurant and FactAvis and data and select the fields\n",
    "    query = db.query(models.FactAvis, models.DimRestaurant).join(models.DimRestaurant)\n",
    "\n",
    "    # Execute the query\n",
    "    \n",
    "    \n",
    "    results = query.all()\n",
    "    \n",
    "    # Create a DataFrame with the results\n",
    "    df = pd.DataFrame([{\n",
    "        \"avis\": avis.commentaire,\n",
    "        \"date\": avis.date,\n",
    "        \"restaurant\": restaurant.nom\n",
    "    } for avis, restaurant in results])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "# List of stopwords\n",
    "mots_vides = stopwords.words(\"french\")\n",
    "\n",
    "# List of punctuations and digits\n",
    "ponctuations = string.punctuation\n",
    "chiffres = string.digits\n",
    "\n",
    "# Function to clean and tokenize document\n",
    "def nettoyage_doc(doc_param):\n",
    "    # Convert to lowercase\n",
    "    doc = doc_param.lower()\n",
    "    # Remove punctuations\n",
    "    doc = \"\".join([w for w in list(doc) if not w in ponctuations])\n",
    "    # Remove digits\n",
    "    # doc = \"\".join([w for w in list(doc) if not w in chiffres])\n",
    "    # Tokenize the document\n",
    "    doc = word_tokenize(doc)\n",
    "    # Lemmatize each term\n",
    "    doc = [lem.lemmatize(terme) for terme in doc]\n",
    "    # Remove stopwords\n",
    "    doc = [w for w in doc if not w in mots_vides]\n",
    "    # Remove terms with less than 3 characters if they are not numeric\n",
    "    doc = [w for w in doc if len(w) >= 3 or w.isnumeric()]\n",
    "    return doc\n",
    "\n",
    "def calculate_similarity(text1: str, text2: str) -> float:\n",
    "    # Clean and tokenize the texts\n",
    "    doc1 = nettoyage_doc(text1)\n",
    "    doc2 = nettoyage_doc(text2)\n",
    "    \n",
    "    # Join tokens back to strings\n",
    "    text1 = \" \".join(doc1)\n",
    "    text2 = \" \".join(doc2)\n",
    "    \n",
    "    print(f\"Texte 1 nettoyé : {text1}\")\n",
    "    print(f\"Texte 2 nettoyé : {text2}\")\n",
    "\n",
    "    # Create TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    return similarity[0][0]  # Return the similarity score (0 to 1)\n",
    "\n",
    "# Example texts\n",
    "text1 = \"Bonjour, je suis un texte de test\"\n",
    "text2 = \"Bonjour, je suis un autre texte de test\"\n",
    "\n",
    "# Calculate similarity\n",
    "similarity_score = calculate_similarity(text1, text2)\n",
    "\n",
    "print(f\"La similarité entre les deux textes est : {similarity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
