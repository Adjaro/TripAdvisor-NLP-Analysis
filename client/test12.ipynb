{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.68it/s]\n",
      "\u001b[92m12:34:57 - LiteLLM:INFO\u001b[0m: utils.py:2802 - \n",
      "LiteLLM completion() model= ministral-8b-latest; provider = mistral\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= ministral-8b-latest; provider = mistral\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m12:34:58 - LiteLLM:INFO\u001b[0m: utils.py:949 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': \"Pour trouver les meilleurs restaurants à Lyon, il est souvent utile de consulter des avis et des recommandations de critiques gastronomiques. Voici quelques suggestions basées sur les avis disponibles :\\n\\n1. **Brasserie Georges** : Bien que certains avis mentionnent qu'il ne s'agit pas d'un haut lieu de la gastronomie lyonnaise, la Brasserie Georges est appréciée pour son service correct, ses prix raisonnables, et ses plats traditionnels comme le pâté en\", 'latency': 0.171875, 'input_tokens': 350, 'output_tokens': 100, 'llm': 'ministral-8b-latest'}\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import  time\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# from .rag_simulation.corpus_ingestion import BDDChunks\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "class AugmentedRAG:\n",
    "    \"\"\"A class for performing a simple RAG process.\n",
    "\n",
    "    This class utilizes a retrieval process to fetch relevant information from a\n",
    "    database (or corpus) and then passes it to a generative model for further processing.\n",
    "\n",
    "    \"\"\"\n",
    "    HF_TOKEN = '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'\n",
    "    MISTRAL_API_KEY =  'dkMKu81kFgJeP7HmIqjztosQTxyiynW6'\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        generation_model: str,\n",
    "        role_prompt: str,\n",
    "        bdd_chunks: BDDChunks,\n",
    "        max_tokens: int,\n",
    "        temperature: int,\n",
    "        top_n: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the SimpleRAG class with the provided parameters.\n",
    "\n",
    "        Args:\n",
    "            generation_model (str): The model used for generating responses.\n",
    "            role_prompt (str): The role of the model as specified by the prompt.\n",
    "            bdd_chunks (Any): The database or chunks of information used in the retrieval process.\n",
    "            max_tokens (int): Maximum number of tokens to generate.\n",
    "            temperature (int): The temperature setting for the generative model.\n",
    "            top_n (int, optional): The number of top documents to retrieve. Defaults to 2.\n",
    "        \"\"\"\n",
    "        self.llm = generation_model\n",
    "        self.bdd = bdd_chunks\n",
    "        self.top_n = top_n\n",
    "        self.role_prompt = role_prompt\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "        self.latency = 0.0\n",
    "        self.input_tokens = 0\n",
    "        self.output_tokens = 0\n",
    "        self.dollor_cost = 0.0\n",
    "\n",
    "    def get_cosim(self, a: NDArray[np.float32], b: NDArray[np.float32]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the cosine similarity between two vectors.\n",
    "\n",
    "        Args:\n",
    "            a (NDArray[np.float32]): The first vector.\n",
    "            b (NDArray[np.float32]): The second vector.\n",
    "\n",
    "        Returns:\n",
    "            float: The cosine similarity between the two vectors.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    def get_top_similarity(\n",
    "            self,\n",
    "            embedding_query: NDArray[np.float32],\n",
    "            embedding_chunks: NDArray[np.float32],\n",
    "            corpus: list[str],\n",
    "        ) -> list[str]:\n",
    "            \"\"\"\n",
    "            Retrieves the top N most similar documents from the corpus based on the query's embedding.\n",
    "\n",
    "            Args:\n",
    "                embedding_query (NDArray[np.float32]): The embedding of the query.\n",
    "                embedding_chunks (NDArray[np.float32]): A NumPy array of embeddings for the documents in the corpus.\n",
    "                corpus (List[str]): A list of documents (strings) corresponding to the embeddings in `embedding_chunks`.\n",
    "                top_n (int, optional): The number of top similar documents to retrieve. Defaults to 5.\n",
    "\n",
    "            Returns:\n",
    "                List[str]: A list of the most similar documents from the corpus, ordered by similarity to the query.\n",
    "            \"\"\"\n",
    "            cos_dist_list = np.array(\n",
    "                [\n",
    "                    self.get_cosim(embedding_query, embed_doc)\n",
    "                    for embed_doc in embedding_chunks\n",
    "                ]\n",
    "            )\n",
    "            indices_of_max_values = np.argsort(cos_dist_list)[-self.top_n :][::-1]\n",
    "            print(indices_of_max_values)\n",
    "            return [corpus[i] for i in indices_of_max_values]\n",
    "\n",
    "\n",
    "    def build_prompt(\n",
    "        self, context: list[str], history: str, query: str\n",
    "    ) -> list[dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Builds a prompt string for a conversational agent based on the given context and query.\n",
    "\n",
    "        Args:\n",
    "            context (str): The context information, typically extracted from books or other sources.\n",
    "            query (str): The user's query or question.\n",
    "\n",
    "        Returns:\n",
    "            list[dict[str, str]]: The RAG prompt in the OpenAI format\n",
    "        \"\"\"\n",
    "        context_joined = \"\\n\".join(context)\n",
    "        system_prompt = self.role_prompt\n",
    "        history_prompt = f\"\"\"\n",
    "        # Historique de conversation:\n",
    "        {history}\n",
    "        \"\"\"\n",
    "        context_prompt = f\"\"\"\n",
    "        Tu disposes de la section \"Contexte\" pour t'aider à répondre aux questions.\n",
    "        # Contexte: \n",
    "        {context_joined}\n",
    "        \"\"\"\n",
    "        query_prompt = f\"\"\"\n",
    "        # Question:\n",
    "        {query}\n",
    "\n",
    "        # Réponse:\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"system\", \"content\": history_prompt},\n",
    "            {\"role\": \"system\", \"content\": context_prompt},\n",
    "            {\"role\": \"user\", \"content\": query_prompt},\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    def _generate(self, prompt_dict: list[dict[str, str]]) -> litellm.ModelResponse:\n",
    "\n",
    "\n",
    "         \n",
    "\n",
    "        response = litellm.completion(\n",
    "            model=f\"mistral/{self.llm}\",\n",
    "            messages=prompt_dict,\n",
    "            max_tokens=self.max_tokens,\n",
    "            temperature=self.temperature,\n",
    "        )  # type: ignore\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "    def call_model(self, prompt_dict: list[dict[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        Calls the LLM with the given prompt and returns the response.\n",
    "\n",
    "        Args:\n",
    "            prompt_dict (List[Dict[str, str]]): A list of dictionaries where each dictionary represents\n",
    "                                                a message prompt with a string key and string value.\n",
    "\n",
    "        Returns:\n",
    "            str: The response generated by the LLM.\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.process_time()\n",
    "        chat_response: str = self._generate(prompt_dict=prompt_dict)\n",
    "        end_time = time.process_time()\n",
    "        self.latency = end_time - start_time\n",
    "\n",
    "        self.input_tokens = chat_response.usage.prompt_tokens\n",
    "        self.output_tokens = chat_response.usage.completion_tokens\n",
    "    \n",
    "\n",
    "        dict_response = {\n",
    "            \"response\": chat_response.choices[0].message.content,\n",
    "            \"latency\": self.latency,\n",
    "            \"input_tokens\": self.input_tokens,\n",
    "            \"output_tokens\": self.output_tokens,\n",
    "            \"llm\": self.llm,           \n",
    "        }\n",
    "        return dict_response\n",
    "        # return str(chat_response.choices[0].message.content)\n",
    "\n",
    "\n",
    "    def __call__(self, query: str, history: dict[str, str]) -> str:\n",
    "        \"\"\"\n",
    "        Process a query and return a response based on the provided history and database.\n",
    "\n",
    "        This method performs the following steps:\n",
    "        1. Queries the ChromaDB instance to retrieve relevant documents based on the input query.\n",
    "        2. Constructs a prompt using the retrieved documents, the provided query, and the history.\n",
    "        3. Sends the prompt to the model for generating a response.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user query to be processed.\n",
    "            history (dict[str, str]): A dictionary containing the conversation history,\n",
    "                where keys represent user inputs and values represent corresponding responses.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response from the model.\n",
    "        \"\"\"\n",
    "        chunks = self.bdd.chroma_db.query(\n",
    "            query_texts=[query],\n",
    "            n_results=self.top_n,\n",
    "        )\n",
    "        chunks_list: list[str] = chunks[\"documents\"][0]\n",
    "        prompt_rag = self.build_prompt(\n",
    "            context=chunks_list, history=str(history), query=query\n",
    "        )\n",
    "        response = self.call_model(prompt_dict=prompt_rag)\n",
    "        return response\n",
    "\n",
    "\n",
    "generation_model = \"ministral-8b-latest\"\n",
    "role_prompt = \"Tu es un assistant virtuel qui aide les utilisateurs à répondre à des questions.\"\n",
    "bdd_chunks = BDDChunks(embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\", path=\"./\")\n",
    "# bdd_chunks = BDDChunks(embedding_model=\"paraphrase-xlm-r-multilingual-v1\", path=\"./\")\n",
    "max_tokens = 100\n",
    "temperature = 0.5\n",
    "\n",
    "# Initialize the SimpleRAG instance\n",
    "simple_rag = AugmentedRAG(\n",
    "    generation_model=generation_model,\n",
    "    role_prompt=role_prompt,\n",
    "    bdd_chunks=bdd_chunks,\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=temperature,\n",
    "\n",
    ")\n",
    "\n",
    "# Define the conversation history\n",
    "history = {\n",
    "    \"user\": \"Quelle est la capitale de la France ?\",\n",
    "    \"bot\": \"La capitale de la France est Paris.\",\n",
    "}\n",
    "\n",
    "# Define the user query\n",
    "query = \" meilleurs  restaurant lyon\"\n",
    "bdd_chunks._create_collection(path=\"./\")\n",
    "\n",
    "# Generate a response using the SimpleRAG instance\n",
    "response = simple_rag(query=query, history=history)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\3011249731.py:135: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\3011249731.py:135: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\3011249731.py:136: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\3011249731.py:136: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\3011249731.py:137: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\3011249731.py:137: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 14.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.67it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.56it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.75it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.62it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.40it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.60it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.58it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.22it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.49it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.02it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.55it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.46it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.54it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.79it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.34it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.59it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.28it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.52it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.17it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.06it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.68it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.90it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.50it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.00it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.43it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.51it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.69it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.65it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  9.61it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.07it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.89it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.53it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.42it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.94it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.66it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.80it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.10it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.73it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.61it/s]\n",
      "100%|██████████| 1518/1518 [04:22<00:00,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from chromadb.config import Settings\n",
    "import logging\n",
    "\n",
    "# Initialize tiktoken encoding\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "class BDDChunks:\n",
    "    \"\"\"\n",
    "    A class to process restaurant data and reviews from a SQLite database,\n",
    "    transforming them into consolidated chunks with embeddings stored in ChromaDB.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str, path: str):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunks instance.\n",
    "\n",
    "        Args:\n",
    "            embedding_model (str): Name of the embedding model to use.\n",
    "            path (str): Path to the dataset or collection.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.client = chromadb.PersistentClient(\n",
    "            path=\"./ChromaDB11\", settings=Settings(anonymized_telemetry=False)\n",
    "        )\n",
    "        self.embedding_name = embedding_model\n",
    "        self.embeddings = SentenceTransformerEmbeddingFunction(\n",
    "            model_name=embedding_model\n",
    "        )\n",
    "        self.chroma_db = None\n",
    "\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Provide a database session.\n",
    "\n",
    "        Yields:\n",
    "            db: Database session instance.\n",
    "        \"\"\"\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "\n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of restaurant names.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurants = db.query(models.DimRestaurant).all()\n",
    "                return [r.nom for r in restaurants]\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred while fetching restaurant names: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _sanitize_collection_name(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Sanitize collection name to meet ChromaDB requirements:\n",
    "        - 3-63 characters\n",
    "        - Alphanumeric with hyphens and underscores\n",
    "        - No consecutive periods\n",
    "        \"\"\"\n",
    "        sanitized = re.sub(r'[^a-zA-Z0-9-_]', '-', name)\n",
    "        sanitized = re.sub(r'^[^a-zA-Z0-9]+', '', sanitized)\n",
    "        sanitized = re.sub(r'[^a-zA-Z0-9]+$', '', sanitized)\n",
    "        sanitized = re.sub(r'\\.{2,}', '.', sanitized)\n",
    "        if len(sanitized) < 3:\n",
    "            sanitized = sanitized + \"000\"[:3-len(sanitized)]\n",
    "        if len(sanitized) > 63:\n",
    "            sanitized = sanitized[:63]\n",
    "        return sanitized\n",
    "\n",
    "    def _create_collection(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Create a new ChromaDB collection to store embeddings.\n",
    "\n",
    "        Args:\n",
    "            path (str): Name of the collection to create in ChromaDB.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            collection_name = self._sanitize_collection_name(path)\n",
    "            self.chroma_db = self.client.get_or_create_collection(\n",
    "                name=collection_name,\n",
    "                embedding_function=self.embeddings,\n",
    "                metadata={\"hnsw:space\": \"cosine\"}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating collection: {e}\")\n",
    "            raise\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean text by removing unwanted characters and spaces.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned text.\n",
    "        \"\"\"\n",
    "        text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces\n",
    "        text = re.sub(r\"[^a-zA-Z0-9À-ÿ\\s]\", \"\", text)  # Remove special characters\n",
    "        return text.strip()\n",
    "\n",
    "    def get_restaurant_reviews_location(self, restaurant_name: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get the location and reviews for a specific restaurant.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): Name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            tuple: DataFrames for reviews, location, and restaurant info.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurant = db.query(models.DimRestaurant).filter(models.DimRestaurant.nom == restaurant_name).first()\n",
    "                location = db.query(models.DimLocation).filter(models.DimLocation.id_location == restaurant.id_location).first()\n",
    "                avis = db.query(models.FaitAvis).filter(models.FaitAvis.id_restaurant == restaurant.id_restaurant).all()\n",
    "\n",
    "            avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
    "            location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
    "            restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
    "\n",
    "            return avis_df, location_df, restaurant_df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred while fetching reviews and locations: {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # def transform_restaurant_chunk(self, restaurant_name: str,  chuncksize: 500) -> pd.DataFrame:\n",
    "    #     \"\"\"\n",
    "    #     Transform restaurant data and reviews into a single structured chunk.\n",
    "\n",
    "    #     Args:\n",
    "    #         restaurant_name (str): Name of the restaurant.\n",
    "\n",
    "    #     Returns:\n",
    "    #         pd.DataFrame: DataFrame containing the chunks.\n",
    "    #     \"\"\"\n",
    "    #     colnames = ['restaurant', 'chunk']\n",
    "    #     avis_df, location_df, restaurant_df = self.get_restaurant_reviews_location(restaurant_name)\n",
    "\n",
    "    #     if restaurant_df.empty:\n",
    "    #         return pd.DataFrame(columns=colnames)\n",
    "\n",
    "    #     # Combine all information into a single chunk\n",
    "    #     all_info = f\"Restaurant: {restaurant_df['nom'].iloc[0]} | Description: {restaurant_df['nom'].iloc[0]} \" \\\n",
    "    #                f\"| Localisation: {location_df['adresse'].iloc[0]}, {location_df['adresse'].iloc[0]} | \" \\\n",
    "    #                f\"Avis: {' | '.join(avis_df['review'].apply(self.clean_text))}\"\n",
    "\n",
    "    #     chunks = [{'restaurant': restaurant_name, 'chunk': all_info}]\n",
    "    #     return pd.DataFrame(chunks, columns=colnames)\n",
    "\n",
    "\n",
    "    def transform_restaurant_chunk(self, restaurant_name: str, chunk_size: int = 5000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform restaurant data and reviews into structured chunks.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): Name of the restaurant.\n",
    "            chunk_size (int, optional): Maximum size of each chunk (in characters). Defaults to 500.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing the chunks with restaurant names.\n",
    "        \"\"\"\n",
    "        colnames = ['restaurant', 'chunk']\n",
    "        avis_df, location_df, restaurant_df = self.get_restaurant_reviews_location(restaurant_name)\n",
    "\n",
    "        if restaurant_df.empty:\n",
    "            return pd.DataFrame(columns=colnames)\n",
    "\n",
    "        # Combine all relevant data into a single string\n",
    "        combined_info = []\n",
    "        \n",
    "        for column in restaurant_df.columns:\n",
    "            value = self.clean_text(str(restaurant_df[column].iloc[0]))\n",
    "            combined_info.append(f\"{column}: {value}\")\n",
    "        \n",
    "        for column in location_df.columns:\n",
    "            value = self.clean_text(str(location_df[column].iloc[0]))\n",
    "            combined_info.append(f\"{column}: {value}\")\n",
    "        \n",
    "        for _, review in avis_df.iterrows():\n",
    "            cleaned_review = self.clean_text(review['review'])\n",
    "            combined_info.append(f\"Review: {cleaned_review}\")\n",
    "        \n",
    "        all_info = \" \".join(combined_info)\n",
    "\n",
    "        # Split the information into chunks of `chunk_size`\n",
    "        chunks = []\n",
    "        while len(all_info) > 0:\n",
    "            chunk = all_info[:chunk_size]\n",
    "            # Add the restaurant name at the beginning of each chunk\n",
    "            chunks.append(f\"Restaurant: {restaurant_name} | {chunk}\")\n",
    "            all_info = all_info[chunk_size:]\n",
    "\n",
    "        # Create a DataFrame for the chunks\n",
    "        chunk_data = [{'restaurant': restaurant_name, 'chunk': chunk} for chunk in chunks]\n",
    "        return pd.DataFrame(chunk_data, columns=colnames)\n",
    "\n",
    "\n",
    "    def create_corpus(self) -> str:\n",
    "        \"\"\"\n",
    "        Create a corpus with a single chunk per restaurant.\n",
    "\n",
    "        Returns:\n",
    "            str: Text corpus containing all information.\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        for restaurant in self.get_all_restaurants_names():\n",
    "            df = self.transform_restaurant_chunk(restaurant)\n",
    "            if not df.empty:\n",
    "                corpus.append(\" \".join(df['chunk'].values))\n",
    "        return \" \".join(corpus)\n",
    "\n",
    "    def split_text_into_chunks(self, corpus: str, chunk_size: int = 100) -> list[str]:\n",
    "        \"\"\"\n",
    "        Split text into chunks of specified size.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): Text to split.\n",
    "            chunk_size (int, optional): Size of each chunk.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of chunks.\n",
    "        \"\"\"\n",
    "        tokenized_corpus = enc.encode(corpus)\n",
    "        chunks = [\n",
    "            \"\".join(enc.decode(tokenized_corpus[i : i + chunk_size]))\n",
    "            for i in tqdm(range(0, len(tokenized_corpus), chunk_size))\n",
    "        ]\n",
    "        return chunks\n",
    "    \n",
    "    \n",
    "\n",
    "    def add_embeddings(self, restaurant_chunks: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Add embeddings for each restaurant chunk to the ChromaDB collection.\n",
    "\n",
    "        Args:\n",
    "            restaurant_chunks (pd.DataFrame): DataFrame containing restaurant names and their corresponding chunks.\n",
    "        \"\"\"\n",
    "        if self.chroma_db is None:\n",
    "            raise ValueError(\"ChromaDB collection is not initialized. Call `_create_collection` first.\")\n",
    "\n",
    "        for _, row in tqdm(restaurant_chunks.iterrows(), total=restaurant_chunks.shape[0]):\n",
    "            restaurant_name = row['restaurant']\n",
    "            chunk = row['chunk']\n",
    "            document_id = str(uuid.uuid4())  # Generate a unique ID for this chunk\n",
    "            self.chroma_db.add(documents=[chunk], ids=[document_id], metadatas=[{\"restaurant\": restaurant_name}])\n",
    "\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Entry point to execute class methods.\n",
    "        \"\"\"\n",
    "        self._create_collection(self.path)\n",
    "        all_restaurant_chunks = []\n",
    "\n",
    "        for restaurant_name in self.get_all_restaurants_names():\n",
    "            restaurant_chunk = self.transform_restaurant_chunk(restaurant_name)\n",
    "            if not restaurant_chunk.empty:\n",
    "                all_restaurant_chunks.append(restaurant_chunk)\n",
    "\n",
    "        if all_restaurant_chunks:\n",
    "            all_chunks_df = pd.concat(all_restaurant_chunks, ignore_index=True)\n",
    "            self.add_embeddings(all_chunks_df)\n",
    "\n",
    "\n",
    "# Test the class\n",
    "if __name__ == \"__main__\":\n",
    "    test = BDDChunks(embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\", path=\"./\")\n",
    "    test()\n",
    "    print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from chromadb.config import Settings\n",
    "import logging\n",
    "\n",
    "# Initialize tiktoken encoding\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "class BDDChunks:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    Each restaurant's data is combined into a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str, path: str):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunks instance.\n",
    "\n",
    "        Args:\n",
    "            embedding_model (str): Name of the embedding model to use.\n",
    "            path (str): Path to the PDF or dataset to process.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.chunks: list[str] | None = None\n",
    "        self.client = chromadb.PersistentClient(\n",
    "            path=\"./ChromaDB7\", settings=Settings(anonymized_telemetry=False)\n",
    "        )\n",
    "        self.embedding_name = embedding_model\n",
    "        self.embeddings = SentenceTransformerEmbeddingFunction(\n",
    "            model_name=embedding_model\n",
    "        )\n",
    "        self.chroma_db = None\n",
    "\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Provide a database session.\n",
    "\n",
    "        Yields:\n",
    "            db: Database session instance.\n",
    "        \"\"\"\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "\n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of restaurant names.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurants = db.query(models.DimRestaurant).all()\n",
    "                return [r.nom for r in restaurants]\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred while fetching restaurant names: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _sanitize_collection_name(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Sanitize collection name to meet ChromaDB requirements:\n",
    "        - 3-63 characters\n",
    "        - Alphanumeric with hyphens and underscores\n",
    "        - No consecutive periods\n",
    "        \"\"\"\n",
    "        sanitized = re.sub(r'[^a-zA-Z0-9-_]', '-', name)\n",
    "        sanitized = re.sub(r'^[^a-zA-Z0-9]+', '', sanitized)\n",
    "        sanitized = re.sub(r'[^a-zA-Z0-9]+$', '', sanitized)\n",
    "        sanitized = re.sub(r'\\.{2,}', '.', sanitized)\n",
    "        if len(sanitized) < 3:\n",
    "            sanitized = sanitized + \"000\"[:3-len(sanitized)]\n",
    "        if len(sanitized) > 63:\n",
    "            sanitized = sanitized[:63]\n",
    "        return sanitized\n",
    "\n",
    "    def _create_collection(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Create a new ChromaDB collection to store embeddings.\n",
    "\n",
    "        Args:\n",
    "            path (str): Name of the collection to create in ChromaDB.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            collection_name = self._sanitize_collection_name(path)\n",
    "            self.chroma_db = self.client.get_or_create_collection(\n",
    "                name=collection_name,\n",
    "                embedding_function=self.embeddings,\n",
    "                metadata={\"hnsw:space\": \"cosine\"}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating collection: {e}\")\n",
    "            raise\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean text by removing unwanted characters and spaces.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned text.\n",
    "        \"\"\"\n",
    "        text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces\n",
    "        text = re.sub(r\"[^a-zA-Z0-9À-ÿ\\s]\", \"\", text)  # Remove special characters\n",
    "        return text.strip()\n",
    "\n",
    "    def get_restaurant_reviews_location(self, restaurant_name: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get the location and reviews for a specific restaurant.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): Name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            tuple: DataFrames for reviews, location, and restaurant info.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurant = db.query(models.DimRestaurant).filter(models.DimRestaurant.nom == restaurant_name).first()\n",
    "                location = db.query(models.DimLocation).filter(models.DimLocation.id_location == restaurant.id_location).first()\n",
    "                avis = db.query(models.FaitAvis).filter(models.FaitAvis.id_restaurant == restaurant.id_restaurant).all()\n",
    "\n",
    "            avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
    "            location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
    "            restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
    "\n",
    "            return avis_df, location_df, restaurant_df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred while fetching reviews and locations: {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    def transform_restaurant_chunk(self, restaurant_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Combine all restaurant data and reviews into a single structured chunk.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): Name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing the chunk.\n",
    "        \"\"\"\n",
    "        colnames = ['restaurant', 'chunk']\n",
    "        avis_df, location_df, restaurant_df = self.get_restaurant_reviews_location(restaurant_name)\n",
    "\n",
    "        if restaurant_df.empty:\n",
    "            return pd.DataFrame(columns=colnames)\n",
    "\n",
    "        # Combine all relevant data into one chunk\n",
    "        combined_info = []\n",
    "\n",
    "        for column in restaurant_df.columns:\n",
    "            value = self.clean_text(str(restaurant_df[column].iloc[0]))\n",
    "            combined_info.append(f\"{column}: {value}\")\n",
    "\n",
    "        for column in location_df.columns:\n",
    "            value = self.clean_text(str(location_df[column].iloc[0]))\n",
    "            combined_info.append(f\"{column}: {value}\")\n",
    "\n",
    "        for _, review in avis_df.iterrows():\n",
    "            cleaned_review = self.clean_text(review['review'][:500])  # Limit review size\n",
    "            combined_info.append(f\"Review: {cleaned_review}\")\n",
    "\n",
    "        chunk = \" \".join(combined_info)\n",
    "        return pd.DataFrame([{'restaurant': restaurant_name, 'chunk': chunk}], columns=colnames)\n",
    "\n",
    "    def create_corpus(self) -> str:\n",
    "        \"\"\"\n",
    "        Create a corpus from restaurant chunks.\n",
    "\n",
    "        Returns:\n",
    "            str: Text corpus.\n",
    "        \"\"\"\n",
    "        corpus = \"\"\n",
    "        for restaurant in self.get_all_restaurants_names():\n",
    "            df = self.transform_restaurant_chunk(restaurant)\n",
    "            corpus += \" \".join(df['chunk'].values) + \" \"\n",
    "        return corpus\n",
    "\n",
    "    def split_text_into_chunks(self, corpus: str, chunk_size: int = 100) -> list[str]:\n",
    "        \"\"\"\n",
    "        Split text into chunks of specified size.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): Text to split.\n",
    "            chunk_size (int, optional): Size of each chunk.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of chunks.\n",
    "        \"\"\"\n",
    "        tokenized_corpus = enc.encode(corpus)\n",
    "        chunks = [\n",
    "            \"\".join(enc.decode(tokenized_corpus[i : i + chunk_size]))\n",
    "            for i in tqdm(range(0, len(tokenized_corpus), chunk_size))\n",
    "        ]\n",
    "        return chunks\n",
    "\n",
    "    def add_embeddings(self, list_chunks: list[str], batch_size: int = 100) -> None:\n",
    "        \"\"\"\n",
    "        Add embeddings to the ChromaDB collection.\n",
    "\n",
    "        Args:\n",
    "            list_chunks (list[str]): List of chunks.\n",
    "            batch_size (int, optional): Batch size.\n",
    "        \"\"\"\n",
    "        if self.chroma_db is None:\n",
    "            raise ValueError(\"ChromaDB collection is not initialized. Call `_create_collection` first.\")\n",
    "\n",
    "        for i in tqdm(range(0, len(list_chunks), batch_size)):\n",
    "            batch_documents = list_chunks[i : i + batch_size]\n",
    "            list_ids = [str(uuid.uuid4()) for _ in batch_documents]\n",
    "            self.chroma_db.add(documents=batch_documents, ids=list_ids)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Entry point to execute class methods.\n",
    "        \"\"\"\n",
    "        corpus = self.create_corpus()\n",
    "        chunks = self.split_text_into_chunks(corpus)\n",
    "        self._create_collection(self.path)\n",
    "        self.add_embeddings(chunks)\n",
    "\n",
    "# # Test the class\n",
    "# if __name__ == \"__main__\":\n",
    "#     test = BDDChunks(embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\", path=\"./\")\n",
    "#     test()\n",
    "#     print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\1822548933.py:136: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\1822548933.py:136: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\1822548933.py:137: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\1822548933.py:137: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\1822548933.py:138: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\1822548933.py:138: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
      "100%|██████████| 70/70 [00:00<00:00, 476.94it/s]\n",
      "Batches: 100%|██████████| 3/3 [00:03<00:00,  1.00s/it]\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from chromadb.config import Settings\n",
    "import logging\n",
    "\n",
    "# Initialize tiktoken encoding\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "class BDDChunks:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str, path: str):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunks instance.\n",
    "\n",
    "        Args:\n",
    "            embedding_model (str): Name of the embedding model to use.\n",
    "            path (str): Path to the PDF or dataset to process.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.chunks: list[str] | None = None\n",
    "        self.client = chromadb.PersistentClient(\n",
    "            path=\"./ChromaDB5\", settings=Settings(anonymized_telemetry=False)\n",
    "        )\n",
    "        self.embedding_name = embedding_model\n",
    "        self.embeddings = SentenceTransformerEmbeddingFunction(\n",
    "            model_name=embedding_model\n",
    "        )\n",
    "        self.chroma_db = None\n",
    "\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Provide a database session.\n",
    "\n",
    "        Yields:\n",
    "            db: Database session instance.\n",
    "        \"\"\"\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "\n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of restaurant names.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurants = db.query(models.DimRestaurant).all()\n",
    "                return [r.nom for r in restaurants]\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred while fetching restaurant names: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _sanitize_collection_name(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Sanitize collection name to meet ChromaDB requirements:\n",
    "        - 3-63 characters\n",
    "        - Alphanumeric with hyphens and underscores\n",
    "        - No consecutive periods\n",
    "        \"\"\"\n",
    "        sanitized = re.sub(r'[^a-zA-Z0-9-_]', '-', name)\n",
    "        sanitized = re.sub(r'^[^a-zA-Z0-9]+', '', sanitized)\n",
    "        sanitized = re.sub(r'[^a-zA-Z0-9]+$', '', sanitized)\n",
    "        sanitized = re.sub(r'\\.{2,}', '.', sanitized)\n",
    "        if len(sanitized) < 3:\n",
    "            sanitized = sanitized + \"000\"[:3-len(sanitized)]\n",
    "        if len(sanitized) > 63:\n",
    "            sanitized = sanitized[:63]\n",
    "        return sanitized\n",
    "\n",
    "    def _create_collection(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Create a new ChromaDB collection to store embeddings.\n",
    "\n",
    "        Args:\n",
    "            path (str): Name of the collection to create in ChromaDB.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            collection_name = self._sanitize_collection_name(path)\n",
    "            self.chroma_db = self.client.get_or_create_collection(\n",
    "                name=collection_name,\n",
    "                embedding_function=self.embeddings,\n",
    "                metadata={\"hnsw:space\": \"cosine\"}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating collection: {e}\")\n",
    "            raise\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean text by removing unwanted characters and spaces.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned text.\n",
    "        \"\"\"\n",
    "        text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces\n",
    "        text = re.sub(r\"[^a-zA-Z0-9À-ÿ\\s]\", \"\", text)  # Remove special characters\n",
    "        return text.strip()\n",
    "\n",
    "    def get_restaurant_reviews_location(self, restaurant_name: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get the location and reviews for a specific restaurant.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): Name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            tuple: DataFrames for reviews, location, and restaurant info.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurant = db.query(models.DimRestaurant).filter(models.DimRestaurant.nom == restaurant_name).first()\n",
    "                location = db.query(models.DimLocation).filter(models.DimLocation.id_location == restaurant.id_location).first()\n",
    "                avis = db.query(models.FaitAvis).filter(models.FaitAvis.id_restaurant == restaurant.id_restaurant).all()\n",
    "\n",
    "            avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
    "            location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
    "            restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
    "\n",
    "            return avis_df, location_df, restaurant_df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred while fetching reviews and locations: {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    def transform_restaurant_chunk(self, restaurant_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform restaurant data and reviews into structured chunks.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): Name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing the chunks.\n",
    "        \"\"\"\n",
    "        colnames = ['restaurant', 'chunk']\n",
    "        avis_df, location_df, restaurant_df = self.get_restaurant_reviews_location(restaurant_name)\n",
    "\n",
    "        if restaurant_df.empty:\n",
    "            return pd.DataFrame(columns=colnames)\n",
    "\n",
    "        chunks = []\n",
    "        for column in restaurant_df.columns:\n",
    "            if column not in [\"nom\", \"description\"]:  # Keep only relevant columns\n",
    "                continue\n",
    "            value = self.clean_text(str(restaurant_df[column].iloc[0]))\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': f\"{column}: {value}\"})\n",
    "\n",
    "        for _, review in avis_df.iterrows():\n",
    "            cleaned_review = self.clean_text(review['review'][:500])  # Limit review size\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': f\"Review: {cleaned_review}\"})\n",
    "\n",
    "        return pd.DataFrame(chunks, columns=colnames)\n",
    "\n",
    "    def create_corpus(self) -> str:\n",
    "        \"\"\"\n",
    "        Create a corpus from restaurant chunks.\n",
    "\n",
    "        Returns:\n",
    "            str: Text corpus.\n",
    "        \"\"\"\n",
    "        corpus = \"\"\n",
    "        for restaurant in self.get_all_restaurants_names():\n",
    "            df = self.transform_restaurant_chunk(restaurant)\n",
    "            corpus += \" \".join(df['chunk'].values) + \" \"\n",
    "        return corpus\n",
    "\n",
    "    def split_text_into_chunks(self, corpus: str, chunk_size: int = 20000) -> list[str]:\n",
    "        \"\"\"\n",
    "        Split text into chunks of specified size.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): Text to split.\n",
    "            chunk_size (int, optional): Size of each chunk.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of chunks.\n",
    "        \"\"\"\n",
    "        tokenized_corpus = enc.encode(corpus)\n",
    "        chunks = [\n",
    "            \"\".join(enc.decode(tokenized_corpus[i : i + chunk_size]))\n",
    "            for i in tqdm(range(0, len(tokenized_corpus), chunk_size))\n",
    "        ]\n",
    "        return chunks\n",
    "\n",
    "    def add_embeddings(self, list_chunks: list[str], batch_size: int = 100) -> None:\n",
    "        \"\"\"\n",
    "        Add embeddings to the ChromaDB collection.\n",
    "\n",
    "        Args:\n",
    "            list_chunks (list[str]): List of chunks.\n",
    "            batch_size (int, optional): Batch size.\n",
    "        \"\"\"\n",
    "        if self.chroma_db is None:\n",
    "            raise ValueError(\"ChromaDB collection is not initialized. Call `_create_collection` first.\")\n",
    "\n",
    "        for i in tqdm(range(0, len(list_chunks), batch_size)):\n",
    "            batch_documents = list_chunks[i : i + batch_size]\n",
    "            list_ids = [str(uuid.uuid4()) for _ in batch_documents]\n",
    "            self.chroma_db.add(documents=batch_documents, ids=list_ids)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Entry point to execute class methods.\n",
    "        \"\"\"\n",
    "        corpus = self.create_corpus()\n",
    "        chunks = self.split_text_into_chunks(corpus)\n",
    "        self._create_collection(self.path)\n",
    "        self.add_embeddings(chunks)\n",
    "\n",
    "# Test the class\n",
    "if __name__ == \"__main__\":\n",
    "    test = BDDChunks(embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\", path=\"./\")\n",
    "    test()\n",
    "    print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\3532425305.py:152: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\3532425305.py:152: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\3532425305.py:153: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\3532425305.py:153: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\3532425305.py:154: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8872\\3532425305.py:154: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
      "100%|██████████| 364/364 [00:00<00:00, 845.91it/s]\n",
      "Batches: 100%|██████████| 4/4 [00:04<00:00,  1.02s/it]\n",
      "Batches: 100%|██████████| 4/4 [00:04<00:00,  1.04s/it]\n",
      "Batches: 100%|██████████| 4/4 [00:04<00:00,  1.07s/it]\n",
      "Batches: 100%|██████████| 2/2 [00:02<00:00,  1.44s/it]\n",
      "100%|██████████| 4/4 [00:21<00:00,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from chromadb.config import Settings\n",
    "import logging\n",
    "\n",
    "# Initialize tiktoken encoding\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "class BDDChunks:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str, path: str):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunks instance.\n",
    "\n",
    "        Args:\n",
    "            embedding_model (str): Name of the embedding model to use.\n",
    "            path (str): Path to the PDF or dataset to process.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.chunks: list[str] | None = None\n",
    "        self.client = chromadb.PersistentClient(\n",
    "            path=\"./ChromaDB3\", settings=Settings(anonymized_telemetry=False)\n",
    "        )\n",
    "        self.embedding_name = embedding_model\n",
    "        self.embeddings = SentenceTransformerEmbeddingFunction(\n",
    "            model_name=embedding_model\n",
    "        )\n",
    "        self.chroma_db = None\n",
    "        \n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Provide a database session.\n",
    "\n",
    "        Yields:\n",
    "            db: Database session instance.\n",
    "        \"\"\"\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "\n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of restaurant names.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurants = db.query(models.DimRestaurant).all()\n",
    "                return [r.nom for r in restaurants]\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred while fetching restaurant names: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _sanitize_collection_name(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Sanitize collection name to meet ChromaDB requirements:\n",
    "        - 3-63 characters\n",
    "        - Alphanumeric with hyphens and underscores\n",
    "        - No consecutive periods\n",
    "        \"\"\"\n",
    "        # Replace invalid characters with hyphens\n",
    "        sanitized = re.sub(r'[^a-zA-Z0-9-_]', '-', name)\n",
    "        \n",
    "        # Ensure name starts and ends with alphanumeric\n",
    "        sanitized = re.sub(r'^[^a-zA-Z0-9]+', '', sanitized)\n",
    "        sanitized = re.sub(r'[^a-zA-Z0-9]+$', '', sanitized)\n",
    "        \n",
    "        # Remove consecutive periods\n",
    "        sanitized = re.sub(r'\\.{2,}', '.', sanitized)\n",
    "        \n",
    "        # Ensure minimum length\n",
    "        if len(sanitized) < 3:\n",
    "            sanitized = sanitized + \"000\"[:3-len(sanitized)]\n",
    "            \n",
    "        # Truncate if too long\n",
    "        if len(sanitized) > 63:\n",
    "            sanitized = sanitized[:63]\n",
    "            \n",
    "        return sanitized\n",
    "\n",
    "    def _create_collection(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Create a new ChromaDB collection to store embeddings.\n",
    "\n",
    "        Args:\n",
    "            path (str): Name of the collection to create in ChromaDB.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create a valid collection name\n",
    "            collection_name = self._sanitize_collection_name(path)\n",
    "            self.chroma_db = self.client.get_or_create_collection(\n",
    "                name=collection_name,\n",
    "                embedding_function=self.embeddings,\n",
    "                metadata={\"hnsw:space\": \"cosine\"}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating collection: {e}\")\n",
    "            raise\n",
    "\n",
    "    def convert_to_arrow_compatible(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert a DataFrame to an Arrow-compatible format.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Converted DataFrame.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == 'object':\n",
    "                df[column] = df[column].astype(str)\n",
    "            elif df[column].dtype == 'int64':\n",
    "                df[column] = df[column].astype('int32')\n",
    "        return df\n",
    "\n",
    "    def get_restaurant_reviews_location(self, restaurant_name: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get the location and reviews for a specific restaurant.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): Name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            tuple: DataFrames for reviews, location, and restaurant info.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurant = db.query(models.DimRestaurant).filter(models.DimRestaurant.nom == restaurant_name).first()\n",
    "                location = db.query(models.DimLocation).filter(models.DimLocation.id_location == restaurant.id_location).first()\n",
    "                avis = db.query(models.FaitAvis).filter(models.FaitAvis.id_restaurant == restaurant.id_restaurant).all()\n",
    "            \n",
    "            avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
    "            location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
    "            restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
    "            \n",
    "            return avis_df, location_df, restaurant_df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred while fetching reviews and locations: {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    def transform_restaurant_chunk(self, restaurant_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform restaurant data and reviews into structured chunks.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): Name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing the chunks.\n",
    "        \"\"\"\n",
    "        colnames = ['restaurant', 'chunk']\n",
    "        avis_df, location_df, restaurant_df = self.get_restaurant_reviews_location(restaurant_name)\n",
    "        \n",
    "        if restaurant_df.empty:\n",
    "            return pd.DataFrame(columns=colnames)\n",
    "\n",
    "        chunks = []\n",
    "        for column in restaurant_df.columns:\n",
    "            value = restaurant_df[column].iloc[0]\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': f\"{column}: {value}\"})\n",
    "\n",
    "        # Include reviews as chunks\n",
    "        for _, review in avis_df.iterrows():\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': f\"Review: {review['review']}\"})\n",
    "\n",
    "        return pd.DataFrame(chunks, columns=colnames)\n",
    "\n",
    "    def create_corpus(self) -> str:\n",
    "        \"\"\"\n",
    "        Create a corpus from restaurant chunks.\n",
    "\n",
    "        Returns:\n",
    "            str: Text corpus.\n",
    "        \"\"\"\n",
    "        corpus = \"\"\n",
    "        for restaurant in self.get_all_restaurants_names():\n",
    "            df = self.transform_restaurant_chunk(restaurant)\n",
    "            corpus += \" \".join(df['chunk'].values) + \" \"\n",
    "        return corpus\n",
    "\n",
    "    def split_text_into_chunks(self, corpus: str, chunk_size: int = 5000) -> list[str]:\n",
    "        \"\"\"\n",
    "        Split text into chunks of specified size.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): Text to split.\n",
    "            chunk_size (int, optional): Size of each chunk.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of chunks.\n",
    "        \"\"\"\n",
    "        tokenized_corpus = enc.encode(corpus)\n",
    "        chunks = [\n",
    "            \"\".join(enc.decode(tokenized_corpus[i : i + chunk_size]))\n",
    "            for i in tqdm(range(0, len(tokenized_corpus), chunk_size))\n",
    "        ]\n",
    "        return chunks\n",
    "\n",
    "    def add_embeddings(self, list_chunks: list[str], batch_size: int = 100) -> None:\n",
    "        \"\"\"\n",
    "        Add embeddings to the ChromaDB collection.\n",
    "\n",
    "        Args:\n",
    "            list_chunks (list[str]): List of chunks.\n",
    "            batch_size (int, optional): Batch size.\n",
    "        \"\"\"\n",
    "        if self.chroma_db is None:\n",
    "            raise ValueError(\"ChromaDB collection is not initialized. Call `_create_collection` first.\")\n",
    "        \n",
    "        if len(list_chunks) < batch_size:\n",
    "            batch_size_for_chromadb = len(list_chunks)\n",
    "        else:\n",
    "            batch_size_for_chromadb = batch_size\n",
    "\n",
    "        for i in tqdm(range(0, len(list_chunks), batch_size_for_chromadb)):\n",
    "            batch_documents = list_chunks[i : i + batch_size_for_chromadb]\n",
    "            list_ids = [str(uuid.uuid4()) for _ in batch_documents]\n",
    "            self.chroma_db.add(documents=batch_documents, ids=list_ids)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Entry point to execute class methods.\n",
    "        \"\"\"\n",
    "        corpus = self.create_corpus()\n",
    "        # self._create_collection\n",
    "        chunks = self.split_text_into_chunks(corpus)\n",
    "        self._create_collection(self.path)\n",
    "        self.add_embeddings(chunks)\n",
    "        # pass\n",
    "\n",
    "# Test the class\n",
    "if __name__ == \"__main__\":\n",
    "    test = BDDChunks(embedding_model=\"paraphrase-multilingual-MiniLM-L12-v2\", path=\"./\")\n",
    "    # test = BDDChunks(embedding_model=\"paraphrase-xlm-r-multilingual-v1\", path=\"./\")\n",
    "    test()\n",
    "    print(\"Done\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2322011855.py:120: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2322011855.py:120: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2322011855.py:121: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2322011855.py:121: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2322011855.py:122: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2322011855.py:122: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
      "100%|██████████| 3638/3638 [00:00<00:00, 14257.46it/s]\n",
      "ERROR:__main__:Error creating collection: 'BDDChunks' object has no attribute '_sanitize_collection_name'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BDDChunks' object has no attribute '_sanitize_collection_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 221\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    220\u001b[0m     test \u001b[38;5;241m=\u001b[39m BDDChunks(embedding_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparaphrase-xlm-r-multilingual-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 221\u001b[0m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[132], line 214\u001b[0m, in \u001b[0;36mBDDChunks.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_corpus()\n\u001b[0;32m    213\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_text_into_chunks(corpus)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_embeddings(chunks)\n",
      "Cell \u001b[1;32mIn[132], line 76\u001b[0m, in \u001b[0;36mBDDChunks._create_collection\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create ChromaDB collection with sanitized name.\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     collection_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_collection_name\u001b[49m(name)\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchroma_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mget_or_create_collection(\n\u001b[0;32m     78\u001b[0m         name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m     79\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings,\n\u001b[0;32m     80\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhnsw:space\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     81\u001b[0m     )\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BDDChunks' object has no attribute '_sanitize_collection_name'"
     ]
    }
   ],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialiser l'encodage tiktoken\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "class BDDChunks:\n",
    "    \"\"\"\n",
    "    Une classe pour traiter des avis issus d'une base SQLite et les stocker sous forme de chunks avec embeddings.\n",
    "    Chaque avis est considéré comme un chunk unique.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str, path: str):\n",
    "        \"\"\"\n",
    "        Initialisation de l'instance BDDChunks.\n",
    "\n",
    "        Args:\n",
    "            embedding_model (str): Nom du modèle d'embedding à utiliser.\n",
    "            path (str): Chemin vers le PDF ou le dataset à traiter.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.chunks: list[str] | None = None\n",
    "        self.client = chromadb.PersistentClient(\n",
    "            path=\"./ChromaDB\", settings=Settings(anonymized_telemetry=False)\n",
    "        )\n",
    "        self.embedding_name = embedding_model\n",
    "        self.embeddings = SentenceTransformerEmbeddingFunction(\n",
    "            model_name=embedding_model\n",
    "        )\n",
    "        self.chroma_db = None\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Fournit une session de base de données.\n",
    "\n",
    "        Yields:\n",
    "            db: Instance de session de base de données.\n",
    "        \"\"\"\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "\n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Récupère tous les noms de restaurants depuis la base SQLite.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: Liste des noms de restaurants.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurants = db.query(models.DimRestaurant).all()\n",
    "                return [r.nom for r in restaurants]\n",
    "        except Exception as e:\n",
    "            print(f\"Une erreur s'est produite lors de la récupération des noms de restaurants : {e}\")\n",
    "            return []\n",
    "\n",
    "    def _create_collection(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Crée une nouvelle collection ChromaDB pour stocker les embeddings.\n",
    "\n",
    "        Args:\n",
    "            path (str): Nom de la collection à créer dans ChromaDB.\n",
    "        \"\"\"\n",
    "        # Crée un nom de collection valide\n",
    "        file_name = \"a\" + os.path.basename(path)[0:50].strip() + \"a\"\n",
    "        file_name = re.sub(r\"\\s+\", \"-\", file_name)\n",
    "\n",
    "        # Initialiser la collection ChromaDB\n",
    "        self.chroma_db = self.client.get_or_create_collection(\n",
    "            name=file_name,\n",
    "            embedding_function=self.embeddings,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "\n",
    "    def convert_to_arrow_compatible(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convertit un DataFrame au format compatible avec Arrow.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame d'entrée.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame converti.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == 'object':\n",
    "                df[column] = df[column].astype(str)\n",
    "            elif df[column].dtype == 'int64':\n",
    "                df[column] = df[column].astype('int32')\n",
    "        return df\n",
    "\n",
    "    def get_restaurant_reviews_location(self, restaurant_name: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Récupère l'emplacement et les avis pour un restaurant donné.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): Nom du restaurant.\n",
    "\n",
    "        Returns:\n",
    "            tuple: DataFrames pour les avis, l'emplacement et les infos du restaurant.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurant = db.query(models.DimRestaurant).filter(models.DimRestaurant.nom == restaurant_name).first()\n",
    "                location = db.query(models.DimLocation).filter(models.DimLocation.id_location == restaurant.id_location).first()\n",
    "                avis = db.query(models.FaitAvis).filter(models.FaitAvis.id_restaurant == restaurant.id_restaurant).all()\n",
    "            \n",
    "            avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
    "            location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
    "            restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
    "            \n",
    "            return avis_df, location_df, restaurant_df\n",
    "        except Exception as e:\n",
    "            print(f\"Une erreur s'est produite lors de la récupération des avis et emplacements : {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    def transform_restaurant_chunk(self, restaurant_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transforme les données d'un restaurant et ses avis en chunks structurés.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): Nom du restaurant.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame contenant les chunks.\n",
    "        \"\"\"\n",
    "        colnames = ['restaurant', 'chunk']\n",
    "        avis_df, location_df, restaurant_df = self.get_restaurant_reviews_location(restaurant_name)\n",
    "        \n",
    "        if restaurant_df.empty:\n",
    "            return pd.DataFrame(columns=colnames)\n",
    "\n",
    "        chunks = []\n",
    "        for column in restaurant_df.columns:\n",
    "            value = restaurant_df[column].iloc[0]\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': f\"{column}: {value}\"})\n",
    "\n",
    "        # Inclure les avis comme chunks\n",
    "        for _, review in avis_df.iterrows():\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': f\"Review: {review['review']}\"})\n",
    "\n",
    "        return pd.DataFrame(chunks, columns=colnames)\n",
    "\n",
    "    def create_corpus(self) -> str:\n",
    "        \"\"\"\n",
    "        Crée un corpus à partir des chunks des restaurants.\n",
    "\n",
    "        Returns:\n",
    "            str: Corpus textuel.\n",
    "        \"\"\"\n",
    "        corpus = \"\"\n",
    "        for restaurant in self.get_all_restaurants_names():\n",
    "            df = self.transform_restaurant_chunk(restaurant)\n",
    "            corpus += \" \".join(df['chunk'].values) + \" \"\n",
    "        return corpus\n",
    "\n",
    "    def split_text_into_chunks(self, corpus: str, chunk_size: int = 500) -> list[str]:\n",
    "        \"\"\"\n",
    "        Divise un texte en chunks de taille spécifiée.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): Texte à diviser.\n",
    "            chunk_size (int, optional): Taille de chaque chunk.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: Liste des chunks.\n",
    "        \"\"\"\n",
    "        tokenized_corpus = enc.encode(corpus)\n",
    "        chunks = [\n",
    "            \"\".join(enc.decode(tokenized_corpus[i : i + chunk_size]))\n",
    "            for i in tqdm(range(0, len(tokenized_corpus), chunk_size))\n",
    "        ]\n",
    "        return chunks\n",
    "\n",
    "    def add_embeddings(self, list_chunks: list[str], batch_size: int = 100) -> None:\n",
    "        \"\"\"\n",
    "        Ajoute les embeddings à la collection ChromaDB.\n",
    "\n",
    "        Args:\n",
    "            list_chunks (list[str]): Liste des chunks.\n",
    "            batch_size (int, optional): Taille du batch.\n",
    "        \"\"\"\n",
    "        if self.chroma_db is None:\n",
    "            raise ValueError(\"ChromaDB collection is not initialized. Call `_create_collection` first.\")\n",
    "        \n",
    "        if len(list_chunks) < batch_size:\n",
    "            batch_size_for_chromadb = len(list_chunks)\n",
    "        else:\n",
    "            batch_size_for_chromadb = batch_size\n",
    "\n",
    "        for i in tqdm(range(0, len(list_chunks), batch_size_for_chromadb)):\n",
    "            batch_documents = list_chunks[i : i + batch_size_for_chromadb]\n",
    "            list_ids = [str(uuid.uuid4()) for _ in batch_documents]\n",
    "            self.chroma_db.add(documents=batch_documents, ids=list_ids)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Point d'entrée pour exécuter les méthodes de la classe.\n",
    "        \"\"\"\n",
    "        corpus = self.create_corpus()\n",
    "        chunks = self.split_text_into_chunks(corpus)\n",
    "        self._create_collection(self.path)\n",
    "        self.add_embeddings(chunks)\n",
    "\n",
    "\n",
    "# Tester la classe\n",
    "if __name__ == \"__main__\":\n",
    "    test = BDDChunks(embedding_model=\"paraphrase-xlm-r-multilingual-v1\", path=\"./\")\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import chromadb\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2395343120.py:124: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2395343120.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2395343120.py:125: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2395343120.py:125: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2395343120.py:126: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2395343120.py:126: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
      "100%|██████████| 3638/3638 [00:00<00:00, 13488.38it/s]\n",
      "  0%|          | 0/37 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 259\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    258\u001b[0m     test \u001b[38;5;241m=\u001b[39m BDDChunks( embedding_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparaphrase-xlm-r-multilingual-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 259\u001b[0m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[128], line 252\u001b[0m, in \u001b[0;36mBDDChunks.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_text_into_chunks(corpus)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# self._create_collection(self.path)\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# self.chroma_db.insert_documents(chunks)\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[128], line 241\u001b[0m, in \u001b[0;36mBDDChunks.add_embeddings\u001b[1;34m(self, list_chunks, batch_size)\u001b[0m\n\u001b[0;32m    237\u001b[0m list_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mstr\u001b[39m(id_chunk) \u001b[38;5;28;01mfor\u001b[39;00m id_chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_documents)))\n\u001b[0;32m    239\u001b[0m ]\n\u001b[0;32m    240\u001b[0m list_id_doc \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m list_ids]\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchroma_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m(documents\u001b[38;5;241m=\u001b[39mbatch_documents, ids\u001b[38;5;241m=\u001b[39mlist_id_doc)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    242\u001b[0m document_ids\u001b[38;5;241m.\u001b[39mextend(list_ids)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import chromadb\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from chromadb.config import Settings\n",
    "\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "class BDDChunks:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str, path: str):\n",
    "        \"\"\"\n",
    "        Initialize a BDDChunks instance.\n",
    "\n",
    "        Args:\n",
    "            embedding_model (str): The name of the embedding model to use for generating embeddings.\n",
    "            path (str): The file path to the PDF or dataset to process.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.chunks: list[str] | None = None\n",
    "        self.client = chromadb.PersistentClient(\n",
    "            path=\"./ChromaDB\", settings=Settings(anonymized_telemetry=False)\n",
    "        )\n",
    "        self.embedding_name = embedding_model\n",
    "        self.embeddings = SentenceTransformerEmbeddingFunction(\n",
    "            model_name=embedding_model\n",
    "        )\n",
    "        self.chroma_db = None\n",
    "        self.db = None\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Provide a database session.\n",
    "\n",
    "        Yields:\n",
    "            db: A session instance from the database.\n",
    "        \"\"\"\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "\n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of restaurant names.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurants = db.query(models.DimRestaurant).all()\n",
    "                return [r.nom for r in restaurants]\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching restaurant names: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    def _create_collection(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Create a new ChromaDB collection for storing embeddings.\n",
    "\n",
    "        Args:\n",
    "            path (str): The name of the collection to create in ChromaDB.\n",
    "        \"\"\"\n",
    "        # Tester qu'en changeant de path, on accède pas au reste\n",
    "        file_name = \"a\" + os.path.basename(path)[0:50].strip() + \"a\"\n",
    "        file_name = re.sub(r\"\\s+\", \"-\", file_name)\n",
    "        # Expected collection name that (1) contains 3-63 characters, (2) starts and ends with an alphanumeric character, (3) otherwise contains only alphanumeric characters, underscores or hyphens (-), (4) contains no two consecutive periods (..)\n",
    "        self.chroma_db = self.client.get_or_create_collection(name=file_name, embedding_function=self.embeddings, metadata={\"hnsw:space\": \"cosine\"})  # type: ignore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def convert_to_arrow_compatible(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert a DataFrame to an Arrow-compatible format.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Converted DataFrame.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == 'object':\n",
    "                df[column] = df[column].astype(str)\n",
    "            elif df[column].dtype == 'int64':\n",
    "                df[column] = df[column].astype('int32')\n",
    "        return df\n",
    "\n",
    "    def get_restaurant_reviews_location(self, restaurant_name: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get the location and reviews for a specific restaurant.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            tuple: DataFrames for reviews, location, and restaurant information.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurant = db.query(models.DimRestaurant).filter(models.DimRestaurant.nom == restaurant_name).first()\n",
    "                location = db.query(models.DimLocation).filter(models.DimLocation.id_location == restaurant.id_location).first()\n",
    "                avis = db.query(models.FaitAvis).filter(models.FaitAvis.id_restaurant == restaurant.id_restaurant).all()\n",
    "            \n",
    "            avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
    "            location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
    "            restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
    "            \n",
    "            return avis_df, location_df, restaurant_df\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching restaurant reviews location: {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    def transform_restaurant_chunk(self, restaurant_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform a restaurant's data and reviews into structured chunks.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the restaurant chunks.\n",
    "        \"\"\"\n",
    "        colnames = ['restaurant', 'chunk']\n",
    "        avis_df, location_df, restaurant_df = self.get_restaurant_reviews_location(restaurant_name)\n",
    "        \n",
    "        if restaurant_df.empty:\n",
    "            return pd.DataFrame(columns=colnames)\n",
    "\n",
    "        chunks = []\n",
    "        for column in restaurant_df.columns:\n",
    "            value = restaurant_df[column].iloc[0]\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': f\"{column}: {value}\"})\n",
    "\n",
    "        # Include review comments as chunks\n",
    "        for _, review in avis_df.iterrows():\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': f\"Review: {review['review']}\"})\n",
    "\n",
    "        return pd.DataFrame(chunks, columns=colnames)\n",
    "\n",
    "    def create_corpus(self) -> str:\n",
    "        \"\"\"\n",
    "        Create a corpus from the restaurant chunks.\n",
    "\n",
    "        Returns:\n",
    "            str: The text corpus.\n",
    "        \"\"\"\n",
    "        corpus = \"\"\n",
    "        for restaurant in self.get_all_restaurants_names():\n",
    "            df = self.transform_restaurant_chunk(restaurant)\n",
    "            corpus += \" \".join(df['chunk'].values) + \" \"\n",
    "        return corpus\n",
    "\n",
    "    # def insert_into_db(self):\n",
    "    #     \"\"\"\n",
    "    #     Insert the chunks into the SQLite database in batches for improved performance.\n",
    "    #     \"\"\"\n",
    "    #     all_restaurants = self.get_all_restaurants_names()\n",
    "    #     batch_size = 100 # Define the batch size for insertion\n",
    "\n",
    "    #     for restaurant in tqdm(all_restaurants):\n",
    "    #         #verifier si le restaurant est deja dans la table rag_avis\n",
    "    #         try:\n",
    "    #             with next(self.get_db()) as db:\n",
    "    #                 restaurant_exists = db.query(models.RagAvis).filter(models.RagAvis.restaurantName == restaurant).first()\n",
    "    #         except Exception as e:\n",
    "    #             print(f\"An error occurred while checking if the restaurant exists in the database: {e}\")\n",
    "    #             restaurant_exists = None\n",
    "\n",
    "    #         if restaurant_exists:\n",
    "    #             print(f\"Restaurant {restaurant} already exists in the database.\")\n",
    "    #             continue\n",
    "    #         df = self.transform_restaurant_chunk(restaurant)\n",
    "    #         chunks = [\n",
    "    #             models.RagAvis(restaurantName=row['restaurant'], review=row['chunk'])\n",
    "    #             for _, row in df.iterrows()\n",
    "    #         ]\n",
    "\n",
    "    #         try:\n",
    "    #             with next(self.get_db()) as db:\n",
    "    #                 for i in range(0, len(chunks), batch_size):\n",
    "    #                     db.bulk_save_objects(chunks[i:i + batch_size])\n",
    "    #                     db.commit()\n",
    "    #         except Exception as e:\n",
    "    #             print(f\"An error occurred while inserting chunks into the database for {restaurant}: {e}\")\n",
    "\n",
    "    def split_text_into_chunks(self, corpus: str, chunk_size: int = 500) -> list[str]:\n",
    "            \"\"\"\n",
    "            Splits a given text corpus into chunks of a specified size.\n",
    " \n",
    "            Args:\n",
    "                corpus (str): The input text corpus to be split into chunks.\n",
    "                chunk_size (int, optional): The size of each chunk. Defaults to 500.\n",
    "\n",
    "            Returns:\n",
    "                list[str]: A list of text chunks.\n",
    "            \"\"\"\n",
    "            tokenized_corpus = enc.encode(corpus)\n",
    "            chunks = [\n",
    "                \"\".join(enc.decode(tokenized_corpus[i : i + chunk_size]))\n",
    "                for i in tqdm(range(0, len(tokenized_corpus), chunk_size))\n",
    "            ]\n",
    "\n",
    "            return chunks\n",
    "    \n",
    "    def add_embeddings(self, list_chunks: list[str], batch_size: int = 100) -> None:\n",
    "        if len(list_chunks) < batch_size:\n",
    "            batch_size_for_chromadb = len(list_chunks)\n",
    "        else:\n",
    "            batch_size_for_chromadb = batch_size\n",
    "\n",
    "        document_ids: list[str] = []\n",
    "\n",
    "        for i in tqdm(\n",
    "            range(0, len(list_chunks), batch_size_for_chromadb)\n",
    "        ):  # On met en place une stratégie d'ajout par batch car ChromaDB ne supporte pas plus de 166 documents d'un coup.\n",
    "            batch_documents = list_chunks[i : i + batch_size_for_chromadb]\n",
    "            list_ids = [\n",
    "                str(id_chunk) for id_chunk in list(range(i, i + len(batch_documents)))\n",
    "            ]\n",
    "            list_id_doc = [str(uuid.uuid4()) for x in list_ids]\n",
    "            self.chroma_db.add(documents=batch_documents, ids=list_id_doc)  # type: ignore\n",
    "            document_ids.extend(list_ids)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Entry point to invoke methods of the class.\n",
    "        \"\"\"\n",
    "        corpus = self.create_corpus()\n",
    "        chunks = self.split_text_into_chunks(corpus)\n",
    "        # self._create_collection(self.path)\n",
    "        # self.chroma_db.insert_documents(chunks)\n",
    "        self.add_embeddings(chunks)\n",
    "\n",
    "\n",
    "\n",
    "# Test the class\n",
    "if __name__ == \"__main__\":\n",
    "    test = BDDChunks( embedding_model=\"paraphrase-xlm-r-multilingual-v1\", path=\"./\")\n",
    "    test()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "class BDDChunksSQLite:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunksSQLite instance.\n",
    "        \"\"\"\n",
    "        self.db = next(self.get_db())\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Provide a database session.\n",
    "\n",
    "        Yields:\n",
    "            db: A session instance from the database.\n",
    "        \"\"\"\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "\n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of restaurant names.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.db as db:\n",
    "                restaurants = db.query(models.DimRestaurant).all()\n",
    "                return [r.nom for r in restaurants]\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching restaurant names: {e}\")\n",
    "            return []\n",
    "\n",
    "    def convert_to_arrow_compatible(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert a DataFrame to an Arrow-compatible format.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Converted DataFrame.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == 'object':\n",
    "                df[column] = df[column].astype(str)\n",
    "            elif df[column].dtype == 'int64':\n",
    "                df[column] = df[column].astype('int32')\n",
    "        return df\n",
    "\n",
    "    def get_restaurant_reviews_location(self, restaurant_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the location and reviews for a specific restaurant.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing restaurant name, location, and reviews.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.db as db:\n",
    "                restaurant = db.query(models.DimRestaurant).filter(models.DimRestaurant.nom == restaurant_name).first()\n",
    "                location = db.query(models.DimLocation).filter(models.DimLocation.id_location == restaurant.id_location).first()\n",
    "                avis = db.query(models.FaitAvis).filter(models.FaitAvis.id_restaurant == restaurant.id_restaurant).all()\n",
    "            \n",
    "            # Convert the data to a DataFrame\n",
    "            avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
    "            location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
    "            restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
    "            \n",
    "            return avis_df, location_df, restaurant_df\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching restaurant reviews location: {e}\")\n",
    "            return pd.DataFrame()\n",
    " \n",
    "    def transform_restaurant_chunk(self, restaurant_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform a restaurant chunk into a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the restaurant.\n",
    "            chunck_size (int): The size of each chunk of reviews.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the restaurant chunk.\n",
    "        \"\"\"\n",
    "        colnames = ['restaurant', 'chunk']\n",
    "        \n",
    "        avis_df, location_df, restaurant_df = self.get_restaurant_reviews_location(restaurant_name)\n",
    "        \n",
    "        # List to hold the chunks before creating the DataFrame\n",
    "        chunks = []\n",
    "        #add classement classement\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' classement : '+ str(restaurant_df['classement'][0]) +' '})\n",
    "        \n",
    "        # horaires\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' horaires : '+ str(restaurant_df['horaires'][0]) +' '})\n",
    "        \n",
    "        # note_globale\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' note_globale : '+ str(restaurant_df['note_globale'][0]) +' '})\n",
    "        \n",
    "        # note_cuisine\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' note_cuisine : '+ str(restaurant_df['note_cuisine'][0]) +' '})\n",
    "        \n",
    "        # note_service\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' note_service : '+ str(restaurant_df['note_service'][0]) +' '})\n",
    "\n",
    "        # note_rapportqualiteprix\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' note_rapportqualiteprix : '+ str(restaurant_df['note_rapportqualiteprix'][0]) +' '})\n",
    "\n",
    "        # note_ambiance\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' note_ambiance : '+ str(restaurant_df['note_ambiance'][0]) +' '})\n",
    "        \n",
    "        # infos_pratiques\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' infos_pratiques : '+ str(restaurant_df['infos_pratiques'][0]) +' '})\n",
    "        \n",
    "        # repas\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' repas : '+ str(restaurant_df['repas'][0]) +' '})\n",
    "               \n",
    "        # fourchette_prix\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' fourchette_prix : '+ str(restaurant_df['fourchette_prix'][0]) +' '})\n",
    "\n",
    "        # fonctionnalites\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' fonctionnalites : '+ str(restaurant_df['fonctionnalites'][0]) +' '})\n",
    "\n",
    "        # type_cuisines\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' type_cuisines : '+ str(restaurant_df['type_cuisines'][0]) +' '})\n",
    "\n",
    "        # nb_avis\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' nb_avis : '+ str(restaurant_df['nb_avis'][0]) +' '})\n",
    "\n",
    "        # nbExcellent \n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' nbExcellent : '+ str(restaurant_df['nbExcellent'][0]) +' '})\n",
    "        \n",
    "        # nbTresbon \n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' nbTresbon : '+ str(restaurant_df['nbTresbon'][0]) +' '})\n",
    "        \n",
    "        # nbMoyen \n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' nbMoyen : '+ str(restaurant_df['nbMoyen'][0]) +' '})\n",
    "        \n",
    "        # nbMediocre \n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' nbMediocre : '+ str(restaurant_df['nbMediocre'][0]) +' '})\n",
    "        \n",
    "        # nbHorrible \n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' nbHorrible : '+ str(restaurant_df['nbHorrible'][0]) +' '})\n",
    "\n",
    "        # Insert all avis in the dataframe with the restaurant name\n",
    "        for i in range(0, len(avis_df)):\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name + 'Commentaire : ' + avis_df['review'][i]})\n",
    "    \n",
    "        # Create DataFrame once all chunks are gathered\n",
    "        df_chunck = pd.DataFrame(chunks, columns=colnames)\n",
    "        \n",
    "        return df_chunck\n",
    "    \n",
    "    def create_corpus(self ) -> str:\n",
    "        \"\"\"\n",
    "        Create a corpus from the restaurant chunks.\n",
    "\n",
    "        Returns:\n",
    "            str: The text corpus.\n",
    "        \"\"\"\n",
    "        corpus = \"\"\n",
    "        for restaurant in self.get_all_restaurants_names():\n",
    "            df = self.transform_restaurant_chunk(restaurant)\n",
    "            corpus += \" \".join(df['chunk'].values)\n",
    "        return corpus\n",
    "    \n",
    "\n",
    "    def insert_into_db(self):\n",
    "        \"\"\"\n",
    "        Insert the chunks into the SQLite database.\n",
    "        \"\"\"\n",
    "        all_restaurants = self.get_all_restaurants_names()\n",
    "        for restaurant in tqdm(all_restaurants):\n",
    "            df = self.transform_restaurant_chunk(restaurant)\n",
    "\n",
    "            #ajouter les  lignes dans la base de données RagAvisBase\n",
    "            for i in range(0, len(df)):\n",
    "                # Insert the chunk into the database\n",
    "                try:\n",
    "                    with self.db as db:\n",
    "                        ragAvis = models.RagAvisBase(restaurant=df['restaurant'][i], chunk=df['chunk'][i])\n",
    "                        db.add(ragAvis)\n",
    "                        db.commit()\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while inserting chunk into the database: {e}\")\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # def get_embedding(self, restaurantName: str) -> list[float]:\n",
    "    #     restaurant_chunck = self.transform_restaurant_chunk(restaurantName, 5)\n",
    "    #     # Get the embeddings for each chunk\n",
    "    #     embeddings = []\n",
    "    #     for chunk in restaurant_chunck['chunk']:\n",
    "    #         embeddings.append(self.embedder.encode(chunk))\n",
    "    #     return embeddings\n",
    "    \n",
    "    # def embedder(self, chunk: str) -> list[float]:\n",
    "    #     \"\"\"\n",
    "    #     Embed a chunk using the SentenceTransformer model.\n",
    "\n",
    "    #     Args:\n",
    "    #         chunk (str): The input chunk to embed.\n",
    "\n",
    "    #     Returns:\n",
    "    #         list[float]: The embedding of the chunk.\n",
    "    #     \"\"\"\n",
    "    #     return self.embedder.encode(chunk)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, *args, **kwds):\n",
    "        \"\"\"\n",
    "        Entry point to invoke methods of the class.\n",
    "        \"\"\"\n",
    "        self.insert_into_db()\n",
    "        \n",
    "\n",
    "# Test the class\n",
    "test = BDDChunksSQLite()\n",
    "# # result = test.get_restaurant_reviews_location(\"Aromatic Restaurant\")\n",
    "# result = test.transform_restaurant_chunk(\"Aromatic Restaurant\")\n",
    "# result.head(5)\n",
    "# #renregistrement des chunks dans un fichier csv\n",
    "# result.to_csv('chunks.csv', index=False)\n",
    "\n",
    "# # corpus = test.split_text_into_chunks(test.create_corpus())\n",
    "# with open('corpus.txt', 'w') as f:\n",
    "#     # f.write(corpus)\n",
    "#     for chunk in corpus:\n",
    "#         f.write(chunk + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while fetching restaurant reviews location: 'DimLocation' object has no attribute 'nom'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import os\n",
    "import uuid\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class BDDChunksSQLite:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "\n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunksSQLite instance.\n",
    "        \"\"\"\n",
    "        self.db = next(self.get_db())\n",
    "        # pass  # Constructor is currently empty but can be extended if needed in the future.\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Provide a database session.\n",
    "\n",
    "        Yields:\n",
    "            db: A session instance from the database.\n",
    "        \"\"\"\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "\n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of restaurant names.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.db as db: \n",
    "                restaurants = db.query(models.DimRestaurant).all()\n",
    "                restaurant_names = [r.nom for r in restaurants]\n",
    "            return restaurant_names\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching restaurant names: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    def convert_to_arrow_compatible(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == 'object':\n",
    "                df[column] = df[column].astype(str)\n",
    "            elif df[column].dtype == 'int64':\n",
    "                df[column] = df[column].astype('int32')\n",
    "        return df\n",
    "\n",
    "    def get_restaurant_reviews_location(self, restaurant_name: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get the location of reviews for a specific restaurant.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of review locations.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.db as db:\n",
    "                # restaurant = db.query(models.DimRestaurant, models.DimLocation, models.).join.filter(models.DimRestaurant.nom == restaurant_name).all()\n",
    "                join_data = db.query(models.DimRestaurant, models.DimLocation, models.FaitAvis) \\\n",
    "                    .join(models.DimLocation, models.DimRestaurant.id_location == models.DimLocation.id_location) \\\n",
    "                    .join(models.FaitAvis, models.DimRestaurant.id_restaurant == models.FaitAvis.id_restaurant) \\\n",
    "                    .filter(models.DimRestaurant.nom == restaurant_name).all()\n",
    "                \n",
    "            # restaurant = pd.DataFrame([schemas.DimRestaurant.from_orm(r).dict() for r in restaurant])\n",
    "\n",
    "            # join_data = pd.DataFrame([r.__dict__ for r in join_data])\n",
    "            data = pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"restaurant\": r[0].nom,\n",
    "                        \"location\": r[1].nom,\n",
    "                        \"avis\": r[2].avis,\n",
    "                    }\n",
    "                    for  restaurant , location  , avis in join_data\n",
    "                ]\n",
    "            )\n",
    "            # restaurant_df = pd.DataFrame([r.__dict__ for r in restaurant])\n",
    "            # location_df = pd.DataFrame([l.__dict__ for l in location])\n",
    "            # avis_df = pd.DataFrame([a.__dict__ for a in avis])\n",
    "\n",
    "\n",
    "            #transformation  of the result to a list of strings\n",
    "            \n",
    "            # return convert_to_arrow_compatible(restaurant)\n",
    "            return join_data\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching restaurant reviews location: {e}\")\n",
    "            return []   \n",
    "    \n",
    "    def __call__(self, *args, **kwds):\n",
    "        self.get_all_restaurants_names()\n",
    "        pass     \n",
    "test = BDDChunksSQLite()\n",
    "test.get_restaurant_reviews_location(\"Aromatic Restaurant\")\n",
    "# print(test.get_all_restaurants_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'model.models' has no attribute 'Restaurant'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 145\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# def  get_restaurant_location_reviews(self, restaurant_name: str) -> list[schemas.Review]:\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m#     \"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m#     Get all reviews for a given restaurant.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m#         embedding = self.generate_fake_embedding(review)  # Generate embedding for the review (chunk)\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m#         self.store_chunk_in_db(restaurant_name, review, embedding)\u001b[39;00m\n\u001b[0;32m    144\u001b[0m bdd_chunks \u001b[38;5;241m=\u001b[39m BDDChunksSQLite()\n\u001b[1;32m--> 145\u001b[0m restaurants \u001b[38;5;241m=\u001b[39m \u001b[43mbdd_chunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all_restaurants_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m restaurants\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# bdd_chunks.process_reviews()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 46\u001b[0m, in \u001b[0;36mBDDChunksSQLite.get_all_restaurants_names\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03mGet all restaurant names from the SQLite database.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    list[str]: A list of restaurant names.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_db()) \u001b[38;5;28;01mas\u001b[39;00m db:\n\u001b[1;32m---> 46\u001b[0m     restaurants \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mquery(\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRestaurant\u001b[49m)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# restaurants_name = [r.nom for r in restaurants]\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restaurants\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'model.models' has no attribute 'Restaurant'"
     ]
    }
   ],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import os\n",
    "import uuid\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "class BDDChunksSQLite:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    \n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self ):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunksSQLite instance.\n",
    "\n",
    "        Args:\n",
    "            sqlite_db_path (str): Path to the SQLite database file.\n",
    "            reviews_table (str): Name of the table containing reviews and restaurant information.\n",
    "            embeddings_table (str): Name of the table where embeddings will be stored.\n",
    "        \"\"\"\n",
    " \n",
    "        # self.db = self.get_db()\n",
    "\n",
    "    # Database dependency\n",
    "    def get_db(self):\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "    \n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of restaurant names.\n",
    "        \"\"\"\n",
    "\n",
    "        with next(self.get_db()) as db:\n",
    "            restaurants = db.query(models.Restaurant).all()\n",
    "        # restaurants_name = [r.nom for r in restaurants]\n",
    "        return restaurants\n",
    "    \n",
    "    # def  get_restaurant_location_reviews(self, restaurant_name: str) -> list[schemas.Review]:\n",
    "    #     \"\"\"\n",
    "    #     Get all reviews for a given restaurant.\n",
    "\n",
    "    #     Args:\n",
    "    #         restaurant_name (str): The name of the restaurant.\n",
    "\n",
    "    #     Returns:\n",
    "    #         list[schemas.Review]: A list of reviews for the restaurant.\n",
    "    #     \"\"\"\n",
    "    #     with self.get_db() as db:\n",
    "    #         reviews = db.query(models.Review).filter(models.Review.restaurant_name == restaurant_name).all()\n",
    "    #     return reviews\n",
    "\n",
    "\n",
    "    # def fetch_reviews_from_db(self) -> list[tuple[str, str]]:\n",
    "    #     \"\"\"\n",
    "    #     Fetch reviews and their associated restaurant names from the SQLite database.\n",
    "\n",
    "    #     Returns:\n",
    "    #         list[tuple[str, str]]: A list of tuples containing restaurant names and reviews.\n",
    "    #     \"\"\"\n",
    "    #     data = []\n",
    "    #     # conn = sqlite3.connect(self.sqlite_db_path)\n",
    "    #     # cursor = conn.cursor()\n",
    "    #     # cursor.execute(f\"SELECT restaurant_name, review FROM {self.reviews_table};\")\n",
    "    #     # data = cursor.fetchall()\n",
    "    #     # conn.close()\n",
    "    #     with self.get_db() as db:\n",
    "    #         data = db.query(models.Review).all()\n",
    "    #         data = [(d.restaurant_name, d.review) for d in data]\n",
    "\n",
    "    #     return data\n",
    "\n",
    "    # def generate_fake_embedding(self, text: str) -> list[float]:\n",
    "    #     \"\"\"\n",
    "    #     Generate a fake embedding for a given text. Replace this with your embedding logic.\n",
    "\n",
    "    #     Args:\n",
    "    #         text (str): The text for which to generate an embedding.\n",
    "\n",
    "    #     Returns:\n",
    "    #         list[float]: A list representing the embedding vector.\n",
    "    #     \"\"\"\n",
    "    #     # Example: Return the length of each word in the text as a fake embedding.\n",
    "    #     return [len(word) for word in text.split()]\n",
    "\n",
    "    # def store_chunk_in_db(self, restaurant_name: str, chunk: str, embedding: list[float]) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Store a chunk and its embedding in the SQLite database.\n",
    "\n",
    "    #     Args:\n",
    "    #         restaurant_name (str): The name of the associated restaurant.\n",
    "    #         chunk (str): The text chunk (in this case, the full review).\n",
    "    #         embedding (list[float]): The embedding vector for the chunk.\n",
    "    #     \"\"\"\n",
    "    #     conn = sqlite3.connect(self.sqlite_db_path)\n",
    "    #     cursor = conn.cursor()\n",
    "\n",
    "    #     # Ensure the embeddings table exists\n",
    "    #     cursor.execute(f\"\"\"\n",
    "    #     CREATE TABLE IF NOT EXISTS {self.embeddings_table} (\n",
    "    #         id TEXT PRIMARY KEY,\n",
    "    #         restaurant_name TEXT,\n",
    "    #         chunk TEXT,\n",
    "    #         embedding TEXT\n",
    "    #     );\n",
    "    #     \"\"\")\n",
    "\n",
    "    #     embedding_str = \",\".join(map(str, embedding))\n",
    "    #     cursor.execute(\n",
    "    #         f\"INSERT INTO {self.embeddings_table} (id, restaurant_name, chunk, embedding) VALUES (?, ?, ?, ?);\",\n",
    "    #         (str(uuid.uuid4()), restaurant_name, chunk, embedding_str),\n",
    "    #     )\n",
    "    #     conn.commit()\n",
    "    #     conn.close()\n",
    "\n",
    "    # def process_reviews(self) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Process each review as a single chunk, generate embeddings, and store them in the database.\n",
    "\n",
    "    #     This method:\n",
    "    #     1. Fetches reviews and restaurant names from the SQLite database.\n",
    "    #     2. Generates embeddings for each review.\n",
    "    #     3. Stores the reviews and embeddings in the SQLite database.\n",
    "    #     \"\"\"\n",
    "    #     data = self.fetch_reviews_from_db()\n",
    "\n",
    "    #     for restaurant_name, review in tqdm(data, desc=\"Processing Reviews\"):\n",
    "    #         embedding = self.generate_fake_embedding(review)  # Generate embedding for the review (chunk)\n",
    "    #         self.store_chunk_in_db(restaurant_name, review, embedding)\n",
    "\n",
    "\n",
    "\n",
    "bdd_chunks = BDDChunksSQLite()\n",
    "restaurants = bdd_chunks.get_all_restaurants_names()\n",
    "restaurants\n",
    "# bdd_chunks.process_reviews()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import os\n",
    "import uuid\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "class BDDChunksSQLite:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    \n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self ):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunksSQLite instance.\n",
    "\n",
    "        Args:\n",
    "            sqlite_db_path (str): Path to the SQLite database file.\n",
    "            reviews_table (str): Name of the table containing reviews and restaurant information.\n",
    "            embeddings_table (str): Name of the table where embeddings will be stored.\n",
    "        \"\"\"\n",
    " \n",
    "        self.db = self.get_db()\n",
    "\n",
    "    # Database dependency\n",
    "    def get_db():\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "    \n",
    "\n",
    "    def fetch_reviews_from_db(self) -> list[tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Fetch reviews and their associated restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[tuple[str, str]]: A list of tuples containing restaurant names and reviews.\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.sqlite_db_path)\n",
    "        cursor = conn.cursor()\n",
    "        query = f\"SELECT restaurant_name, review FROM {self.reviews_table};\"\n",
    "        cursor.execute(query)\n",
    "        data = cursor.fetchall()\n",
    "        conn.close()\n",
    "        return data\n",
    "\n",
    "    def generate_fake_embedding(self, text: str) -> list[float]:\n",
    "        \"\"\"\n",
    "        Generate a fake embedding for a given text. Replace this with your embedding logic.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text for which to generate an embedding.\n",
    "\n",
    "        Returns:\n",
    "            list[float]: A list representing the embedding vector.\n",
    "        \"\"\"\n",
    "        # Example: Return the length of each word in the text as a fake embedding.\n",
    "        return [len(word) for word in text.split()]\n",
    "\n",
    "    def store_chunk_in_db(self, restaurant_name: str, chunk: str, embedding: list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Store a chunk and its embedding in the SQLite database.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the associated restaurant.\n",
    "            chunk (str): The text chunk (in this case, the full review).\n",
    "            embedding (list[float]): The embedding vector for the chunk.\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.sqlite_db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Ensure the embeddings table exists\n",
    "        cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {self.embeddings_table} (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            restaurant_name TEXT,\n",
    "            chunk TEXT,\n",
    "            embedding TEXT\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        embedding_str = \",\".join(map(str, embedding))\n",
    "        cursor.execute(\n",
    "            f\"INSERT INTO {self.embeddings_table} (id, restaurant_name, chunk, embedding) VALUES (?, ?, ?, ?);\",\n",
    "            (str(uuid.uuid4()), restaurant_name, chunk, embedding_str),\n",
    "        )\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def process_reviews(self) -> None:\n",
    "        \"\"\"\n",
    "        Process each review as a single chunk, generate embeddings, and store them in the database.\n",
    "\n",
    "        This method:\n",
    "        1. Fetches reviews and restaurant names from the SQLite database.\n",
    "        2. Generates embeddings for each review.\n",
    "        3. Stores the reviews and embeddings in the SQLite database.\n",
    "        \"\"\"\n",
    "        data = self.fetch_reviews_from_db()\n",
    "\n",
    "        for restaurant_name, review in tqdm(data, desc=\"Processing Reviews\"):\n",
    "            embedding = self.generate_fake_embedding(review)  # Generate embedding for the review (chunk)\n",
    "            self.store_chunk_in_db(restaurant_name, review, embedding)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sqlite_db_path = \"path/to/your/database.sqlite\"  # Path to your SQLite DB\n",
    "reviews_table = \"reviews\"  # Table containing reviews\n",
    "embeddings_table = \"embeddings\"  # Table to store embeddings\n",
    "\n",
    "bdd_chunks = BDDChunksSQLite(sqlite_db_path, reviews_table, embeddings_table)\n",
    "bdd_chunks.process_reviews()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m embeddings_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Table to store embeddings\u001b[39;00m\n\u001b[0;32m    106\u001b[0m bdd_chunks \u001b[38;5;241m=\u001b[39m BDDChunksSQLite(sqlite_db_path, reviews_table, embeddings_table)\n\u001b[1;32m--> 107\u001b[0m \u001b[43mbdd_chunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 94\u001b[0m, in \u001b[0;36mBDDChunksSQLite.process_reviews\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_reviews\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    Process each review as a single chunk, generate embeddings, and store them in the database.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m    3. Stores the reviews and embeddings in the SQLite database.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_reviews_from_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m restaurant_name, review \u001b[38;5;129;01min\u001b[39;00m tqdm(data, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m         embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_fake_embedding(review)  \u001b[38;5;66;03m# Generate embedding for the review (chunk)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 34\u001b[0m, in \u001b[0;36mBDDChunksSQLite.fetch_reviews_from_db\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_reviews_from_db\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    Fetch reviews and their associated restaurant names from the SQLite database.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m        list[tuple[str, str]]: A list of tuples containing restaurant names and reviews.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43msqlite3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqlite_db_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m     36\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT restaurant_name, review FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreviews_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class BDDChunksSQLite:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    \n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sqlite_db_path: str, reviews_table: str, embeddings_table: str):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunksSQLite instance.\n",
    "\n",
    "        Args:\n",
    "            sqlite_db_path (str): Path to the SQLite database file.\n",
    "            reviews_table (str): Name of the table containing reviews and restaurant information.\n",
    "            embeddings_table (str): Name of the table where embeddings will be stored.\n",
    "        \"\"\"\n",
    "        self.sqlite_db_path = sqlite_db_path\n",
    "        self.reviews_table = reviews_table\n",
    "        self.embeddings_table = embeddings_table\n",
    "\n",
    "    def fetch_reviews_from_db(self) -> list[tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Fetch reviews and their associated restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[tuple[str, str]]: A list of tuples containing restaurant names and reviews.\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.sqlite_db_path)\n",
    "        cursor = conn.cursor()\n",
    "        query = f\"SELECT restaurant_name, review FROM {self.reviews_table};\"\n",
    "        cursor.execute(query)\n",
    "        data = cursor.fetchall()\n",
    "        conn.close()\n",
    "        return data\n",
    "\n",
    "    def generate_fake_embedding(self, text: str) -> list[float]:\n",
    "        \"\"\"\n",
    "        Generate a fake embedding for a given text. Replace this with your embedding logic.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text for which to generate an embedding.\n",
    "\n",
    "        Returns:\n",
    "            list[float]: A list representing the embedding vector.\n",
    "        \"\"\"\n",
    "        # Example: Return the length of each word in the text as a fake embedding.\n",
    "        return [len(word) for word in text.split()]\n",
    "\n",
    "    def store_chunk_in_db(self, restaurant_name: str, chunk: str, embedding: list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Store a chunk and its embedding in the SQLite database.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the associated restaurant.\n",
    "            chunk (str): The text chunk (in this case, the full review).\n",
    "            embedding (list[float]): The embedding vector for the chunk.\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.sqlite_db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Ensure the embeddings table exists\n",
    "        cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {self.embeddings_table} (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            restaurant_name TEXT,\n",
    "            chunk TEXT,\n",
    "            embedding TEXT\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        embedding_str = \",\".join(map(str, embedding))\n",
    "        cursor.execute(\n",
    "            f\"INSERT INTO {self.embeddings_table} (id, restaurant_name, chunk, embedding) VALUES (?, ?, ?, ?);\",\n",
    "            (str(uuid.uuid4()), restaurant_name, chunk, embedding_str),\n",
    "        )\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def process_reviews(self) -> None:\n",
    "        \"\"\"\n",
    "        Process each review as a single chunk, generate embeddings, and store them in the database.\n",
    "\n",
    "        This method:\n",
    "        1. Fetches reviews and restaurant names from the SQLite database.\n",
    "        2. Generates embeddings for each review.\n",
    "        3. Stores the reviews and embeddings in the SQLite database.\n",
    "        \"\"\"\n",
    "        data = self.fetch_reviews_from_db()\n",
    "\n",
    "        for restaurant_name, review in tqdm(data, desc=\"Processing Reviews\"):\n",
    "            embedding = self.generate_fake_embedding(review)  # Generate embedding for the review (chunk)\n",
    "            self.store_chunk_in_db(restaurant_name, review, embedding)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sqlite_db_path = \"path/to/your/database.sqlite\"  # Path to your SQLite DB\n",
    "reviews_table = \"reviews\"  # Table containing reviews\n",
    "embeddings_table = \"embeddings\"  # Table to store embeddings\n",
    "\n",
    "bdd_chunks = BDDChunksSQLite(sqlite_db_path, reviews_table, embeddings_table)\n",
    "bdd_chunks.process_reviews()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
