{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.32it/s]\n",
      "\u001b[92m13:20:28 - LiteLLM:INFO\u001b[0m: utils.py:2802 - \n",
      "LiteLLM completion() model= ministral-8b-latest; provider = mistral\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= ministral-8b-latest; provider = mistral\n",
      "INFO:httpx:HTTP Request: POST https://api.mistral.ai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[92m13:20:30 - LiteLLM:INFO\u001b[0m: utils.py:949 - Wrapper: Completed Call, calling success_handler\n",
      "INFO:LiteLLM:Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'La Table de la Boucherie Bello est un restaurant situé à Lyon, en France, qui a été fondé par un ancien banquier reconverti en boucher. Le restaurant est connu pour sa qualité exceptionnelle de la viande, préparée par le boucher lui-même, ainsi que pour ses vins bio et son ambiance chaleureuse. Le chef et le boucher travaillent ensemble pour offrir des plats de qualité, et le service est souvent décrit comme attentionné et professionnel. Le restaurant est', 'latency': 0.15625, 'input_tokens': 1154, 'output_tokens': 100, 'llm': 'ministral-8b-latest'}\n"
     ]
    }
   ],
   "source": [
    "import litellm\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import  time\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# from .rag_simulation.corpus_ingestion import BDDChunks\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "\n",
    "class AugmentedRAG:\n",
    "    \"\"\"A class for performing a simple RAG process.\n",
    "\n",
    "    This class utilizes a retrieval process to fetch relevant information from a\n",
    "    database (or corpus) and then passes it to a generative model for further processing.\n",
    "\n",
    "    \"\"\"\n",
    "    HF_TOKEN = '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'\n",
    "    MISTRAL_API_KEY =  'dkMKu81kFgJeP7HmIqjztosQTxyiynW6'\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        generation_model: str,\n",
    "        role_prompt: str,\n",
    "        bdd_chunks: BDDChunks,\n",
    "        max_tokens: int,\n",
    "        temperature: int,\n",
    "        top_n: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the SimpleRAG class with the provided parameters.\n",
    "\n",
    "        Args:\n",
    "            generation_model (str): The model used for generating responses.\n",
    "            role_prompt (str): The role of the model as specified by the prompt.\n",
    "            bdd_chunks (Any): The database or chunks of information used in the retrieval process.\n",
    "            max_tokens (int): Maximum number of tokens to generate.\n",
    "            temperature (int): The temperature setting for the generative model.\n",
    "            top_n (int, optional): The number of top documents to retrieve. Defaults to 2.\n",
    "        \"\"\"\n",
    "        self.llm = generation_model\n",
    "        self.bdd = bdd_chunks\n",
    "        self.top_n = top_n\n",
    "        self.role_prompt = role_prompt\n",
    "        self.max_tokens = max_tokens\n",
    "        self.temperature = temperature\n",
    "        self.latency = 0.0\n",
    "        self.input_tokens = 0\n",
    "        self.output_tokens = 0\n",
    "        self.dollor_cost = 0.0\n",
    "\n",
    "    def get_cosim(self, a: NDArray[np.float32], b: NDArray[np.float32]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the cosine similarity between two vectors.\n",
    "\n",
    "        Args:\n",
    "            a (NDArray[np.float32]): The first vector.\n",
    "            b (NDArray[np.float32]): The second vector.\n",
    "\n",
    "        Returns:\n",
    "            float: The cosine similarity between the two vectors.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    \n",
    "    def get_top_similarity(\n",
    "            self,\n",
    "            embedding_query: NDArray[np.float32],\n",
    "            embedding_chunks: NDArray[np.float32],\n",
    "            corpus: list[str],\n",
    "        ) -> list[str]:\n",
    "            \"\"\"\n",
    "            Retrieves the top N most similar documents from the corpus based on the query's embedding.\n",
    "\n",
    "            Args:\n",
    "                embedding_query (NDArray[np.float32]): The embedding of the query.\n",
    "                embedding_chunks (NDArray[np.float32]): A NumPy array of embeddings for the documents in the corpus.\n",
    "                corpus (List[str]): A list of documents (strings) corresponding to the embeddings in `embedding_chunks`.\n",
    "                top_n (int, optional): The number of top similar documents to retrieve. Defaults to 5.\n",
    "\n",
    "            Returns:\n",
    "                List[str]: A list of the most similar documents from the corpus, ordered by similarity to the query.\n",
    "            \"\"\"\n",
    "            cos_dist_list = np.array(\n",
    "                [\n",
    "                    self.get_cosim(embedding_query, embed_doc)\n",
    "                    for embed_doc in embedding_chunks\n",
    "                ]\n",
    "            )\n",
    "            indices_of_max_values = np.argsort(cos_dist_list)[-self.top_n :][::-1]\n",
    "            print(indices_of_max_values)\n",
    "            return [corpus[i] for i in indices_of_max_values]\n",
    "\n",
    "\n",
    "    def build_prompt(\n",
    "        self, context: list[str], history: str, query: str\n",
    "    ) -> list[dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Builds a prompt string for a conversational agent based on the given context and query.\n",
    "\n",
    "        Args:\n",
    "            context (str): The context information, typically extracted from books or other sources.\n",
    "            query (str): The user's query or question.\n",
    "\n",
    "        Returns:\n",
    "            list[dict[str, str]]: The RAG prompt in the OpenAI format\n",
    "        \"\"\"\n",
    "        context_joined = \"\\n\".join(context)\n",
    "        system_prompt = self.role_prompt\n",
    "        history_prompt = f\"\"\"\n",
    "        # Historique de conversation:\n",
    "        {history}\n",
    "        \"\"\"\n",
    "        context_prompt = f\"\"\"\n",
    "        Tu disposes de la section \"Contexte\" pour t'aider à répondre aux questions.\n",
    "        # Contexte: \n",
    "        {context_joined}\n",
    "        \"\"\"\n",
    "        query_prompt = f\"\"\"\n",
    "        # Question:\n",
    "        {query}\n",
    "\n",
    "        # Réponse:\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"system\", \"content\": history_prompt},\n",
    "            {\"role\": \"system\", \"content\": context_prompt},\n",
    "            {\"role\": \"user\", \"content\": query_prompt},\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    def _generate(self, prompt_dict: list[dict[str, str]]) -> litellm.ModelResponse:\n",
    "\n",
    "\n",
    "         \n",
    "\n",
    "        response = litellm.completion(\n",
    "            model=f\"mistral/{self.llm}\",\n",
    "            messages=prompt_dict,\n",
    "            max_tokens=self.max_tokens,\n",
    "            temperature=self.temperature,\n",
    "        )  # type: ignore\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "    def call_model(self, prompt_dict: list[dict[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        Calls the LLM with the given prompt and returns the response.\n",
    "\n",
    "        Args:\n",
    "            prompt_dict (List[Dict[str, str]]): A list of dictionaries where each dictionary represents\n",
    "                                                a message prompt with a string key and string value.\n",
    "\n",
    "        Returns:\n",
    "            str: The response generated by the LLM.\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.process_time()\n",
    "        chat_response: str = self._generate(prompt_dict=prompt_dict)\n",
    "        end_time = time.process_time()\n",
    "        self.latency = end_time - start_time\n",
    "\n",
    "        self.input_tokens = chat_response.usage.prompt_tokens\n",
    "        self.output_tokens = chat_response.usage.completion_tokens\n",
    "    \n",
    "\n",
    "        dict_response = {\n",
    "            \"response\": chat_response.choices[0].message.content,\n",
    "            \"latency\": self.latency,\n",
    "            \"input_tokens\": self.input_tokens,\n",
    "            \"output_tokens\": self.output_tokens,\n",
    "            \"llm\": self.llm,           \n",
    "        }\n",
    "        return dict_response\n",
    "        # return str(chat_response.choices[0].message.content)\n",
    "\n",
    "\n",
    "    def __call__(self, query: str, history: dict[str, str]) -> str:\n",
    "        \"\"\"\n",
    "        Process a query and return a response based on the provided history and database.\n",
    "\n",
    "        This method performs the following steps:\n",
    "        1. Queries the ChromaDB instance to retrieve relevant documents based on the input query.\n",
    "        2. Constructs a prompt using the retrieved documents, the provided query, and the history.\n",
    "        3. Sends the prompt to the model for generating a response.\n",
    "\n",
    "        Args:\n",
    "            query (str): The user query to be processed.\n",
    "            history (dict[str, str]): A dictionary containing the conversation history,\n",
    "                where keys represent user inputs and values represent corresponding responses.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response from the model.\n",
    "        \"\"\"\n",
    "        chunks = self.bdd.chroma_db.query(\n",
    "            query_texts=[query],\n",
    "            n_results=self.top_n,\n",
    "        )\n",
    "        chunks_list: list[str] = chunks[\"documents\"][0]\n",
    "        prompt_rag = self.build_prompt(\n",
    "            context=chunks_list, history=str(history), query=query\n",
    "        )\n",
    "        response = self.call_model(prompt_dict=prompt_rag)\n",
    "        return response\n",
    "\n",
    "\n",
    "generation_model = \"ministral-8b-latest\"\n",
    "role_prompt = \"Tu es un assistant virtuel qui aide les utilisateurs à répondre à des questions.\"\n",
    "bdd_chunks = BDDChunks(embedding_model=\"paraphrase-xlm-r-multilingual-v1\", path=\"./\")\n",
    "max_tokens = 100\n",
    "temperature = 0.5\n",
    "\n",
    "# Initialize the SimpleRAG instance\n",
    "simple_rag = AugmentedRAG(\n",
    "    generation_model=generation_model,\n",
    "    role_prompt=role_prompt,\n",
    "    bdd_chunks=bdd_chunks,\n",
    "    max_tokens=max_tokens,\n",
    "    temperature=temperature,\n",
    "\n",
    ")\n",
    "\n",
    "# Define the conversation history\n",
    "history = {\n",
    "    \"user\": \"Quelle est la capitale de la France ?\",\n",
    "    \"bot\": \"La capitale de la France est Paris.\",\n",
    "}\n",
    "\n",
    "# Define the user query\n",
    "query = \"specialiter La Table de la Boucherie Bello\"\n",
    "bdd_chunks._create_collection(path=\"./\")\n",
    "\n",
    "# Generate a response using the SimpleRAG instance\n",
    "response = simple_rag(query=query, history=history)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 253\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;66;03m# chunks = self.split_text_into_chunks(corpus)\u001b[39;00m\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;66;03m# self._create_collection(self.path)\u001b[39;00m\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;66;03m# self.add_embeddings(chunks)\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m \n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# Test the class\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 253\u001b[0m     test \u001b[38;5;241m=\u001b[39m \u001b[43mBDDChunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparaphrase-xlm-r-multilingual-v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    254\u001b[0m     test()\n",
      "Cell \u001b[1;32mIn[1], line 37\u001b[0m, in \u001b[0;36mBDDChunks.__init__\u001b[1;34m(self, embedding_model, path)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m chromadb\u001b[38;5;241m.\u001b[39mPersistentClient(\n\u001b[0;32m     34\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ChromaDB\u001b[39m\u001b[38;5;124m\"\u001b[39m, settings\u001b[38;5;241m=\u001b[39mSettings(anonymized_telemetry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     35\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_name \u001b[38;5;241m=\u001b[39m embedding_model\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformerEmbeddingFunction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_model\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchroma_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     42\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\chromadb\\utils\\embedding_functions\\sentence_transformer_embedding_function.py:32\u001b[0m, in \u001b[0;36mSentenceTransformerEmbeddingFunction.__init__\u001b[1;34m(self, model_name, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 32\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     35\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe sentence_transformers python package is not installed. Please install it with `pip install sentence_transformers`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sentence_transformers\\__init__.py:14\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[0;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[0;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm, trange\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, is_torch_npu_available\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchEncoding\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\transformers\\utils\\import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1803\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\transformers\\utils\\import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1817\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1818\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1819\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1820\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1821\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1822\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\transformers\\models\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     albert,\n\u001b[0;32m     17\u001b[0m     align,\n\u001b[0;32m     18\u001b[0m     altclip,\n\u001b[0;32m     19\u001b[0m     aria,\n\u001b[0;32m     20\u001b[0m     audio_spectrogram_transformer,\n\u001b[0;32m     21\u001b[0m     auto,\n\u001b[0;32m     22\u001b[0m     autoformer,\n\u001b[0;32m     23\u001b[0m     bamba,\n\u001b[0;32m     24\u001b[0m     bark,\n\u001b[0;32m     25\u001b[0m     bart,\n\u001b[0;32m     26\u001b[0m     barthez,\n\u001b[0;32m     27\u001b[0m     bartpho,\n\u001b[0;32m     28\u001b[0m     beit,\n\u001b[0;32m     29\u001b[0m     bert,\n\u001b[0;32m     30\u001b[0m     bert_generation,\n\u001b[0;32m     31\u001b[0m     bert_japanese,\n\u001b[0;32m     32\u001b[0m     bertweet,\n\u001b[0;32m     33\u001b[0m     big_bird,\n\u001b[0;32m     34\u001b[0m     bigbird_pegasus,\n\u001b[0;32m     35\u001b[0m     biogpt,\n\u001b[0;32m     36\u001b[0m     bit,\n\u001b[0;32m     37\u001b[0m     blenderbot,\n\u001b[0;32m     38\u001b[0m     blenderbot_small,\n\u001b[0;32m     39\u001b[0m     blip,\n\u001b[0;32m     40\u001b[0m     blip_2,\n\u001b[0;32m     41\u001b[0m     bloom,\n\u001b[0;32m     42\u001b[0m     bridgetower,\n\u001b[0;32m     43\u001b[0m     bros,\n\u001b[0;32m     44\u001b[0m     byt5,\n\u001b[0;32m     45\u001b[0m     camembert,\n\u001b[0;32m     46\u001b[0m     canine,\n\u001b[0;32m     47\u001b[0m     chameleon,\n\u001b[0;32m     48\u001b[0m     chinese_clip,\n\u001b[0;32m     49\u001b[0m     clap,\n\u001b[0;32m     50\u001b[0m     clip,\n\u001b[0;32m     51\u001b[0m     clipseg,\n\u001b[0;32m     52\u001b[0m     clvp,\n\u001b[0;32m     53\u001b[0m     code_llama,\n\u001b[0;32m     54\u001b[0m     codegen,\n\u001b[0;32m     55\u001b[0m     cohere,\n\u001b[0;32m     56\u001b[0m     cohere2,\n\u001b[0;32m     57\u001b[0m     colpali,\n\u001b[0;32m     58\u001b[0m     conditional_detr,\n\u001b[0;32m     59\u001b[0m     convbert,\n\u001b[0;32m     60\u001b[0m     convnext,\n\u001b[0;32m     61\u001b[0m     convnextv2,\n\u001b[0;32m     62\u001b[0m     cpm,\n\u001b[0;32m     63\u001b[0m     cpmant,\n\u001b[0;32m     64\u001b[0m     ctrl,\n\u001b[0;32m     65\u001b[0m     cvt,\n\u001b[0;32m     66\u001b[0m     dac,\n\u001b[0;32m     67\u001b[0m     data2vec,\n\u001b[0;32m     68\u001b[0m     dbrx,\n\u001b[0;32m     69\u001b[0m     deberta,\n\u001b[0;32m     70\u001b[0m     deberta_v2,\n\u001b[0;32m     71\u001b[0m     decision_transformer,\n\u001b[0;32m     72\u001b[0m     deformable_detr,\n\u001b[0;32m     73\u001b[0m     deit,\n\u001b[0;32m     74\u001b[0m     deprecated,\n\u001b[0;32m     75\u001b[0m     depth_anything,\n\u001b[0;32m     76\u001b[0m     detr,\n\u001b[0;32m     77\u001b[0m     dialogpt,\n\u001b[0;32m     78\u001b[0m     diffllama,\n\u001b[0;32m     79\u001b[0m     dinat,\n\u001b[0;32m     80\u001b[0m     dinov2,\n\u001b[0;32m     81\u001b[0m     dinov2_with_registers,\n\u001b[0;32m     82\u001b[0m     distilbert,\n\u001b[0;32m     83\u001b[0m     dit,\n\u001b[0;32m     84\u001b[0m     donut,\n\u001b[0;32m     85\u001b[0m     dpr,\n\u001b[0;32m     86\u001b[0m     dpt,\n\u001b[0;32m     87\u001b[0m     efficientnet,\n\u001b[0;32m     88\u001b[0m     electra,\n\u001b[0;32m     89\u001b[0m     emu3,\n\u001b[0;32m     90\u001b[0m     encodec,\n\u001b[0;32m     91\u001b[0m     encoder_decoder,\n\u001b[0;32m     92\u001b[0m     ernie,\n\u001b[0;32m     93\u001b[0m     esm,\n\u001b[0;32m     94\u001b[0m     falcon,\n\u001b[0;32m     95\u001b[0m     falcon_mamba,\n\u001b[0;32m     96\u001b[0m     fastspeech2_conformer,\n\u001b[0;32m     97\u001b[0m     flaubert,\n\u001b[0;32m     98\u001b[0m     flava,\n\u001b[0;32m     99\u001b[0m     fnet,\n\u001b[0;32m    100\u001b[0m     focalnet,\n\u001b[0;32m    101\u001b[0m     fsmt,\n\u001b[0;32m    102\u001b[0m     funnel,\n\u001b[0;32m    103\u001b[0m     fuyu,\n\u001b[0;32m    104\u001b[0m     gemma,\n\u001b[0;32m    105\u001b[0m     gemma2,\n\u001b[0;32m    106\u001b[0m     git,\n\u001b[0;32m    107\u001b[0m     glm,\n\u001b[0;32m    108\u001b[0m     glpn,\n\u001b[0;32m    109\u001b[0m     gpt2,\n\u001b[0;32m    110\u001b[0m     gpt_bigcode,\n\u001b[0;32m    111\u001b[0m     gpt_neo,\n\u001b[0;32m    112\u001b[0m     gpt_neox,\n\u001b[0;32m    113\u001b[0m     gpt_neox_japanese,\n\u001b[0;32m    114\u001b[0m     gpt_sw3,\n\u001b[0;32m    115\u001b[0m     gptj,\n\u001b[0;32m    116\u001b[0m     granite,\n\u001b[0;32m    117\u001b[0m     granitemoe,\n\u001b[0;32m    118\u001b[0m     grounding_dino,\n\u001b[0;32m    119\u001b[0m     groupvit,\n\u001b[0;32m    120\u001b[0m     herbert,\n\u001b[0;32m    121\u001b[0m     hiera,\n\u001b[0;32m    122\u001b[0m     hubert,\n\u001b[0;32m    123\u001b[0m     ibert,\n\u001b[0;32m    124\u001b[0m     idefics,\n\u001b[0;32m    125\u001b[0m     idefics2,\n\u001b[0;32m    126\u001b[0m     idefics3,\n\u001b[0;32m    127\u001b[0m     ijepa,\n\u001b[0;32m    128\u001b[0m     imagegpt,\n\u001b[0;32m    129\u001b[0m     informer,\n\u001b[0;32m    130\u001b[0m     instructblip,\n\u001b[0;32m    131\u001b[0m     instructblipvideo,\n\u001b[0;32m    132\u001b[0m     jamba,\n\u001b[0;32m    133\u001b[0m     jetmoe,\n\u001b[0;32m    134\u001b[0m     kosmos2,\n\u001b[0;32m    135\u001b[0m     layoutlm,\n\u001b[0;32m    136\u001b[0m     layoutlmv2,\n\u001b[0;32m    137\u001b[0m     layoutlmv3,\n\u001b[0;32m    138\u001b[0m     layoutxlm,\n\u001b[0;32m    139\u001b[0m     led,\n\u001b[0;32m    140\u001b[0m     levit,\n\u001b[0;32m    141\u001b[0m     lilt,\n\u001b[0;32m    142\u001b[0m     llama,\n\u001b[0;32m    143\u001b[0m     llava,\n\u001b[0;32m    144\u001b[0m     llava_next,\n\u001b[0;32m    145\u001b[0m     llava_next_video,\n\u001b[0;32m    146\u001b[0m     llava_onevision,\n\u001b[0;32m    147\u001b[0m     longformer,\n\u001b[0;32m    148\u001b[0m     longt5,\n\u001b[0;32m    149\u001b[0m     luke,\n\u001b[0;32m    150\u001b[0m     lxmert,\n\u001b[0;32m    151\u001b[0m     m2m_100,\n\u001b[0;32m    152\u001b[0m     mamba,\n\u001b[0;32m    153\u001b[0m     mamba2,\n\u001b[0;32m    154\u001b[0m     marian,\n\u001b[0;32m    155\u001b[0m     markuplm,\n\u001b[0;32m    156\u001b[0m     mask2former,\n\u001b[0;32m    157\u001b[0m     maskformer,\n\u001b[0;32m    158\u001b[0m     mbart,\n\u001b[0;32m    159\u001b[0m     mbart50,\n\u001b[0;32m    160\u001b[0m     megatron_bert,\n\u001b[0;32m    161\u001b[0m     megatron_gpt2,\n\u001b[0;32m    162\u001b[0m     mgp_str,\n\u001b[0;32m    163\u001b[0m     mimi,\n\u001b[0;32m    164\u001b[0m     mistral,\n\u001b[0;32m    165\u001b[0m     mixtral,\n\u001b[0;32m    166\u001b[0m     mllama,\n\u001b[0;32m    167\u001b[0m     mluke,\n\u001b[0;32m    168\u001b[0m     mobilebert,\n\u001b[0;32m    169\u001b[0m     mobilenet_v1,\n\u001b[0;32m    170\u001b[0m     mobilenet_v2,\n\u001b[0;32m    171\u001b[0m     mobilevit,\n\u001b[0;32m    172\u001b[0m     mobilevitv2,\n\u001b[0;32m    173\u001b[0m     modernbert,\n\u001b[0;32m    174\u001b[0m     moonshine,\n\u001b[0;32m    175\u001b[0m     moshi,\n\u001b[0;32m    176\u001b[0m     mpnet,\n\u001b[0;32m    177\u001b[0m     mpt,\n\u001b[0;32m    178\u001b[0m     mra,\n\u001b[0;32m    179\u001b[0m     mt5,\n\u001b[0;32m    180\u001b[0m     musicgen,\n\u001b[0;32m    181\u001b[0m     musicgen_melody,\n\u001b[0;32m    182\u001b[0m     mvp,\n\u001b[0;32m    183\u001b[0m     myt5,\n\u001b[0;32m    184\u001b[0m     nemotron,\n\u001b[0;32m    185\u001b[0m     nllb,\n\u001b[0;32m    186\u001b[0m     nllb_moe,\n\u001b[0;32m    187\u001b[0m     nougat,\n\u001b[0;32m    188\u001b[0m     nystromformer,\n\u001b[0;32m    189\u001b[0m     olmo,\n\u001b[0;32m    190\u001b[0m     olmo2,\n\u001b[0;32m    191\u001b[0m     olmoe,\n\u001b[0;32m    192\u001b[0m     omdet_turbo,\n\u001b[0;32m    193\u001b[0m     oneformer,\n\u001b[0;32m    194\u001b[0m     openai,\n\u001b[0;32m    195\u001b[0m     opt,\n\u001b[0;32m    196\u001b[0m     owlv2,\n\u001b[0;32m    197\u001b[0m     owlvit,\n\u001b[0;32m    198\u001b[0m     paligemma,\n\u001b[0;32m    199\u001b[0m     patchtsmixer,\n\u001b[0;32m    200\u001b[0m     patchtst,\n\u001b[0;32m    201\u001b[0m     pegasus,\n\u001b[0;32m    202\u001b[0m     pegasus_x,\n\u001b[0;32m    203\u001b[0m     perceiver,\n\u001b[0;32m    204\u001b[0m     persimmon,\n\u001b[0;32m    205\u001b[0m     phi,\n\u001b[0;32m    206\u001b[0m     phi3,\n\u001b[0;32m    207\u001b[0m     phimoe,\n\u001b[0;32m    208\u001b[0m     phobert,\n\u001b[0;32m    209\u001b[0m     pix2struct,\n\u001b[0;32m    210\u001b[0m     pixtral,\n\u001b[0;32m    211\u001b[0m     plbart,\n\u001b[0;32m    212\u001b[0m     poolformer,\n\u001b[0;32m    213\u001b[0m     pop2piano,\n\u001b[0;32m    214\u001b[0m     prophetnet,\n\u001b[0;32m    215\u001b[0m     pvt,\n\u001b[0;32m    216\u001b[0m     pvt_v2,\n\u001b[0;32m    217\u001b[0m     qwen2,\n\u001b[0;32m    218\u001b[0m     qwen2_audio,\n\u001b[0;32m    219\u001b[0m     qwen2_moe,\n\u001b[0;32m    220\u001b[0m     qwen2_vl,\n\u001b[0;32m    221\u001b[0m     rag,\n\u001b[0;32m    222\u001b[0m     recurrent_gemma,\n\u001b[0;32m    223\u001b[0m     reformer,\n\u001b[0;32m    224\u001b[0m     regnet,\n\u001b[0;32m    225\u001b[0m     rembert,\n\u001b[0;32m    226\u001b[0m     resnet,\n\u001b[0;32m    227\u001b[0m     roberta,\n\u001b[0;32m    228\u001b[0m     roberta_prelayernorm,\n\u001b[0;32m    229\u001b[0m     roc_bert,\n\u001b[0;32m    230\u001b[0m     roformer,\n\u001b[0;32m    231\u001b[0m     rt_detr,\n\u001b[0;32m    232\u001b[0m     rwkv,\n\u001b[0;32m    233\u001b[0m     sam,\n\u001b[0;32m    234\u001b[0m     seamless_m4t,\n\u001b[0;32m    235\u001b[0m     seamless_m4t_v2,\n\u001b[0;32m    236\u001b[0m     segformer,\n\u001b[0;32m    237\u001b[0m     seggpt,\n\u001b[0;32m    238\u001b[0m     sew,\n\u001b[0;32m    239\u001b[0m     sew_d,\n\u001b[0;32m    240\u001b[0m     siglip,\n\u001b[0;32m    241\u001b[0m     speech_encoder_decoder,\n\u001b[0;32m    242\u001b[0m     speech_to_text,\n\u001b[0;32m    243\u001b[0m     speecht5,\n\u001b[0;32m    244\u001b[0m     splinter,\n\u001b[0;32m    245\u001b[0m     squeezebert,\n\u001b[0;32m    246\u001b[0m     stablelm,\n\u001b[0;32m    247\u001b[0m     starcoder2,\n\u001b[0;32m    248\u001b[0m     superpoint,\n\u001b[0;32m    249\u001b[0m     swiftformer,\n\u001b[0;32m    250\u001b[0m     swin,\n\u001b[0;32m    251\u001b[0m     swin2sr,\n\u001b[0;32m    252\u001b[0m     swinv2,\n\u001b[0;32m    253\u001b[0m     switch_transformers,\n\u001b[0;32m    254\u001b[0m     t5,\n\u001b[0;32m    255\u001b[0m     table_transformer,\n\u001b[0;32m    256\u001b[0m     tapas,\n\u001b[0;32m    257\u001b[0m     textnet,\n\u001b[0;32m    258\u001b[0m     time_series_transformer,\n\u001b[0;32m    259\u001b[0m     timesformer,\n\u001b[0;32m    260\u001b[0m     timm_backbone,\n\u001b[0;32m    261\u001b[0m     timm_wrapper,\n\u001b[0;32m    262\u001b[0m     trocr,\n\u001b[0;32m    263\u001b[0m     tvp,\n\u001b[0;32m    264\u001b[0m     udop,\n\u001b[0;32m    265\u001b[0m     umt5,\n\u001b[0;32m    266\u001b[0m     unispeech,\n\u001b[0;32m    267\u001b[0m     unispeech_sat,\n\u001b[0;32m    268\u001b[0m     univnet,\n\u001b[0;32m    269\u001b[0m     upernet,\n\u001b[0;32m    270\u001b[0m     video_llava,\n\u001b[0;32m    271\u001b[0m     videomae,\n\u001b[0;32m    272\u001b[0m     vilt,\n\u001b[0;32m    273\u001b[0m     vipllava,\n\u001b[0;32m    274\u001b[0m     vision_encoder_decoder,\n\u001b[0;32m    275\u001b[0m     vision_text_dual_encoder,\n\u001b[0;32m    276\u001b[0m     visual_bert,\n\u001b[0;32m    277\u001b[0m     vit,\n\u001b[0;32m    278\u001b[0m     vit_mae,\n\u001b[0;32m    279\u001b[0m     vit_msn,\n\u001b[0;32m    280\u001b[0m     vitdet,\n\u001b[0;32m    281\u001b[0m     vitmatte,\n\u001b[0;32m    282\u001b[0m     vitpose,\n\u001b[0;32m    283\u001b[0m     vitpose_backbone,\n\u001b[0;32m    284\u001b[0m     vits,\n\u001b[0;32m    285\u001b[0m     vivit,\n\u001b[0;32m    286\u001b[0m     wav2vec2,\n\u001b[0;32m    287\u001b[0m     wav2vec2_bert,\n\u001b[0;32m    288\u001b[0m     wav2vec2_conformer,\n\u001b[0;32m    289\u001b[0m     wav2vec2_phoneme,\n\u001b[0;32m    290\u001b[0m     wav2vec2_with_lm,\n\u001b[0;32m    291\u001b[0m     wavlm,\n\u001b[0;32m    292\u001b[0m     whisper,\n\u001b[0;32m    293\u001b[0m     x_clip,\n\u001b[0;32m    294\u001b[0m     xglm,\n\u001b[0;32m    295\u001b[0m     xlm,\n\u001b[0;32m    296\u001b[0m     xlm_roberta,\n\u001b[0;32m    297\u001b[0m     xlm_roberta_xl,\n\u001b[0;32m    298\u001b[0m     xlnet,\n\u001b[0;32m    299\u001b[0m     xmod,\n\u001b[0;32m    300\u001b[0m     yolos,\n\u001b[0;32m    301\u001b[0m     yoso,\n\u001b[0;32m    302\u001b[0m     zamba,\n\u001b[0;32m    303\u001b[0m     zoedepth,\n\u001b[0;32m    304\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\transformers\\models\\dinat\\__init__.py:27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     26\u001b[0m _file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 27\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m _LazyModule(\u001b[38;5;18m__name__\u001b[39m, _file, \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_file\u001b[49m\u001b[43m)\u001b[49m, module_spec\u001b[38;5;241m=\u001b[39m__spec__)\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\transformers\\utils\\import_utils.py:2236\u001b[0m, in \u001b[0;36mdefine_import_structure\u001b[1;34m(module_path)\u001b[0m\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IMPORT_STRUCTURE_T:\n\u001b[0;32m   2216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;124;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[0;32m   2218\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2234\u001b[0m \u001b[38;5;124;03m    The import structure is a dict defined with frozensets as keys, and dicts of strings to sets of objects.\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2236\u001b[0m     import_structure \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m spread_import_structure(import_structure)\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\transformers\\utils\\import_utils.py:2006\u001b[0m, in \u001b[0;36mcreate_import_structure_from_path\u001b[1;34m(module_path)\u001b[0m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m module_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2004\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 2006\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   2007\u001b[0m     file_content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   2009\u001b[0m \u001b[38;5;66;03m# Remove the .py suffix\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\codecs.py:309\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.__init__\u001b[1;34m(self, errors)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBufferedIncrementalDecoder\u001b[39;00m(IncrementalDecoder):\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    This subclass of IncrementalDecoder can be used as the baseclass for an\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    incremental decoder if the decoder must be able to handle incomplete\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m    byte sequences.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    310\u001b[0m         IncrementalDecoder\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, errors)\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;66;03m# undecoded input that is kept between calls to decode()\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from chromadb.config import Settings\n",
    "import logging\n",
    "\n",
    "# Initialize tiktoken encoding\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "class BDDChunks:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str, path: str):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunks instance.\n",
    "\n",
    "        Args:\n",
    "            embedding_model (str): Name of the embedding model to use.\n",
    "            path (str): Path to the PDF or dataset to process.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.chunks: list[str] | None = None\n",
    "        self.client = chromadb.PersistentClient(\n",
    "            path=\"./ChromaDB\", settings=Settings(anonymized_telemetry=False)\n",
    "        )\n",
    "        self.embedding_name = embedding_model\n",
    "        self.embeddings = SentenceTransformerEmbeddingFunction(\n",
    "            model_name=embedding_model\n",
    "        )\n",
    "        self.chroma_db = None\n",
    "        \n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Provide a database session.\n",
    "\n",
    "        Yields:\n",
    "            db: Database session instance.\n",
    "        \"\"\"\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "\n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of restaurant names.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurants = db.query(models.DimRestaurant).all()\n",
    "                return [r.nom for r in restaurants]\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred while fetching restaurant names: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _sanitize_collection_name(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Sanitize collection name to meet ChromaDB requirements:\n",
    "        - 3-63 characters\n",
    "        - Alphanumeric with hyphens and underscores\n",
    "        - No consecutive periods\n",
    "        \"\"\"\n",
    "        # Replace invalid characters with hyphens\n",
    "        sanitized = re.sub(r'[^a-zA-Z0-9-_]', '-', name)\n",
    "        \n",
    "        # Ensure name starts and ends with alphanumeric\n",
    "        sanitized = re.sub(r'^[^a-zA-Z0-9]+', '', sanitized)\n",
    "        sanitized = re.sub(r'[^a-zA-Z0-9]+$', '', sanitized)\n",
    "        \n",
    "        # Remove consecutive periods\n",
    "        sanitized = re.sub(r'\\.{2,}', '.', sanitized)\n",
    "        \n",
    "        # Ensure minimum length\n",
    "        if len(sanitized) < 3:\n",
    "            sanitized = sanitized + \"000\"[:3-len(sanitized)]\n",
    "            \n",
    "        # Truncate if too long\n",
    "        if len(sanitized) > 63:\n",
    "            sanitized = sanitized[:63]\n",
    "            \n",
    "        return sanitized\n",
    "\n",
    "    def _create_collection(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Create a new ChromaDB collection to store embeddings.\n",
    "\n",
    "        Args:\n",
    "            path (str): Name of the collection to create in ChromaDB.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create a valid collection name\n",
    "            collection_name = self._sanitize_collection_name(path)\n",
    "            self.chroma_db = self.client.get_or_create_collection(\n",
    "                name=collection_name,\n",
    "                embedding_function=self.embeddings,\n",
    "                metadata={\"hnsw:space\": \"cosine\"}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error creating collection: {e}\")\n",
    "            raise\n",
    "\n",
    "    def convert_to_arrow_compatible(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert a DataFrame to an Arrow-compatible format.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Converted DataFrame.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == 'object':\n",
    "                df[column] = df[column].astype(str)\n",
    "            elif df[column].dtype == 'int64':\n",
    "                df[column] = df[column].astype('int32')\n",
    "        return df\n",
    "\n",
    "    def get_restaurant_reviews_location(self, restaurant_name: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get the location and reviews for a specific restaurant.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): Name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            tuple: DataFrames for reviews, location, and restaurant info.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurant = db.query(models.DimRestaurant).filter(models.DimRestaurant.nom == restaurant_name).first()\n",
    "                location = db.query(models.DimLocation).filter(models.DimLocation.id_location == restaurant.id_location).first()\n",
    "                avis = db.query(models.FaitAvis).filter(models.FaitAvis.id_restaurant == restaurant.id_restaurant).all()\n",
    "            \n",
    "            avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
    "            location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
    "            restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
    "            \n",
    "            return avis_df, location_df, restaurant_df\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"An error occurred while fetching reviews and locations: {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    def transform_restaurant_chunk(self, restaurant_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform restaurant data and reviews into structured chunks.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): Name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing the chunks.\n",
    "        \"\"\"\n",
    "        colnames = ['restaurant', 'chunk']\n",
    "        avis_df, location_df, restaurant_df = self.get_restaurant_reviews_location(restaurant_name)\n",
    "        \n",
    "        if restaurant_df.empty:\n",
    "            return pd.DataFrame(columns=colnames)\n",
    "\n",
    "        chunks = []\n",
    "        for column in restaurant_df.columns:\n",
    "            value = restaurant_df[column].iloc[0]\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': f\"{column}: {value}\"})\n",
    "\n",
    "        # Include reviews as chunks\n",
    "        for _, review in avis_df.iterrows():\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': f\"Review: {review['review']}\"})\n",
    "\n",
    "        return pd.DataFrame(chunks, columns=colnames)\n",
    "\n",
    "    def create_corpus(self) -> str:\n",
    "        \"\"\"\n",
    "        Create a corpus from restaurant chunks.\n",
    "\n",
    "        Returns:\n",
    "            str: Text corpus.\n",
    "        \"\"\"\n",
    "        corpus = \"\"\n",
    "        for restaurant in self.get_all_restaurants_names():\n",
    "            df = self.transform_restaurant_chunk(restaurant)\n",
    "            corpus += \" \".join(df['chunk'].values) + \" \"\n",
    "        return corpus\n",
    "\n",
    "    def split_text_into_chunks(self, corpus: str, chunk_size: int = 500) -> list[str]:\n",
    "        \"\"\"\n",
    "        Split text into chunks of specified size.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): Text to split.\n",
    "            chunk_size (int, optional): Size of each chunk.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of chunks.\n",
    "        \"\"\"\n",
    "        tokenized_corpus = enc.encode(corpus)\n",
    "        chunks = [\n",
    "            \"\".join(enc.decode(tokenized_corpus[i : i + chunk_size]))\n",
    "            for i in tqdm(range(0, len(tokenized_corpus), chunk_size))\n",
    "        ]\n",
    "        return chunks\n",
    "\n",
    "    def add_embeddings(self, list_chunks: list[str], batch_size: int = 100) -> None:\n",
    "        \"\"\"\n",
    "        Add embeddings to the ChromaDB collection.\n",
    "\n",
    "        Args:\n",
    "            list_chunks (list[str]): List of chunks.\n",
    "            batch_size (int, optional): Batch size.\n",
    "        \"\"\"\n",
    "        if self.chroma_db is None:\n",
    "            raise ValueError(\"ChromaDB collection is not initialized. Call `_create_collection` first.\")\n",
    "        \n",
    "        if len(list_chunks) < batch_size:\n",
    "            batch_size_for_chromadb = len(list_chunks)\n",
    "        else:\n",
    "            batch_size_for_chromadb = batch_size\n",
    "\n",
    "        for i in tqdm(range(0, len(list_chunks), batch_size_for_chromadb)):\n",
    "            batch_documents = list_chunks[i : i + batch_size_for_chromadb]\n",
    "            list_ids = [str(uuid.uuid4()) for _ in batch_documents]\n",
    "            self.chroma_db.add(documents=batch_documents, ids=list_ids)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Entry point to execute class methods.\n",
    "        \"\"\"\n",
    "        # corpus = self.create_corpus()\n",
    "        self._create_collection\n",
    "        # chunks = self.split_text_into_chunks(corpus)\n",
    "        # self._create_collection(self.path)\n",
    "        # self.add_embeddings(chunks)\n",
    "\n",
    "\n",
    "# Test the class\n",
    "if __name__ == \"__main__\":\n",
    "    test = BDDChunks(embedding_model=\"paraphrase-xlm-r-multilingual-v1\", path=\"./\")\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2322011855.py:120: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2322011855.py:120: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2322011855.py:121: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2322011855.py:121: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2322011855.py:122: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2322011855.py:122: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
      "100%|██████████| 3638/3638 [00:00<00:00, 14257.46it/s]\n",
      "ERROR:__main__:Error creating collection: 'BDDChunks' object has no attribute '_sanitize_collection_name'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BDDChunks' object has no attribute '_sanitize_collection_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 221\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    220\u001b[0m     test \u001b[38;5;241m=\u001b[39m BDDChunks(embedding_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparaphrase-xlm-r-multilingual-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 221\u001b[0m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[132], line 214\u001b[0m, in \u001b[0;36mBDDChunks.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_corpus()\n\u001b[0;32m    213\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_text_into_chunks(corpus)\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_embeddings(chunks)\n",
      "Cell \u001b[1;32mIn[132], line 76\u001b[0m, in \u001b[0;36mBDDChunks._create_collection\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create ChromaDB collection with sanitized name.\"\"\"\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     collection_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_collection_name\u001b[49m(name)\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchroma_db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mget_or_create_collection(\n\u001b[0;32m     78\u001b[0m         name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[0;32m     79\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings,\n\u001b[0;32m     80\u001b[0m         metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhnsw:space\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     81\u001b[0m     )\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BDDChunks' object has no attribute '_sanitize_collection_name'"
     ]
    }
   ],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# Initialiser l'encodage tiktoken\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "class BDDChunks:\n",
    "    \"\"\"\n",
    "    Une classe pour traiter des avis issus d'une base SQLite et les stocker sous forme de chunks avec embeddings.\n",
    "    Chaque avis est considéré comme un chunk unique.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str, path: str):\n",
    "        \"\"\"\n",
    "        Initialisation de l'instance BDDChunks.\n",
    "\n",
    "        Args:\n",
    "            embedding_model (str): Nom du modèle d'embedding à utiliser.\n",
    "            path (str): Chemin vers le PDF ou le dataset à traiter.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.chunks: list[str] | None = None\n",
    "        self.client = chromadb.PersistentClient(\n",
    "            path=\"./ChromaDB\", settings=Settings(anonymized_telemetry=False)\n",
    "        )\n",
    "        self.embedding_name = embedding_model\n",
    "        self.embeddings = SentenceTransformerEmbeddingFunction(\n",
    "            model_name=embedding_model\n",
    "        )\n",
    "        self.chroma_db = None\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Fournit une session de base de données.\n",
    "\n",
    "        Yields:\n",
    "            db: Instance de session de base de données.\n",
    "        \"\"\"\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "\n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Récupère tous les noms de restaurants depuis la base SQLite.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: Liste des noms de restaurants.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurants = db.query(models.DimRestaurant).all()\n",
    "                return [r.nom for r in restaurants]\n",
    "        except Exception as e:\n",
    "            print(f\"Une erreur s'est produite lors de la récupération des noms de restaurants : {e}\")\n",
    "            return []\n",
    "\n",
    "    def _create_collection(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Crée une nouvelle collection ChromaDB pour stocker les embeddings.\n",
    "\n",
    "        Args:\n",
    "            path (str): Nom de la collection à créer dans ChromaDB.\n",
    "        \"\"\"\n",
    "        # Crée un nom de collection valide\n",
    "        file_name = \"a\" + os.path.basename(path)[0:50].strip() + \"a\"\n",
    "        file_name = re.sub(r\"\\s+\", \"-\", file_name)\n",
    "\n",
    "        # Initialiser la collection ChromaDB\n",
    "        self.chroma_db = self.client.get_or_create_collection(\n",
    "            name=file_name,\n",
    "            embedding_function=self.embeddings,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "\n",
    "    def convert_to_arrow_compatible(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convertit un DataFrame au format compatible avec Arrow.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame d'entrée.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame converti.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == 'object':\n",
    "                df[column] = df[column].astype(str)\n",
    "            elif df[column].dtype == 'int64':\n",
    "                df[column] = df[column].astype('int32')\n",
    "        return df\n",
    "\n",
    "    def get_restaurant_reviews_location(self, restaurant_name: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Récupère l'emplacement et les avis pour un restaurant donné.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): Nom du restaurant.\n",
    "\n",
    "        Returns:\n",
    "            tuple: DataFrames pour les avis, l'emplacement et les infos du restaurant.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurant = db.query(models.DimRestaurant).filter(models.DimRestaurant.nom == restaurant_name).first()\n",
    "                location = db.query(models.DimLocation).filter(models.DimLocation.id_location == restaurant.id_location).first()\n",
    "                avis = db.query(models.FaitAvis).filter(models.FaitAvis.id_restaurant == restaurant.id_restaurant).all()\n",
    "            \n",
    "            avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
    "            location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
    "            restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
    "            \n",
    "            return avis_df, location_df, restaurant_df\n",
    "        except Exception as e:\n",
    "            print(f\"Une erreur s'est produite lors de la récupération des avis et emplacements : {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    def transform_restaurant_chunk(self, restaurant_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transforme les données d'un restaurant et ses avis en chunks structurés.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): Nom du restaurant.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame contenant les chunks.\n",
    "        \"\"\"\n",
    "        colnames = ['restaurant', 'chunk']\n",
    "        avis_df, location_df, restaurant_df = self.get_restaurant_reviews_location(restaurant_name)\n",
    "        \n",
    "        if restaurant_df.empty:\n",
    "            return pd.DataFrame(columns=colnames)\n",
    "\n",
    "        chunks = []\n",
    "        for column in restaurant_df.columns:\n",
    "            value = restaurant_df[column].iloc[0]\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': f\"{column}: {value}\"})\n",
    "\n",
    "        # Inclure les avis comme chunks\n",
    "        for _, review in avis_df.iterrows():\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': f\"Review: {review['review']}\"})\n",
    "\n",
    "        return pd.DataFrame(chunks, columns=colnames)\n",
    "\n",
    "    def create_corpus(self) -> str:\n",
    "        \"\"\"\n",
    "        Crée un corpus à partir des chunks des restaurants.\n",
    "\n",
    "        Returns:\n",
    "            str: Corpus textuel.\n",
    "        \"\"\"\n",
    "        corpus = \"\"\n",
    "        for restaurant in self.get_all_restaurants_names():\n",
    "            df = self.transform_restaurant_chunk(restaurant)\n",
    "            corpus += \" \".join(df['chunk'].values) + \" \"\n",
    "        return corpus\n",
    "\n",
    "    def split_text_into_chunks(self, corpus: str, chunk_size: int = 500) -> list[str]:\n",
    "        \"\"\"\n",
    "        Divise un texte en chunks de taille spécifiée.\n",
    "\n",
    "        Args:\n",
    "            corpus (str): Texte à diviser.\n",
    "            chunk_size (int, optional): Taille de chaque chunk.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: Liste des chunks.\n",
    "        \"\"\"\n",
    "        tokenized_corpus = enc.encode(corpus)\n",
    "        chunks = [\n",
    "            \"\".join(enc.decode(tokenized_corpus[i : i + chunk_size]))\n",
    "            for i in tqdm(range(0, len(tokenized_corpus), chunk_size))\n",
    "        ]\n",
    "        return chunks\n",
    "\n",
    "    def add_embeddings(self, list_chunks: list[str], batch_size: int = 100) -> None:\n",
    "        \"\"\"\n",
    "        Ajoute les embeddings à la collection ChromaDB.\n",
    "\n",
    "        Args:\n",
    "            list_chunks (list[str]): Liste des chunks.\n",
    "            batch_size (int, optional): Taille du batch.\n",
    "        \"\"\"\n",
    "        if self.chroma_db is None:\n",
    "            raise ValueError(\"ChromaDB collection is not initialized. Call `_create_collection` first.\")\n",
    "        \n",
    "        if len(list_chunks) < batch_size:\n",
    "            batch_size_for_chromadb = len(list_chunks)\n",
    "        else:\n",
    "            batch_size_for_chromadb = batch_size\n",
    "\n",
    "        for i in tqdm(range(0, len(list_chunks), batch_size_for_chromadb)):\n",
    "            batch_documents = list_chunks[i : i + batch_size_for_chromadb]\n",
    "            list_ids = [str(uuid.uuid4()) for _ in batch_documents]\n",
    "            self.chroma_db.add(documents=batch_documents, ids=list_ids)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Point d'entrée pour exécuter les méthodes de la classe.\n",
    "        \"\"\"\n",
    "        corpus = self.create_corpus()\n",
    "        chunks = self.split_text_into_chunks(corpus)\n",
    "        self._create_collection(self.path)\n",
    "        self.add_embeddings(chunks)\n",
    "\n",
    "\n",
    "# Tester la classe\n",
    "if __name__ == \"__main__\":\n",
    "    test = BDDChunks(embedding_model=\"paraphrase-xlm-r-multilingual-v1\", path=\"./\")\n",
    "    test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import chromadb\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2395343120.py:124: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2395343120.py:124: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2395343120.py:125: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2395343120.py:125: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2395343120.py:126: PydanticDeprecatedSince20: The `from_orm` method is deprecated; set `model_config['from_attributes']=True` and use `model_validate` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
      "C:\\Users\\ediad\\AppData\\Local\\Temp\\ipykernel_8296\\2395343120.py:126: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
      "100%|██████████| 3638/3638 [00:00<00:00, 13488.38it/s]\n",
      "  0%|          | 0/37 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'add'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 259\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    258\u001b[0m     test \u001b[38;5;241m=\u001b[39m BDDChunks( embedding_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparaphrase-xlm-r-multilingual-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 259\u001b[0m     \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[128], line 252\u001b[0m, in \u001b[0;36mBDDChunks.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_text_into_chunks(corpus)\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# self._create_collection(self.path)\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# self.chroma_db.insert_documents(chunks)\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[128], line 241\u001b[0m, in \u001b[0;36mBDDChunks.add_embeddings\u001b[1;34m(self, list_chunks, batch_size)\u001b[0m\n\u001b[0;32m    237\u001b[0m list_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mstr\u001b[39m(id_chunk) \u001b[38;5;28;01mfor\u001b[39;00m id_chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_documents)))\n\u001b[0;32m    239\u001b[0m ]\n\u001b[0;32m    240\u001b[0m list_id_doc \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m list_ids]\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchroma_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m(documents\u001b[38;5;241m=\u001b[39mbatch_documents, ids\u001b[38;5;241m=\u001b[39mlist_id_doc)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    242\u001b[0m document_ids\u001b[38;5;241m.\u001b[39mextend(list_ids)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'add'"
     ]
    }
   ],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import chromadb\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from chromadb.config import Settings\n",
    "\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "class BDDChunks:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model: str, path: str):\n",
    "        \"\"\"\n",
    "        Initialize a BDDChunks instance.\n",
    "\n",
    "        Args:\n",
    "            embedding_model (str): The name of the embedding model to use for generating embeddings.\n",
    "            path (str): The file path to the PDF or dataset to process.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        self.chunks: list[str] | None = None\n",
    "        self.client = chromadb.PersistentClient(\n",
    "            path=\"./ChromaDB\", settings=Settings(anonymized_telemetry=False)\n",
    "        )\n",
    "        self.embedding_name = embedding_model\n",
    "        self.embeddings = SentenceTransformerEmbeddingFunction(\n",
    "            model_name=embedding_model\n",
    "        )\n",
    "        self.chroma_db = None\n",
    "        self.db = None\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Provide a database session.\n",
    "\n",
    "        Yields:\n",
    "            db: A session instance from the database.\n",
    "        \"\"\"\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "\n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of restaurant names.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurants = db.query(models.DimRestaurant).all()\n",
    "                return [r.nom for r in restaurants]\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching restaurant names: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    def _create_collection(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Create a new ChromaDB collection for storing embeddings.\n",
    "\n",
    "        Args:\n",
    "            path (str): The name of the collection to create in ChromaDB.\n",
    "        \"\"\"\n",
    "        # Tester qu'en changeant de path, on accède pas au reste\n",
    "        file_name = \"a\" + os.path.basename(path)[0:50].strip() + \"a\"\n",
    "        file_name = re.sub(r\"\\s+\", \"-\", file_name)\n",
    "        # Expected collection name that (1) contains 3-63 characters, (2) starts and ends with an alphanumeric character, (3) otherwise contains only alphanumeric characters, underscores or hyphens (-), (4) contains no two consecutive periods (..)\n",
    "        self.chroma_db = self.client.get_or_create_collection(name=file_name, embedding_function=self.embeddings, metadata={\"hnsw:space\": \"cosine\"})  # type: ignore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def convert_to_arrow_compatible(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert a DataFrame to an Arrow-compatible format.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Converted DataFrame.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == 'object':\n",
    "                df[column] = df[column].astype(str)\n",
    "            elif df[column].dtype == 'int64':\n",
    "                df[column] = df[column].astype('int32')\n",
    "        return df\n",
    "\n",
    "    def get_restaurant_reviews_location(self, restaurant_name: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get the location and reviews for a specific restaurant.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            tuple: DataFrames for reviews, location, and restaurant information.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with next(self.get_db()) as db:\n",
    "                restaurant = db.query(models.DimRestaurant).filter(models.DimRestaurant.nom == restaurant_name).first()\n",
    "                location = db.query(models.DimLocation).filter(models.DimLocation.id_location == restaurant.id_location).first()\n",
    "                avis = db.query(models.FaitAvis).filter(models.FaitAvis.id_restaurant == restaurant.id_restaurant).all()\n",
    "            \n",
    "            avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
    "            location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
    "            restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
    "            \n",
    "            return avis_df, location_df, restaurant_df\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching restaurant reviews location: {e}\")\n",
    "            return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    def transform_restaurant_chunk(self, restaurant_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform a restaurant's data and reviews into structured chunks.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the restaurant chunks.\n",
    "        \"\"\"\n",
    "        colnames = ['restaurant', 'chunk']\n",
    "        avis_df, location_df, restaurant_df = self.get_restaurant_reviews_location(restaurant_name)\n",
    "        \n",
    "        if restaurant_df.empty:\n",
    "            return pd.DataFrame(columns=colnames)\n",
    "\n",
    "        chunks = []\n",
    "        for column in restaurant_df.columns:\n",
    "            value = restaurant_df[column].iloc[0]\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': f\"{column}: {value}\"})\n",
    "\n",
    "        # Include review comments as chunks\n",
    "        for _, review in avis_df.iterrows():\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': f\"Review: {review['review']}\"})\n",
    "\n",
    "        return pd.DataFrame(chunks, columns=colnames)\n",
    "\n",
    "    def create_corpus(self) -> str:\n",
    "        \"\"\"\n",
    "        Create a corpus from the restaurant chunks.\n",
    "\n",
    "        Returns:\n",
    "            str: The text corpus.\n",
    "        \"\"\"\n",
    "        corpus = \"\"\n",
    "        for restaurant in self.get_all_restaurants_names():\n",
    "            df = self.transform_restaurant_chunk(restaurant)\n",
    "            corpus += \" \".join(df['chunk'].values) + \" \"\n",
    "        return corpus\n",
    "\n",
    "    # def insert_into_db(self):\n",
    "    #     \"\"\"\n",
    "    #     Insert the chunks into the SQLite database in batches for improved performance.\n",
    "    #     \"\"\"\n",
    "    #     all_restaurants = self.get_all_restaurants_names()\n",
    "    #     batch_size = 100 # Define the batch size for insertion\n",
    "\n",
    "    #     for restaurant in tqdm(all_restaurants):\n",
    "    #         #verifier si le restaurant est deja dans la table rag_avis\n",
    "    #         try:\n",
    "    #             with next(self.get_db()) as db:\n",
    "    #                 restaurant_exists = db.query(models.RagAvis).filter(models.RagAvis.restaurantName == restaurant).first()\n",
    "    #         except Exception as e:\n",
    "    #             print(f\"An error occurred while checking if the restaurant exists in the database: {e}\")\n",
    "    #             restaurant_exists = None\n",
    "\n",
    "    #         if restaurant_exists:\n",
    "    #             print(f\"Restaurant {restaurant} already exists in the database.\")\n",
    "    #             continue\n",
    "    #         df = self.transform_restaurant_chunk(restaurant)\n",
    "    #         chunks = [\n",
    "    #             models.RagAvis(restaurantName=row['restaurant'], review=row['chunk'])\n",
    "    #             for _, row in df.iterrows()\n",
    "    #         ]\n",
    "\n",
    "    #         try:\n",
    "    #             with next(self.get_db()) as db:\n",
    "    #                 for i in range(0, len(chunks), batch_size):\n",
    "    #                     db.bulk_save_objects(chunks[i:i + batch_size])\n",
    "    #                     db.commit()\n",
    "    #         except Exception as e:\n",
    "    #             print(f\"An error occurred while inserting chunks into the database for {restaurant}: {e}\")\n",
    "\n",
    "    def split_text_into_chunks(self, corpus: str, chunk_size: int = 500) -> list[str]:\n",
    "            \"\"\"\n",
    "            Splits a given text corpus into chunks of a specified size.\n",
    " \n",
    "            Args:\n",
    "                corpus (str): The input text corpus to be split into chunks.\n",
    "                chunk_size (int, optional): The size of each chunk. Defaults to 500.\n",
    "\n",
    "            Returns:\n",
    "                list[str]: A list of text chunks.\n",
    "            \"\"\"\n",
    "            tokenized_corpus = enc.encode(corpus)\n",
    "            chunks = [\n",
    "                \"\".join(enc.decode(tokenized_corpus[i : i + chunk_size]))\n",
    "                for i in tqdm(range(0, len(tokenized_corpus), chunk_size))\n",
    "            ]\n",
    "\n",
    "            return chunks\n",
    "    \n",
    "    def add_embeddings(self, list_chunks: list[str], batch_size: int = 100) -> None:\n",
    "        if len(list_chunks) < batch_size:\n",
    "            batch_size_for_chromadb = len(list_chunks)\n",
    "        else:\n",
    "            batch_size_for_chromadb = batch_size\n",
    "\n",
    "        document_ids: list[str] = []\n",
    "\n",
    "        for i in tqdm(\n",
    "            range(0, len(list_chunks), batch_size_for_chromadb)\n",
    "        ):  # On met en place une stratégie d'ajout par batch car ChromaDB ne supporte pas plus de 166 documents d'un coup.\n",
    "            batch_documents = list_chunks[i : i + batch_size_for_chromadb]\n",
    "            list_ids = [\n",
    "                str(id_chunk) for id_chunk in list(range(i, i + len(batch_documents)))\n",
    "            ]\n",
    "            list_id_doc = [str(uuid.uuid4()) for x in list_ids]\n",
    "            self.chroma_db.add(documents=batch_documents, ids=list_id_doc)  # type: ignore\n",
    "            document_ids.extend(list_ids)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Entry point to invoke methods of the class.\n",
    "        \"\"\"\n",
    "        corpus = self.create_corpus()\n",
    "        chunks = self.split_text_into_chunks(corpus)\n",
    "        # self._create_collection(self.path)\n",
    "        # self.chroma_db.insert_documents(chunks)\n",
    "        self.add_embeddings(chunks)\n",
    "\n",
    "\n",
    "\n",
    "# Test the class\n",
    "if __name__ == \"__main__\":\n",
    "    test = BDDChunks( embedding_model=\"paraphrase-xlm-r-multilingual-v1\", path=\"./\")\n",
    "    test()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "\n",
    "class BDDChunksSQLite:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunksSQLite instance.\n",
    "        \"\"\"\n",
    "        self.db = next(self.get_db())\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Provide a database session.\n",
    "\n",
    "        Yields:\n",
    "            db: A session instance from the database.\n",
    "        \"\"\"\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "\n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of restaurant names.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.db as db:\n",
    "                restaurants = db.query(models.DimRestaurant).all()\n",
    "                return [r.nom for r in restaurants]\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching restaurant names: {e}\")\n",
    "            return []\n",
    "\n",
    "    def convert_to_arrow_compatible(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert a DataFrame to an Arrow-compatible format.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Input DataFrame.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Converted DataFrame.\n",
    "        \"\"\"\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == 'object':\n",
    "                df[column] = df[column].astype(str)\n",
    "            elif df[column].dtype == 'int64':\n",
    "                df[column] = df[column].astype('int32')\n",
    "        return df\n",
    "\n",
    "    def get_restaurant_reviews_location(self, restaurant_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the location and reviews for a specific restaurant.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing restaurant name, location, and reviews.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.db as db:\n",
    "                restaurant = db.query(models.DimRestaurant).filter(models.DimRestaurant.nom == restaurant_name).first()\n",
    "                location = db.query(models.DimLocation).filter(models.DimLocation.id_location == restaurant.id_location).first()\n",
    "                avis = db.query(models.FaitAvis).filter(models.FaitAvis.id_restaurant == restaurant.id_restaurant).all()\n",
    "            \n",
    "            # Convert the data to a DataFrame\n",
    "            avis_df = pd.DataFrame([schemas.FaitAvis.from_orm(a).dict() for a in avis])\n",
    "            location_df = pd.DataFrame([schemas.DimLocation.from_orm(location).dict()])\n",
    "            restaurant_df = pd.DataFrame([schemas.DimRestaurant.from_orm(restaurant).dict()])\n",
    "            \n",
    "            return avis_df, location_df, restaurant_df\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching restaurant reviews location: {e}\")\n",
    "            return pd.DataFrame()\n",
    " \n",
    "    def transform_restaurant_chunk(self, restaurant_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform a restaurant chunk into a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the restaurant.\n",
    "            chunck_size (int): The size of each chunk of reviews.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the restaurant chunk.\n",
    "        \"\"\"\n",
    "        colnames = ['restaurant', 'chunk']\n",
    "        \n",
    "        avis_df, location_df, restaurant_df = self.get_restaurant_reviews_location(restaurant_name)\n",
    "        \n",
    "        # List to hold the chunks before creating the DataFrame\n",
    "        chunks = []\n",
    "        #add classement classement\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' classement : '+ str(restaurant_df['classement'][0]) +' '})\n",
    "        \n",
    "        # horaires\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' horaires : '+ str(restaurant_df['horaires'][0]) +' '})\n",
    "        \n",
    "        # note_globale\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' note_globale : '+ str(restaurant_df['note_globale'][0]) +' '})\n",
    "        \n",
    "        # note_cuisine\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' note_cuisine : '+ str(restaurant_df['note_cuisine'][0]) +' '})\n",
    "        \n",
    "        # note_service\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' note_service : '+ str(restaurant_df['note_service'][0]) +' '})\n",
    "\n",
    "        # note_rapportqualiteprix\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' note_rapportqualiteprix : '+ str(restaurant_df['note_rapportqualiteprix'][0]) +' '})\n",
    "\n",
    "        # note_ambiance\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' note_ambiance : '+ str(restaurant_df['note_ambiance'][0]) +' '})\n",
    "        \n",
    "        # infos_pratiques\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' infos_pratiques : '+ str(restaurant_df['infos_pratiques'][0]) +' '})\n",
    "        \n",
    "        # repas\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' repas : '+ str(restaurant_df['repas'][0]) +' '})\n",
    "               \n",
    "        # fourchette_prix\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' fourchette_prix : '+ str(restaurant_df['fourchette_prix'][0]) +' '})\n",
    "\n",
    "        # fonctionnalites\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' fonctionnalites : '+ str(restaurant_df['fonctionnalites'][0]) +' '})\n",
    "\n",
    "        # type_cuisines\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' type_cuisines : '+ str(restaurant_df['type_cuisines'][0]) +' '})\n",
    "\n",
    "        # nb_avis\n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' nb_avis : '+ str(restaurant_df['nb_avis'][0]) +' '})\n",
    "\n",
    "        # nbExcellent \n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' nbExcellent : '+ str(restaurant_df['nbExcellent'][0]) +' '})\n",
    "        \n",
    "        # nbTresbon \n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' nbTresbon : '+ str(restaurant_df['nbTresbon'][0]) +' '})\n",
    "        \n",
    "        # nbMoyen \n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' nbMoyen : '+ str(restaurant_df['nbMoyen'][0]) +' '})\n",
    "        \n",
    "        # nbMediocre \n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' nbMediocre : '+ str(restaurant_df['nbMediocre'][0]) +' '})\n",
    "        \n",
    "        # nbHorrible \n",
    "        chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name +' nbHorrible : '+ str(restaurant_df['nbHorrible'][0]) +' '})\n",
    "\n",
    "        # Insert all avis in the dataframe with the restaurant name\n",
    "        for i in range(0, len(avis_df)):\n",
    "            chunks.append({'restaurant': restaurant_name, 'chunk': 'restaurant: ' + restaurant_name + 'Commentaire : ' + avis_df['review'][i]})\n",
    "    \n",
    "        # Create DataFrame once all chunks are gathered\n",
    "        df_chunck = pd.DataFrame(chunks, columns=colnames)\n",
    "        \n",
    "        return df_chunck\n",
    "    \n",
    "    def create_corpus(self ) -> str:\n",
    "        \"\"\"\n",
    "        Create a corpus from the restaurant chunks.\n",
    "\n",
    "        Returns:\n",
    "            str: The text corpus.\n",
    "        \"\"\"\n",
    "        corpus = \"\"\n",
    "        for restaurant in self.get_all_restaurants_names():\n",
    "            df = self.transform_restaurant_chunk(restaurant)\n",
    "            corpus += \" \".join(df['chunk'].values)\n",
    "        return corpus\n",
    "    \n",
    "\n",
    "    def insert_into_db(self):\n",
    "        \"\"\"\n",
    "        Insert the chunks into the SQLite database.\n",
    "        \"\"\"\n",
    "        all_restaurants = self.get_all_restaurants_names()\n",
    "        for restaurant in tqdm(all_restaurants):\n",
    "            df = self.transform_restaurant_chunk(restaurant)\n",
    "\n",
    "            #ajouter les  lignes dans la base de données RagAvisBase\n",
    "            for i in range(0, len(df)):\n",
    "                # Insert the chunk into the database\n",
    "                try:\n",
    "                    with self.db as db:\n",
    "                        ragAvis = models.RagAvisBase(restaurant=df['restaurant'][i], chunk=df['chunk'][i])\n",
    "                        db.add(ragAvis)\n",
    "                        db.commit()\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred while inserting chunk into the database: {e}\")\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # def get_embedding(self, restaurantName: str) -> list[float]:\n",
    "    #     restaurant_chunck = self.transform_restaurant_chunk(restaurantName, 5)\n",
    "    #     # Get the embeddings for each chunk\n",
    "    #     embeddings = []\n",
    "    #     for chunk in restaurant_chunck['chunk']:\n",
    "    #         embeddings.append(self.embedder.encode(chunk))\n",
    "    #     return embeddings\n",
    "    \n",
    "    # def embedder(self, chunk: str) -> list[float]:\n",
    "    #     \"\"\"\n",
    "    #     Embed a chunk using the SentenceTransformer model.\n",
    "\n",
    "    #     Args:\n",
    "    #         chunk (str): The input chunk to embed.\n",
    "\n",
    "    #     Returns:\n",
    "    #         list[float]: The embedding of the chunk.\n",
    "    #     \"\"\"\n",
    "    #     return self.embedder.encode(chunk)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, *args, **kwds):\n",
    "        \"\"\"\n",
    "        Entry point to invoke methods of the class.\n",
    "        \"\"\"\n",
    "        self.insert_into_db()\n",
    "        \n",
    "\n",
    "# Test the class\n",
    "test = BDDChunksSQLite()\n",
    "# # result = test.get_restaurant_reviews_location(\"Aromatic Restaurant\")\n",
    "# result = test.transform_restaurant_chunk(\"Aromatic Restaurant\")\n",
    "# result.head(5)\n",
    "# #renregistrement des chunks dans un fichier csv\n",
    "# result.to_csv('chunks.csv', index=False)\n",
    "\n",
    "# # corpus = test.split_text_into_chunks(test.create_corpus())\n",
    "# with open('corpus.txt', 'w') as f:\n",
    "#     # f.write(corpus)\n",
    "#     for chunk in corpus:\n",
    "#         f.write(chunk + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while fetching restaurant reviews location: 'DimLocation' object has no attribute 'nom'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import os\n",
    "import uuid\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "class BDDChunksSQLite:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "\n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunksSQLite instance.\n",
    "        \"\"\"\n",
    "        self.db = next(self.get_db())\n",
    "        # pass  # Constructor is currently empty but can be extended if needed in the future.\n",
    "\n",
    "    def get_db(self):\n",
    "        \"\"\"\n",
    "        Provide a database session.\n",
    "\n",
    "        Yields:\n",
    "            db: A session instance from the database.\n",
    "        \"\"\"\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "\n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of restaurant names.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.db as db: \n",
    "                restaurants = db.query(models.DimRestaurant).all()\n",
    "                restaurant_names = [r.nom for r in restaurants]\n",
    "            return restaurant_names\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching restaurant names: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    def convert_to_arrow_compatible(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == 'object':\n",
    "                df[column] = df[column].astype(str)\n",
    "            elif df[column].dtype == 'int64':\n",
    "                df[column] = df[column].astype('int32')\n",
    "        return df\n",
    "\n",
    "    def get_restaurant_reviews_location(self, restaurant_name: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get the location of reviews for a specific restaurant.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the restaurant.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of review locations.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with self.db as db:\n",
    "                # restaurant = db.query(models.DimRestaurant, models.DimLocation, models.).join.filter(models.DimRestaurant.nom == restaurant_name).all()\n",
    "                join_data = db.query(models.DimRestaurant, models.DimLocation, models.FaitAvis) \\\n",
    "                    .join(models.DimLocation, models.DimRestaurant.id_location == models.DimLocation.id_location) \\\n",
    "                    .join(models.FaitAvis, models.DimRestaurant.id_restaurant == models.FaitAvis.id_restaurant) \\\n",
    "                    .filter(models.DimRestaurant.nom == restaurant_name).all()\n",
    "                \n",
    "            # restaurant = pd.DataFrame([schemas.DimRestaurant.from_orm(r).dict() for r in restaurant])\n",
    "\n",
    "            # join_data = pd.DataFrame([r.__dict__ for r in join_data])\n",
    "            data = pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"restaurant\": r[0].nom,\n",
    "                        \"location\": r[1].nom,\n",
    "                        \"avis\": r[2].avis,\n",
    "                    }\n",
    "                    for  restaurant , location  , avis in join_data\n",
    "                ]\n",
    "            )\n",
    "            # restaurant_df = pd.DataFrame([r.__dict__ for r in restaurant])\n",
    "            # location_df = pd.DataFrame([l.__dict__ for l in location])\n",
    "            # avis_df = pd.DataFrame([a.__dict__ for a in avis])\n",
    "\n",
    "\n",
    "            #transformation  of the result to a list of strings\n",
    "            \n",
    "            # return convert_to_arrow_compatible(restaurant)\n",
    "            return join_data\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while fetching restaurant reviews location: {e}\")\n",
    "            return []   \n",
    "    \n",
    "    def __call__(self, *args, **kwds):\n",
    "        self.get_all_restaurants_names()\n",
    "        pass     \n",
    "test = BDDChunksSQLite()\n",
    "test.get_restaurant_reviews_location(\"Aromatic Restaurant\")\n",
    "# print(test.get_all_restaurants_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'model.models' has no attribute 'Restaurant'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 145\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# def  get_restaurant_location_reviews(self, restaurant_name: str) -> list[schemas.Review]:\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m#     \"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m#     Get all reviews for a given restaurant.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m#         embedding = self.generate_fake_embedding(review)  # Generate embedding for the review (chunk)\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m#         self.store_chunk_in_db(restaurant_name, review, embedding)\u001b[39;00m\n\u001b[0;32m    144\u001b[0m bdd_chunks \u001b[38;5;241m=\u001b[39m BDDChunksSQLite()\n\u001b[1;32m--> 145\u001b[0m restaurants \u001b[38;5;241m=\u001b[39m \u001b[43mbdd_chunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all_restaurants_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m restaurants\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# bdd_chunks.process_reviews()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 46\u001b[0m, in \u001b[0;36mBDDChunksSQLite.get_all_restaurants_names\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03mGet all restaurant names from the SQLite database.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    list[str]: A list of restaurant names.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_db()) \u001b[38;5;28;01mas\u001b[39;00m db:\n\u001b[1;32m---> 46\u001b[0m     restaurants \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mquery(\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRestaurant\u001b[49m)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# restaurants_name = [r.nom for r in restaurants]\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m restaurants\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'model.models' has no attribute 'Restaurant'"
     ]
    }
   ],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import os\n",
    "import uuid\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "class BDDChunksSQLite:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    \n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self ):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunksSQLite instance.\n",
    "\n",
    "        Args:\n",
    "            sqlite_db_path (str): Path to the SQLite database file.\n",
    "            reviews_table (str): Name of the table containing reviews and restaurant information.\n",
    "            embeddings_table (str): Name of the table where embeddings will be stored.\n",
    "        \"\"\"\n",
    " \n",
    "        # self.db = self.get_db()\n",
    "\n",
    "    # Database dependency\n",
    "    def get_db(self):\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "    \n",
    "    def get_all_restaurants_names(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Get all restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: A list of restaurant names.\n",
    "        \"\"\"\n",
    "\n",
    "        with next(self.get_db()) as db:\n",
    "            restaurants = db.query(models.Restaurant).all()\n",
    "        # restaurants_name = [r.nom for r in restaurants]\n",
    "        return restaurants\n",
    "    \n",
    "    # def  get_restaurant_location_reviews(self, restaurant_name: str) -> list[schemas.Review]:\n",
    "    #     \"\"\"\n",
    "    #     Get all reviews for a given restaurant.\n",
    "\n",
    "    #     Args:\n",
    "    #         restaurant_name (str): The name of the restaurant.\n",
    "\n",
    "    #     Returns:\n",
    "    #         list[schemas.Review]: A list of reviews for the restaurant.\n",
    "    #     \"\"\"\n",
    "    #     with self.get_db() as db:\n",
    "    #         reviews = db.query(models.Review).filter(models.Review.restaurant_name == restaurant_name).all()\n",
    "    #     return reviews\n",
    "\n",
    "\n",
    "    # def fetch_reviews_from_db(self) -> list[tuple[str, str]]:\n",
    "    #     \"\"\"\n",
    "    #     Fetch reviews and their associated restaurant names from the SQLite database.\n",
    "\n",
    "    #     Returns:\n",
    "    #         list[tuple[str, str]]: A list of tuples containing restaurant names and reviews.\n",
    "    #     \"\"\"\n",
    "    #     data = []\n",
    "    #     # conn = sqlite3.connect(self.sqlite_db_path)\n",
    "    #     # cursor = conn.cursor()\n",
    "    #     # cursor.execute(f\"SELECT restaurant_name, review FROM {self.reviews_table};\")\n",
    "    #     # data = cursor.fetchall()\n",
    "    #     # conn.close()\n",
    "    #     with self.get_db() as db:\n",
    "    #         data = db.query(models.Review).all()\n",
    "    #         data = [(d.restaurant_name, d.review) for d in data]\n",
    "\n",
    "    #     return data\n",
    "\n",
    "    # def generate_fake_embedding(self, text: str) -> list[float]:\n",
    "    #     \"\"\"\n",
    "    #     Generate a fake embedding for a given text. Replace this with your embedding logic.\n",
    "\n",
    "    #     Args:\n",
    "    #         text (str): The text for which to generate an embedding.\n",
    "\n",
    "    #     Returns:\n",
    "    #         list[float]: A list representing the embedding vector.\n",
    "    #     \"\"\"\n",
    "    #     # Example: Return the length of each word in the text as a fake embedding.\n",
    "    #     return [len(word) for word in text.split()]\n",
    "\n",
    "    # def store_chunk_in_db(self, restaurant_name: str, chunk: str, embedding: list[float]) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Store a chunk and its embedding in the SQLite database.\n",
    "\n",
    "    #     Args:\n",
    "    #         restaurant_name (str): The name of the associated restaurant.\n",
    "    #         chunk (str): The text chunk (in this case, the full review).\n",
    "    #         embedding (list[float]): The embedding vector for the chunk.\n",
    "    #     \"\"\"\n",
    "    #     conn = sqlite3.connect(self.sqlite_db_path)\n",
    "    #     cursor = conn.cursor()\n",
    "\n",
    "    #     # Ensure the embeddings table exists\n",
    "    #     cursor.execute(f\"\"\"\n",
    "    #     CREATE TABLE IF NOT EXISTS {self.embeddings_table} (\n",
    "    #         id TEXT PRIMARY KEY,\n",
    "    #         restaurant_name TEXT,\n",
    "    #         chunk TEXT,\n",
    "    #         embedding TEXT\n",
    "    #     );\n",
    "    #     \"\"\")\n",
    "\n",
    "    #     embedding_str = \",\".join(map(str, embedding))\n",
    "    #     cursor.execute(\n",
    "    #         f\"INSERT INTO {self.embeddings_table} (id, restaurant_name, chunk, embedding) VALUES (?, ?, ?, ?);\",\n",
    "    #         (str(uuid.uuid4()), restaurant_name, chunk, embedding_str),\n",
    "    #     )\n",
    "    #     conn.commit()\n",
    "    #     conn.close()\n",
    "\n",
    "    # def process_reviews(self) -> None:\n",
    "    #     \"\"\"\n",
    "    #     Process each review as a single chunk, generate embeddings, and store them in the database.\n",
    "\n",
    "    #     This method:\n",
    "    #     1. Fetches reviews and restaurant names from the SQLite database.\n",
    "    #     2. Generates embeddings for each review.\n",
    "    #     3. Stores the reviews and embeddings in the SQLite database.\n",
    "    #     \"\"\"\n",
    "    #     data = self.fetch_reviews_from_db()\n",
    "\n",
    "    #     for restaurant_name, review in tqdm(data, desc=\"Processing Reviews\"):\n",
    "    #         embedding = self.generate_fake_embedding(review)  # Generate embedding for the review (chunk)\n",
    "    #         self.store_chunk_in_db(restaurant_name, review, embedding)\n",
    "\n",
    "\n",
    "\n",
    "bdd_chunks = BDDChunksSQLite()\n",
    "restaurants = bdd_chunks.get_all_restaurants_names()\n",
    "restaurants\n",
    "# bdd_chunks.process_reviews()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import os\n",
    "import uuid\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "class BDDChunksSQLite:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    \n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self ):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunksSQLite instance.\n",
    "\n",
    "        Args:\n",
    "            sqlite_db_path (str): Path to the SQLite database file.\n",
    "            reviews_table (str): Name of the table containing reviews and restaurant information.\n",
    "            embeddings_table (str): Name of the table where embeddings will be stored.\n",
    "        \"\"\"\n",
    " \n",
    "        self.db = self.get_db()\n",
    "\n",
    "    # Database dependency\n",
    "    def get_db():\n",
    "        db = database.SessionLocal()\n",
    "        try:\n",
    "            yield db\n",
    "        finally:\n",
    "            db.close()\n",
    "    \n",
    "\n",
    "    def fetch_reviews_from_db(self) -> list[tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Fetch reviews and their associated restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[tuple[str, str]]: A list of tuples containing restaurant names and reviews.\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.sqlite_db_path)\n",
    "        cursor = conn.cursor()\n",
    "        query = f\"SELECT restaurant_name, review FROM {self.reviews_table};\"\n",
    "        cursor.execute(query)\n",
    "        data = cursor.fetchall()\n",
    "        conn.close()\n",
    "        return data\n",
    "\n",
    "    def generate_fake_embedding(self, text: str) -> list[float]:\n",
    "        \"\"\"\n",
    "        Generate a fake embedding for a given text. Replace this with your embedding logic.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text for which to generate an embedding.\n",
    "\n",
    "        Returns:\n",
    "            list[float]: A list representing the embedding vector.\n",
    "        \"\"\"\n",
    "        # Example: Return the length of each word in the text as a fake embedding.\n",
    "        return [len(word) for word in text.split()]\n",
    "\n",
    "    def store_chunk_in_db(self, restaurant_name: str, chunk: str, embedding: list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Store a chunk and its embedding in the SQLite database.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the associated restaurant.\n",
    "            chunk (str): The text chunk (in this case, the full review).\n",
    "            embedding (list[float]): The embedding vector for the chunk.\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.sqlite_db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Ensure the embeddings table exists\n",
    "        cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {self.embeddings_table} (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            restaurant_name TEXT,\n",
    "            chunk TEXT,\n",
    "            embedding TEXT\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        embedding_str = \",\".join(map(str, embedding))\n",
    "        cursor.execute(\n",
    "            f\"INSERT INTO {self.embeddings_table} (id, restaurant_name, chunk, embedding) VALUES (?, ?, ?, ?);\",\n",
    "            (str(uuid.uuid4()), restaurant_name, chunk, embedding_str),\n",
    "        )\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def process_reviews(self) -> None:\n",
    "        \"\"\"\n",
    "        Process each review as a single chunk, generate embeddings, and store them in the database.\n",
    "\n",
    "        This method:\n",
    "        1. Fetches reviews and restaurant names from the SQLite database.\n",
    "        2. Generates embeddings for each review.\n",
    "        3. Stores the reviews and embeddings in the SQLite database.\n",
    "        \"\"\"\n",
    "        data = self.fetch_reviews_from_db()\n",
    "\n",
    "        for restaurant_name, review in tqdm(data, desc=\"Processing Reviews\"):\n",
    "            embedding = self.generate_fake_embedding(review)  # Generate embedding for the review (chunk)\n",
    "            self.store_chunk_in_db(restaurant_name, review, embedding)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sqlite_db_path = \"path/to/your/database.sqlite\"  # Path to your SQLite DB\n",
    "reviews_table = \"reviews\"  # Table containing reviews\n",
    "embeddings_table = \"embeddings\"  # Table to store embeddings\n",
    "\n",
    "bdd_chunks = BDDChunksSQLite(sqlite_db_path, reviews_table, embeddings_table)\n",
    "bdd_chunks.process_reviews()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m embeddings_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Table to store embeddings\u001b[39;00m\n\u001b[0;32m    106\u001b[0m bdd_chunks \u001b[38;5;241m=\u001b[39m BDDChunksSQLite(sqlite_db_path, reviews_table, embeddings_table)\n\u001b[1;32m--> 107\u001b[0m \u001b[43mbdd_chunks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 94\u001b[0m, in \u001b[0;36mBDDChunksSQLite.process_reviews\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_reviews\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    Process each review as a single chunk, generate embeddings, and store them in the database.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m    3. Stores the reviews and embeddings in the SQLite database.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_reviews_from_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m restaurant_name, review \u001b[38;5;129;01min\u001b[39;00m tqdm(data, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m         embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_fake_embedding(review)  \u001b[38;5;66;03m# Generate embedding for the review (chunk)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 34\u001b[0m, in \u001b[0;36mBDDChunksSQLite.fetch_reviews_from_db\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_reviews_from_db\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    Fetch reviews and their associated restaurant names from the SQLite database.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m        list[tuple[str, str]]: A list of tuples containing restaurant names and reviews.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43msqlite3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqlite_db_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[0;32m     36\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT restaurant_name, review FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreviews_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class BDDChunksSQLite:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    \n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sqlite_db_path: str, reviews_table: str, embeddings_table: str):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunksSQLite instance.\n",
    "\n",
    "        Args:\n",
    "            sqlite_db_path (str): Path to the SQLite database file.\n",
    "            reviews_table (str): Name of the table containing reviews and restaurant information.\n",
    "            embeddings_table (str): Name of the table where embeddings will be stored.\n",
    "        \"\"\"\n",
    "        self.sqlite_db_path = sqlite_db_path\n",
    "        self.reviews_table = reviews_table\n",
    "        self.embeddings_table = embeddings_table\n",
    "\n",
    "    def fetch_reviews_from_db(self) -> list[tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Fetch reviews and their associated restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[tuple[str, str]]: A list of tuples containing restaurant names and reviews.\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.sqlite_db_path)\n",
    "        cursor = conn.cursor()\n",
    "        query = f\"SELECT restaurant_name, review FROM {self.reviews_table};\"\n",
    "        cursor.execute(query)\n",
    "        data = cursor.fetchall()\n",
    "        conn.close()\n",
    "        return data\n",
    "\n",
    "    def generate_fake_embedding(self, text: str) -> list[float]:\n",
    "        \"\"\"\n",
    "        Generate a fake embedding for a given text. Replace this with your embedding logic.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text for which to generate an embedding.\n",
    "\n",
    "        Returns:\n",
    "            list[float]: A list representing the embedding vector.\n",
    "        \"\"\"\n",
    "        # Example: Return the length of each word in the text as a fake embedding.\n",
    "        return [len(word) for word in text.split()]\n",
    "\n",
    "    def store_chunk_in_db(self, restaurant_name: str, chunk: str, embedding: list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Store a chunk and its embedding in the SQLite database.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the associated restaurant.\n",
    "            chunk (str): The text chunk (in this case, the full review).\n",
    "            embedding (list[float]): The embedding vector for the chunk.\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.sqlite_db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Ensure the embeddings table exists\n",
    "        cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {self.embeddings_table} (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            restaurant_name TEXT,\n",
    "            chunk TEXT,\n",
    "            embedding TEXT\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        embedding_str = \",\".join(map(str, embedding))\n",
    "        cursor.execute(\n",
    "            f\"INSERT INTO {self.embeddings_table} (id, restaurant_name, chunk, embedding) VALUES (?, ?, ?, ?);\",\n",
    "            (str(uuid.uuid4()), restaurant_name, chunk, embedding_str),\n",
    "        )\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def process_reviews(self) -> None:\n",
    "        \"\"\"\n",
    "        Process each review as a single chunk, generate embeddings, and store them in the database.\n",
    "\n",
    "        This method:\n",
    "        1. Fetches reviews and restaurant names from the SQLite database.\n",
    "        2. Generates embeddings for each review.\n",
    "        3. Stores the reviews and embeddings in the SQLite database.\n",
    "        \"\"\"\n",
    "        data = self.fetch_reviews_from_db()\n",
    "\n",
    "        for restaurant_name, review in tqdm(data, desc=\"Processing Reviews\"):\n",
    "            embedding = self.generate_fake_embedding(review)  # Generate embedding for the review (chunk)\n",
    "            self.store_chunk_in_db(restaurant_name, review, embedding)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sqlite_db_path = \"path/to/your/database.sqlite\"  # Path to your SQLite DB\n",
    "reviews_table = \"reviews\"  # Table containing reviews\n",
    "embeddings_table = \"embeddings\"  # Table to store embeddings\n",
    "\n",
    "bdd_chunks = BDDChunksSQLite(sqlite_db_path, reviews_table, embeddings_table)\n",
    "bdd_chunks.process_reviews()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
