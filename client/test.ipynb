{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 113\u001b[0m\n\u001b[0;32m    110\u001b[0m reviews_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Table containing reviews\u001b[39;00m\n\u001b[0;32m    111\u001b[0m embeddings_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Table to store embeddings\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m bdd_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mBDDChunksSQLite\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlite_db_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreviews_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m bdd_chunks\u001b[38;5;241m.\u001b[39mprocess_reviews()\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 4 were given"
     ]
    }
   ],
   "source": [
    "from model import models, schemas\n",
    "from utils import database\n",
    "import os\n",
    "import uuid\n",
    "import sqlite3\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class BDDChunksSQLite:\n",
    "    \"\"\"\n",
    "    A class to process reviews from a SQLite database and store them as chunks with embeddings.\n",
    "    \n",
    "    Each review is considered a single chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, db = database.SessionLocal()):\n",
    "        \"\"\"\n",
    "        Initialize the BDDChunksSQLite instance.\n",
    "\n",
    "        Args:\n",
    "            sqlite_db_path (str): Path to the SQLite database file.\n",
    "            reviews_table (str): Name of the table containing reviews and restaurant information.\n",
    "            embeddings_table (str): Name of the table where embeddings will be stored.\n",
    "        \"\"\"\n",
    "        # self.sqlite_db_path = sqlite_db_path\n",
    "        # self.reviews_table = reviews_table\n",
    "        # self.embeddings_table = embeddings_table\n",
    "        self.db = self.get_db()\n",
    "\n",
    "    def get_db(self):\n",
    "        return database.SessionLocal()\n",
    "    \n",
    "\n",
    "    def fetch_reviews_from_db(self) -> list[tuple[str, str]]:\n",
    "        \"\"\"\n",
    "        Fetch reviews and their associated restaurant names from the SQLite database.\n",
    "\n",
    "        Returns:\n",
    "            list[tuple[str, str]]: A list of tuples containing restaurant names and reviews.\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.sqlite_db_path)\n",
    "        cursor = conn.cursor()\n",
    "        query = f\"SELECT restaurant_name, review FROM {self.reviews_table};\"\n",
    "        cursor.execute(query)\n",
    "        data = cursor.fetchall()\n",
    "        conn.close()\n",
    "        return data\n",
    "\n",
    "    def generate_fake_embedding(self, text: str) -> list[float]:\n",
    "        \"\"\"\n",
    "        Generate a fake embedding for a given text. Replace this with your embedding logic.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text for which to generate an embedding.\n",
    "\n",
    "        Returns:\n",
    "            list[float]: A list representing the embedding vector.\n",
    "        \"\"\"\n",
    "        # Example: Return the length of each word in the text as a fake embedding.\n",
    "        return [len(word) for word in text.split()]\n",
    "\n",
    "    def store_chunk_in_db(self, restaurant_name: str, chunk: str, embedding: list[float]) -> None:\n",
    "        \"\"\"\n",
    "        Store a chunk and its embedding in the SQLite database.\n",
    "\n",
    "        Args:\n",
    "            restaurant_name (str): The name of the associated restaurant.\n",
    "            chunk (str): The text chunk (in this case, the full review).\n",
    "            embedding (list[float]): The embedding vector for the chunk.\n",
    "        \"\"\"\n",
    "        conn = sqlite3.connect(self.sqlite_db_path)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Ensure the embeddings table exists\n",
    "        cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {self.embeddings_table} (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            restaurant_name TEXT,\n",
    "            chunk TEXT,\n",
    "            embedding TEXT\n",
    "        );\n",
    "        \"\"\")\n",
    "\n",
    "        embedding_str = \",\".join(map(str, embedding))\n",
    "        cursor.execute(\n",
    "            f\"INSERT INTO {self.embeddings_table} (id, restaurant_name, chunk, embedding) VALUES (?, ?, ?, ?);\",\n",
    "            (str(uuid.uuid4()), restaurant_name, chunk, embedding_str),\n",
    "        )\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def process_reviews(self) -> None:\n",
    "        \"\"\"\n",
    "        Process each review as a single chunk, generate embeddings, and store them in the database.\n",
    "\n",
    "        This method:\n",
    "        1. Fetches reviews and restaurant names from the SQLite database.\n",
    "        2. Generates embeddings for each review.\n",
    "        3. Stores the reviews and embeddings in the SQLite database.\n",
    "        \"\"\"\n",
    "        data = self.fetch_reviews_from_db()\n",
    "\n",
    "        for restaurant_name, review in tqdm(data, desc=\"Processing Reviews\"):\n",
    "            embedding = self.generate_fake_embedding(review)  # Generate embedding for the review (chunk)\n",
    "            self.store_chunk_in_db(restaurant_name, review, embedding)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# sqlite_db_path = \"path/to/your/database.sqlite\"  # Path to your SQLite DB\n",
    "# reviews_table = \"reviews\"  # Table containing reviews\n",
    "# embeddings_table = \"embeddings\"  # Table to store embeddings\n",
    "\n",
    "bdd_chunks = BDDChunksSQLite(sqlite_db_path, reviews_table, embeddings_table)\n",
    "bdd_chunks.process_reviews()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'DimLocation' has no attribute 'id_restaurant'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     46\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(get_db())\n\u001b[1;32m---> 47\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mjoin_restaurant_avis_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# df = df.drop(columns=['id_avis', 'id_restaurant', 'id_date'])\u001b[39;00m\n\u001b[0;32m     49\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m, in \u001b[0;36mjoin_restaurant_avis_date\u001b[1;34m(db)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin_restaurant_avis_date\u001b[39m(db: Session) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Join the DimRestaurant, FactAvis, and DimDate and select the fields\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     query \u001b[38;5;241m=\u001b[39m db\u001b[38;5;241m.\u001b[39mquery(models\u001b[38;5;241m.\u001b[39mDimLocation, models\u001b[38;5;241m.\u001b[39mDimRestaurant)\\\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;241m.\u001b[39mjoin(models\u001b[38;5;241m.\u001b[39mDimRestaurant, \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDimLocation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid_restaurant\u001b[49m \u001b[38;5;241m==\u001b[39m models\u001b[38;5;241m.\u001b[39mDimRestaurant\u001b[38;5;241m.\u001b[39mid_restaurant)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Execute the query\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     results \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mall()\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'DimLocation' has no attribute 'id_restaurant'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from sqlalchemy.orm import Session\n",
    "from typing import List\n",
    "from model import models, schemas\n",
    "from utils import database\n",
    "import math\n",
    "\n",
    "# Database dependency\n",
    "def get_db():\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "\n",
    "def join_restaurant_avis_date(db: Session) -> pd.DataFrame:\n",
    "    # Join the DimRestaurant, FactAvis, and DimDate and select the fields\n",
    "    query = db.query(models.DimLocation, models.DimRestaurant)\\\n",
    "        .join(models.DimRestaurant, models.DimLocation.id_restaurant == models.DimRestaurant.id_restaurant)\n",
    "\n",
    "    # Execute the query\n",
    "    results = query.all()\n",
    "        # Create a DataFrame with the results\n",
    "    df = pd.DataFrame([{\n",
    "        \"restaurant\": restaurant.nom,\n",
    "        \"type_cuisines\": restaurant.type_cuisines,\n",
    "        \"fonctionnalites\": restaurant.fonctionnalites,\n",
    "        \"infos_pratiques\": restaurant.infos_pratiques,\n",
    "        \"classement\": restaurant.classement,\n",
    "        'longitude': location.longitude,\n",
    "        'latitude': location.latitude\n",
    "        \n",
    "    } for restaurant, location in results])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "db = next(get_db())\n",
    "df = join_restaurant_avis_date(db)\n",
    "# df = df.drop(columns=['id_avis', 'id_restaurant', 'id_date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La distance entre Paris et Londres est d'environ 343.56 km.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calcule la distance entre deux points géographiques en utilisant la formule de Haversine.\n",
    "\n",
    "    :param lat1: Latitude du premier point (en degrés).\n",
    "    :param lon1: Longitude du premier point (en degrés).\n",
    "    :param lat2: Latitude du deuxième point (en degrés).\n",
    "    :param lon2: Longitude du deuxième point (en degrés).\n",
    "    :return: Distance entre les deux points (en kilomètres).\n",
    "    \"\"\"\n",
    "    # Rayon de la Terre en kilomètres\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convertir les degrés en radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "\n",
    "    # Différences de latitude et de longitude\n",
    "    delta_lat = lat2_rad - lat1_rad\n",
    "    delta_lon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Formule de Haversine\n",
    "    a = math.sin(delta_lat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "# Exemple d'utilisation\n",
    "lat1, lon1 = 48.8566, 2.3522  # Paris\n",
    "lat2, lon2 = 51.5074, -0.1278  # Londres\n",
    "\n",
    "distance = haversine(lat1, lon1, lat2, lon2)\n",
    "print(f\"La distance entre Paris et Londres est d'environ {distance:.2f} km.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similarité entre les deux textes est : 0.88\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Charger un modèle pour générer des embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Petit modèle rapide et efficace\n",
    "\n",
    "def calculate_similarity(text1: str, text2: str) -> float:\n",
    "    # Générer les embeddings pour les deux textes\n",
    "    embedding1 = model.encode(text1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(text2, convert_to_tensor=True)\n",
    "    \n",
    "    # Calculer la similarité cosinus\n",
    "    similarity = util.cos_sim(embedding1, embedding2)\n",
    "    \n",
    "    return similarity.item()  # Retourne un score de similarité (0 à 1)\n",
    "\n",
    "# Champs texte\n",
    "# text1 = input(\"Entrez le premier texte : \")\n",
    "# text2 = input(\"Entrez le deuxième texte : \")\n",
    "text1 = \"Bonjour, je suis un texte de test je veux  1  franx\"\n",
    "text2 = \"Bonjour, je suis un autre texte de test\"\n",
    "# Calcul de la similarité\n",
    "similarity_score = calculate_similarity(text1, text2)\n",
    "\n",
    "print(f\"La similarité entre les deux textes est : {similarity_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from sqlalchemy.orm import Session\n",
    "from typing import List\n",
    "from model import models, schemas\n",
    "from utils import database\n",
    "import math\n",
    "\n",
    "# Database dependency\n",
    "def get_db():\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "\n",
    "def join_restaurant_avis_date(db: Session) -> pd.DataFrame:\n",
    "    # Join the DimRestaurant, FactAvis, and DimDate and select the fields\n",
    "    query = db.query(models.FaitAvis, models.DimRestaurant, models.DimDate)\\\n",
    "              .join(models.DimRestaurant, models.FaitAvis.id_restaurant == models.DimRestaurant.id_restaurant)\n",
    "              \n",
    "\n",
    "    # Execute the query\n",
    "    results = query.all()\n",
    "    \n",
    "    # Create a DataFrame with the results\n",
    "    df = pd.DataFrame([{\n",
    "        \"restaurant\": restaurant.nom,\n",
    "        \"type_cuisines\": restaurant.type_cuisines,\n",
    "        \"fonctionnalites\": restaurant.fonctionnalites,\n",
    "        \"infos_pratiques\": restaurant.infos_pratiques,\n",
    "        \"classement\": restaurant.classement,\n",
    "        \n",
    "    } for avis, restaurant, date in results])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "# List of stopwords\n",
    "mots_vides = stopwords.words(\"french\")\n",
    "\n",
    "# List of punctuations and digits\n",
    "ponctuations = string.punctuation\n",
    "chiffres = string.digits\n",
    "\n",
    "# Function to clean and tokenize document\n",
    "def nettoyage_doc(doc_param):\n",
    "    # Convert to lowercase\n",
    "    doc = doc_param.lower()\n",
    "    # Remove punctuations\n",
    "    doc = \"\".join([w for w in list(doc) if not w in ponctuations])\n",
    "    # Remove digits\n",
    "    doc = \"\".join([w for w in list(doc) if not w in chiffres])\n",
    "    Tokenize the document\n",
    "    doc = word_tokenize(doc)\n",
    "    # Lemmatize each term\n",
    "    doc = [lem.lemmatize(terme) for terme in doc]\n",
    "    # Remove stopwords\n",
    "    doc = [w for w in doc if not w in mots_vides]\n",
    "    # Remove terms with less than 3 characters if they are not numeric\n",
    "    doc = [w for w in doc if len(w) >= 3 or w.isnumeric()]\n",
    "    return doc\n",
    "\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calcule la distance entre deux points géographiques en utilisant la formule de Haversine.\n",
    "\n",
    "    :param lat1: Latitude du premier point (en degrés).\n",
    "    :param lon1: Longitude du premier point (en degrés).\n",
    "    :param lat2: Latitude du deuxième point (en degrés).\n",
    "    :param lon2: Longitude du deuxième point (en degrés).\n",
    "    :return: Distance entre les deux points (en kilomètres).\n",
    "    \"\"\"\n",
    "    # Rayon de la Terre en kilomètres\n",
    "    R = 6371.0\n",
    "\n",
    "    # Convertir les degrés en radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "\n",
    "    # Différences de latitude et de longitude\n",
    "    delta_lat = lat2_rad - lat1_rad\n",
    "    delta_lon = lon2_rad - lon1_rad\n",
    "\n",
    "    # Formule de Haversine\n",
    "    a = math.sin(delta_lat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "def calculate_similarity_notes(note1: float, note2: float) -> float:\n",
    "    # Calculate the absolute difference between the two notes\n",
    "    diff = abs(note1 - note2)\n",
    "    \n",
    "    # Normalize the difference to a similarity score (0 to 1)\n",
    "    similarity = 1 / (1 + diff)\n",
    "    return similarity\n",
    "\n",
    "def calculate_similarity_texte(text1: str, text2: str) -> float:\n",
    "    # Clean and tokenize the texts\n",
    "    doc1 = nettoyage_doc(text1)\n",
    "    doc2 = nettoyage_doc(text2)\n",
    "    \n",
    "    # Join tokens back to strings\n",
    "    text1 = \" \".join(doc1)\n",
    "    text2 = \" \".join(doc2)\n",
    "    \n",
    "    # print(f\"Texte 1 nettoyé : {text1}\")\n",
    "    # print(f\"Texte 2 nettoyé : {text2}\")\n",
    "\n",
    "    # Create TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    return similarity[0][0]  # Return the similarity score (0 to 1)\n",
    "\n",
    "\n",
    "\n",
    "def build_similarity_matrix(data , method) -> pd.DataFrame:\n",
    "    # Initialize an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Loop through all pairs of texts\n",
    "    for i in range(len(data)):\n",
    "        for j in range(i+1, len(data)):\n",
    "            # Calculate similarity between the two texts\n",
    "            if method == \"texte\":\n",
    "                similarity = calculate_similarity_texte(data[i], data[j])\n",
    "            elif method == \"notes\":\n",
    "                similarity = calculate_similarity_notes(data[i], data[j])\n",
    "            elif method == \"geographique\":\n",
    "                similarity = haversine(data[i][0], data[i][1], data[j][0], data[j][1])\n",
    "            # Add the similarity to the DataFrame\n",
    "            df = df.append({\n",
    "                \"restaurant1\": data[i],\n",
    "                \"restaurant2\": data[j],\n",
    "                \"similarity\": similarity\n",
    "            }, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# def construireMesMatrix(db):\n",
    "#     data = join_restaurant_avis_date( db)\n",
    "#     # print(data)\n",
    "#     return data\n",
    "\n",
    "# #cconstruction  de la matrice de similarité\n",
    "# def build_similarity_matrix(texts: List[str]) -> pd.DataFrame:\n",
    "#     # Initialize an empty DataFrame\n",
    "#     df = pd.DataFrame()\n",
    "    \n",
    "#     # Loop through all pairs of texts\n",
    "#     for i in range(len(texts)):\n",
    "#         for j in range(i+1, len(texts)):\n",
    "#             # Calculate similarity between the two texts\n",
    "#             similarity = calculate_similarity(texts[i], texts[j])\n",
    "            \n",
    "#             # Add the similarity to the DataFrame\n",
    "#             df = df.append({\n",
    "#                 \"text1\": texts[i],\n",
    "#                 \"text2\": texts[j],\n",
    "#                 \"similarity\": similarity\n",
    "#             }, ignore_index=True)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# Example texts\n",
    "# text1 = \"Bonjour, je suis un texte de test\"\n",
    "# text2 = \"Bonjour, je suis un autre texte de test\"\n",
    "\n",
    "\n",
    "\n",
    "# Calculate similarity\n",
    "# similarity_score = calculate_similarity(text1, text2)\n",
    "# print(f\"La similarité entre les deux textes est : {similarity_score:.2f}\")\n",
    "# db = next(get_db())\n",
    "# contruireDF = join_restaurant_avis_date(db)\n",
    "# print(contruireDF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte 1 nettoyé : bonjour texte test\n",
      "Texte 2 nettoyé : bonjour autre texte test\n",
      "La similarité entre les deux textes est : 0.78\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from sqlalchemy.orm import Session\n",
    "from typing import List\n",
    "from model import models, schemas\n",
    "from utils import database\n",
    "\n",
    "\n",
    "# Database dependency\n",
    "def get_db():\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "def join_restaurant_avis_date(db: Session) -> pd.DataFrame:\n",
    "    # Join the DimRestaurant and FactAvis and data and select the fields\n",
    "    query = db.query(models.FactAvis, models.DimRestaurant).join(models.DimRestaurant)\n",
    "\n",
    "    # Execute the query\n",
    "    \n",
    "    \n",
    "    results = query.all()\n",
    "    \n",
    "    # Create a DataFrame with the results\n",
    "    df = pd.DataFrame([{\n",
    "        \"avis\": avis.commentaire,\n",
    "        \"date\": avis.date,\n",
    "        \"restaurant\": restaurant.nom\n",
    "    } for avis, restaurant in results])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "# List of stopwords\n",
    "mots_vides = stopwords.words(\"french\")\n",
    "\n",
    "# List of punctuations and digits\n",
    "ponctuations = string.punctuation\n",
    "chiffres = string.digits\n",
    "\n",
    "# Function to clean and tokenize document\n",
    "def nettoyage_doc(doc_param):\n",
    "    # Convert to lowercase\n",
    "    doc = doc_param.lower()\n",
    "    # Remove punctuations\n",
    "    doc = \"\".join([w for w in list(doc) if not w in ponctuations])\n",
    "    # Remove digits\n",
    "    # doc = \"\".join([w for w in list(doc) if not w in chiffres])\n",
    "    # Tokenize the document\n",
    "    doc = word_tokenize(doc)\n",
    "    # Lemmatize each term\n",
    "    doc = [lem.lemmatize(terme) for terme in doc]\n",
    "    # Remove stopwords\n",
    "    doc = [w for w in doc if not w in mots_vides]\n",
    "    # Remove terms with less than 3 characters if they are not numeric\n",
    "    doc = [w for w in doc if len(w) >= 3 or w.isnumeric()]\n",
    "    return doc\n",
    "\n",
    "def calculate_similarity(text1: str, text2: str) -> float:\n",
    "    # Clean and tokenize the texts\n",
    "    doc1 = nettoyage_doc(text1)\n",
    "    doc2 = nettoyage_doc(text2)\n",
    "    \n",
    "    # Join tokens back to strings\n",
    "    text1 = \" \".join(doc1)\n",
    "    text2 = \" \".join(doc2)\n",
    "    \n",
    "    print(f\"Texte 1 nettoyé : {text1}\")\n",
    "    print(f\"Texte 2 nettoyé : {text2}\")\n",
    "\n",
    "    # Create TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "    return similarity[0][0]  # Return the similarity score (0 to 1)\n",
    "\n",
    "# Example texts\n",
    "text1 = \"Bonjour, je suis un texte de test\"\n",
    "text2 = \"Bonjour, je suis un autre texte de test\"\n",
    "\n",
    "# Calculate similarity\n",
    "similarity_score = calculate_similarity(text1, text2)\n",
    "\n",
    "print(f\"La similarité entre les deux textes est : {similarity_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
