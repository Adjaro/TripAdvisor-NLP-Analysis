{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d7171160-Reviews-KUMA_cantine-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration du proxy ScraperAPI\n",
    "SCRAPER_API_KEY = \"3dae357cf3aebcf599e69aa21964b29fab07e7cfdfd65cce5de27ff6162aa8e1\"  # Remplacez par votre clé API ScraperAPI\n",
    "proxies = {\n",
    "    \"https\": f\"http://scraperapi:{SCRAPER_API_KEY}@proxy-server.scraperapi.com:8001\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Scraper les informations d'un restaurant\n",
    "def scrape_restaurant(restaurant_url):\n",
    "    try:\n",
    "        response = requests.get(restaurant_url, proxies=proxies, headers=headers, verify=False)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            nom = soup.find('h1', class_='rRtyp').text.strip() if soup.find('h1', class_='rRtyp') else \"Nom non trouvé\"\n",
    "            localisation = soup.find('div', class_='biGQs _P pZUbB hmDzD').text.strip() if soup.find('div', class_='biGQs _P pZUbB hmDzD') else \"Localisation non trouvée\"\n",
    "            categorie = \"Restaurant\"\n",
    "            tags = soup.find('span', class_='VdWAl').text.strip() if soup.find('span', class_='VdWAl') else \"Tags non trouvés\"\n",
    "            \n",
    "            note_globale = None\n",
    "            note_div = soup.find('div', class_='biGQs _P fiohW hzzSG uuBRH')\n",
    "            note_span = soup.find('span', class_='biGQs _P fiohW uuBRH')\n",
    "            \n",
    "            if note_div:\n",
    "                note_globale = note_div.text.strip()\n",
    "            elif note_span:\n",
    "                note_globale = note_span.text.strip()\n",
    "\n",
    "            if note_globale:\n",
    "                note_globale = float(note_globale.replace(\",\", \".\"))\n",
    "            else:\n",
    "                note_globale = 0.0\n",
    "\n",
    "            return {\n",
    "                \"nom\": nom,\n",
    "                \"localisation\": localisation,\n",
    "                \"categorie\": categorie,\n",
    "                \"tags\": tags,\n",
    "                \"note_globale\": note_globale,\n",
    "            }\n",
    "        else:\n",
    "            logger.error(f\"Erreur lors de la récupération de la page {restaurant_url}, code de statut {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors du scraping du restaurant {restaurant_url} : {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "# Enregistrer dans un fichier CSV\n",
    "def save_to_csv(data, filename=\"restaurants.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    logger.info(f\"Données enregistrées dans {filename}\")\n",
    "\n",
    "# Script principal\n",
    "def main():\n",
    "    restaurant_urls = [\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d7171160-Reviews-KUMA_cantine-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "    ]\n",
    "\n",
    "    restaurant_urls = list(set(restaurant_urls))  # Supprimer les doublons\n",
    "    all_restaurants = []\n",
    "\n",
    "    try:\n",
    "        for idx, url in enumerate(restaurant_urls, start=1):\n",
    "            logger.info(f\"Scraping des informations pour : {url}\")\n",
    "            restaurant_data = scrape_restaurant(url)\n",
    "            if restaurant_data:\n",
    "                restaurant_data[\"id_restaurant\"] = idx\n",
    "                all_restaurants.append(restaurant_data)\n",
    "                time.sleep(random.uniform(5, 20))\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur générale : {e}\", exc_info=True)\n",
    "\n",
    "    if all_restaurants:\n",
    "        save_to_csv(all_restaurants)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d7171160-Reviews-KUMA_cantine-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la récupération de la page https://www.tripadvisor.fr/Restaurant_Review-g187265-d7171160-Reviews-KUMA_cantine-Lyon_Rhone_Auvergne_Rhone_Alpes.html, code de statut 401\n",
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d1330943-Reviews-Mattsam_Restaurant_Messob-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la récupération de la page https://www.tripadvisor.fr/Restaurant_Review-g187265-d1330943-Reviews-Mattsam_Restaurant_Messob-Lyon_Rhone_Auvergne_Rhone_Alpes.html, code de statut 401\n",
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d7698838-Reviews-Brasserie_des_Confluences-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la récupération de la page https://www.tripadvisor.fr/Restaurant_Review-g187265-d7698838-Reviews-Brasserie_des_Confluences-Lyon_Rhone_Auvergne_Rhone_Alpes.html, code de statut 401\n",
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d715010-Reviews-Christian_Tetedoie-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la récupération de la page https://www.tripadvisor.fr/Restaurant_Review-g187265-d715010-Reviews-Christian_Tetedoie-Lyon_Rhone_Auvergne_Rhone_Alpes.html, code de statut 401\n",
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d6694423-Reviews-Gang_Nam-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la récupération de la page https://www.tripadvisor.fr/Restaurant_Review-g187265-d6694423-Reviews-Gang_Nam-Lyon_Rhone_Auvergne_Rhone_Alpes.html, code de statut 401\n",
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d4338972-Reviews-Creperie_La_Marie_Morgane-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la récupération de la page https://www.tripadvisor.fr/Restaurant_Review-g187265-d4338972-Reviews-Creperie_La_Marie_Morgane-Lyon_Rhone_Auvergne_Rhone_Alpes.html, code de statut 401\n",
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d15130370-Reviews-Fiston_Bouchon_Lyonnais-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la récupération de la page https://www.tripadvisor.fr/Restaurant_Review-g187265-d15130370-Reviews-Fiston_Bouchon_Lyonnais-Lyon_Rhone_Auvergne_Rhone_Alpes.html, code de statut 401\n",
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d3727154-Reviews-Les_Terrasses_de_Lyon-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la récupération de la page https://www.tripadvisor.fr/Restaurant_Review-g187265-d3727154-Reviews-Les_Terrasses_de_Lyon-Lyon_Rhone_Auvergne_Rhone_Alpes.html, code de statut 401\n",
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d6694423-Reviews-Gang_Nam-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la récupération de la page https://www.tripadvisor.fr/Restaurant_Review-g187265-d6694423-Reviews-Gang_Nam-Lyon_Rhone_Auvergne_Rhone_Alpes.html, code de statut 401\n",
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d9597301-Reviews-Kenbo-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur lors de la récupération de la page https://www.tripadvisor.fr/Restaurant_Review-g187265-d9597301-Reviews-Kenbo-Lyon_Rhone_Auvergne_Rhone_Alpes.html, code de statut 401\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import time \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Configuration du proxy ScraperAPI\n",
    "SCRAPER_API_KEY = \"3dae357cf3aebcf599e69aa21964b29fab07e7cfdfd65cce5de27ff6162aa8e1\"  # Remplacez par votre clé API ScraperAPI\n",
    "proxies = {\n",
    "    \"https\": f\"http://scraperapi:{SCRAPER_API_KEY}@proxy-server.scraperapi.com:8001\"\n",
    "}\n",
    "\n",
    "# Scraper les informations d'un restaurant\n",
    "def scrape_restaurant(restaurant_url):\n",
    "    try:\n",
    "        # Faire une requête HTTP avec le proxy ScraperAPI\n",
    "        response = requests.get(restaurant_url, proxies=proxies, verify=False)\n",
    "        \n",
    "        # Si la requête est réussie\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extraire les informations du restaurant\n",
    "            nom = soup.find('h1', class_='rRtyp').text.strip() if soup.find('h1', class_='rRtyp') else \"Nom non trouvé\"\n",
    "            \n",
    "            localisation = soup.find('div', class_='biGQs _P pZUbB hmDzD').text.strip() if soup.find('div', class_='biGQs _P pZUbB hmDzD') else \"Localisation non trouvée\"\n",
    "            categorie = \"Restaurant\"\n",
    "            tags = soup.find('span', class_='VdWAl').text.strip() if soup.find('span', class_='VdWAl') else \"Tags non trouvés\"\n",
    "            # Chercher la note globale dans un <div> ou un <span>\n",
    "            note_globale = None\n",
    "            note_div = soup.find('div', class_='biGQs _P fiohW hzzSG uuBRH')\n",
    "            note_span = soup.find('span', class_='biGQs _P fiohW uuBRH')\n",
    "            \n",
    "            if note_div:\n",
    "                note_globale = note_div.text.strip()\n",
    "            elif note_span:\n",
    "                note_globale = note_span.text.strip()\n",
    "\n",
    "            # Convertir la note en float, en remplaçant la virgule par un point si nécessaire\n",
    "            if note_globale:\n",
    "                note_globale = float(note_globale.replace(\",\", \".\"))\n",
    "            else:\n",
    "                note_globale = 0.0\n",
    "\n",
    "\n",
    "            # Retourner les informations sous forme de dictionnaire\n",
    "            return {\n",
    "                \n",
    "                \"nom\": nom,\n",
    "                \"localisation\": localisation,\n",
    "                \"categorie\": categorie,\n",
    "                \"tags\": tags,\n",
    "                \"note_globale\": note_globale,\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Erreur lors de la récupération de la page {restaurant_url}, code de statut {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du scraping du restaurant {restaurant_url} : {e}\")\n",
    "        return None\n",
    "\n",
    "# Enregistrer dans un fichier CSV\n",
    "def save_to_csv(data, filename=\"restaurants.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"Données enregistrées dans {filename}\")\n",
    "\n",
    "# Script principal\n",
    "def main():\n",
    "    # Liste des URLs des restaurants à scraper\n",
    "    restaurant_urls = [\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d7171160-Reviews-KUMA_cantine-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d1330943-Reviews-Mattsam_Restaurant_Messob-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d7698838-Reviews-Brasserie_des_Confluences-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d715010-Reviews-Christian_Tetedoie-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d6694423-Reviews-Gang_Nam-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d4338972-Reviews-Creperie_La_Marie_Morgane-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d15130370-Reviews-Fiston_Bouchon_Lyonnais-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d3727154-Reviews-Les_Terrasses_de_Lyon-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d6694423-Reviews-Gang_Nam-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d9597301-Reviews-Kenbo-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"\n",
    "    ]\n",
    "\n",
    "    all_restaurants = []  # Liste pour stocker les informations des restaurants\n",
    "\n",
    "    try:\n",
    "        restaurant_id = 1  # Initialiser le compteur\n",
    "        for url in restaurant_urls:\n",
    "            print(f\"Scraping des informations pour : {url}\")\n",
    "            restaurant_data = scrape_restaurant(url)\n",
    "            if restaurant_data:\n",
    "                restaurant_data[\"id_restaurant\"] = restaurant_id  # Ajouter l'ID\n",
    "                all_restaurants.append(restaurant_data)\n",
    "                restaurant_id += 1\n",
    "                time.sleep(random.uniform(10, 30))  # Pause aléatoire entre les requêtes\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur générale : {e}\")\n",
    "\n",
    "    # Sauvegarder les données dans un fichier CSV\n",
    "    if all_restaurants:\n",
    "        save_to_csv(all_restaurants)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import uuid\n",
    "from dateutil import parser\n",
    "from utils import database\n",
    "from model import models, schemas\n",
    "from functools import lru_cache\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "models.Base.metadata.create_all(bind=database.engine)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def get_month_mapping():\n",
    "    return {\n",
    "        'janvier': 'January', 'février': 'February', 'mars': 'March',\n",
    "        'avril': 'April', 'mai': 'May', 'juin': 'June', 'juillet': 'July',\n",
    "        'août': 'August', 'septembre': 'September', 'octobre': 'October',\n",
    "        'novembre': 'November', 'décembre': 'December'\n",
    "    }\n",
    "\n",
    "def parse_date(date_str):\n",
    "    try:\n",
    "        fr_to_en = get_month_mapping()\n",
    "        day, month, year = date_str.split(' ')\n",
    "        month_en = fr_to_en[month.lower()]\n",
    "        date_en = f\"{day} {month_en} {year}\"\n",
    "        return parser.parse(date_en, dayfirst=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing date {date_str}: {e}\")\n",
    "\n",
    "# Charger un fichier JSON\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Obtenir la liste des fichiers JSON\n",
    "def get_data_list(data_dir='./data'):\n",
    "    return [f for f in os.listdir(data_dir) if f.endswith('.json')]\n",
    "\n",
    "\n",
    "# Insérer des données en base de données (optimisé pour les batchs)\n",
    "def insert_data(dict_data):\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        # Insérer la localisation\n",
    "        id_location = str(uuid.uuid4())\n",
    "        dict_location = {\n",
    "            'id_location': id_location,\n",
    "            'longitude': dict_data['longitude'],\n",
    "            'latitude': dict_data['latitude'],\n",
    "            'adresse': dict_data['adresse']\n",
    "        }\n",
    "        location = models.DimLocation(**schemas.DimLocation(**dict_location).model_dump())\n",
    "        db.add(location)\n",
    "\n",
    "        # Insérer le restaurant\n",
    "        id_restaurant = str(uuid.uuid4())\n",
    "        dict_restaurant = {\n",
    "            'id_restaurant': id_restaurant,\n",
    "            'nom': dict_data['nom'],\n",
    "            'id_location': id_location\n",
    "        }\n",
    "        restaurant = models.DimRestaurant(**schemas.DimRestaurant(**dict_restaurant).model_dump())\n",
    "        db.add(restaurant)\n",
    "\n",
    "        # Préparer les entrées pour les avis et les dates\n",
    "        avis_entries = []\n",
    "        date_entries = []\n",
    "\n",
    "        for avis in dict_data['avis']:\n",
    "            # Insérer la date\n",
    "            id_date = str(uuid.uuid4())\n",
    "            date_temp = parse_date(avis['date'])\n",
    "            jour_temp, mois_temp, annee_temp = avis['date'].split(' ')\n",
    "            dict_time = {\n",
    "                'id_date': id_date,\n",
    "                'date': date_temp,\n",
    "                'mois': str(mois_temp),\n",
    "                'annee': str(annee_temp),\n",
    "                'jour': str(jour_temp),\n",
    "            }\n",
    "            date_entry = models.DimDate(**schemas.DimDate(**dict_time).model_dump())\n",
    "            date_entries.append(date_entry)\n",
    "\n",
    "            # Insérer l'avis\n",
    "            id_avis = str(uuid.uuid4())\n",
    "            dict_avis = {\n",
    "                'id_avis': id_avis,\n",
    "                'id_restaurant': id_restaurant,\n",
    "                'id_date': id_date,\n",
    "                'note': avis['nb_etoiles']\n",
    "            }\n",
    "            avis_entry = models.FaitAvis(**schemas.FaitAvis(**dict_avis).model_dump())\n",
    "            avis_entries.append(avis_entry)\n",
    "\n",
    "        # Exécuter les insertions groupées\n",
    "        db.add_all(date_entries)\n",
    "        db.add_all(avis_entries)\n",
    "        db.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur : {e}\")\n",
    "        db.rollback()\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "# Charger tous les fichiers JSON en mémoire\n",
    "def load_all_json(data_dir='./data'):\n",
    "    data_list = []\n",
    "    for file in get_data_list(data_dir):\n",
    "        data = read_json_file(f'{data_dir}/{file}')\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "# Insérer les données des fichiers JSON\n",
    "def insert_json_data(data_dir='./data'):\n",
    "    all_data = load_all_json(data_dir)\n",
    "    for data in all_data:\n",
    "        insert_data(data)\n",
    "\n",
    "# Lancer l'importation\n",
    "if __name__ == \"__main__\":\n",
    "    insert_json_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from fastapi import BackgroundTasks\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "async def scrape_task(url: str):\n",
    "    try:\n",
    "        scraper = TripadvisorScraper(url)\n",
    "        data = scraper.scrapper()\n",
    "        db = next(get_db())\n",
    "        try:\n",
    "            data_exists = await check_existing_data(db, data['nom'], data['adresse'])\n",
    "            if not data_exists:\n",
    "                logger.info(\"Inserting scraped data...\")\n",
    "                await insert_json_data(data)\n",
    "            else:\n",
    "                logger.info(\"Data already exists in database\")\n",
    "        finally:\n",
    "            db.close()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during scraping: {e}\")\n",
    "        raise\n",
    "\n",
    "@app.post(\"/scrape\")\n",
    "async def scrape(background_tasks: BackgroundTasks, url: str):\n",
    "    background_tasks.add_task(scrape_task, url)\n",
    "    return {\"message\": \"Scraping in progress\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "class RestaurantScraper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.nom_restaurant = None\n",
    "        self.nb_total_commentaires = None\n",
    "        self.nb_pages = None\n",
    "        self.nb_commentaires_par_page = None\n",
    "        \n",
    "        self.driver = None\n",
    "\n",
    "    def create_driver(self):\n",
    "        service = Service('chromedriver.exe')\n",
    "        user_agents = [\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        ]\n",
    "        options = uc.ChromeOptions()\n",
    "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "        options.add_argument(\"--incognito\")\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        user_agent = random.choice(user_agents)\n",
    "        options.add_argument(f'--user-agent={user_agent}')\n",
    "        return uc.Chrome(options=options, service=service)\n",
    "\n",
    "    def handle_cookies(self):\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 30).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[id='onetrust-accept-btn-handler']\"))\n",
    "            ).click()\n",
    "        except TimeoutException:\n",
    "            print(\"Pas de bannière cookies trouvée.\")\n",
    "\n",
    "    def find_restaurant_name(self):\n",
    "        try:\n",
    "            name_element = self.driver.find_element(By.XPATH, \"//h1[@class='biGQs _P egaXP rRtyp']\")\n",
    "            print(f\"Nom trouvé : {name_element.text}\")\n",
    "            self.nom_restaurant = name_element.text\n",
    "            return name_element.text\n",
    "        except NoSuchElementException:\n",
    "            return None\n",
    "\n",
    "    def extraire_infos(self, texte):\n",
    "        texte = texte.replace(\"\\u202f\", \"\")\n",
    "        chiffres = [int(s) for s in re.findall(r'\\d+', texte)]\n",
    "        \n",
    "        if len(chiffres) >= 2:\n",
    "            nb_commentaires_par_page = chiffres[1]\n",
    "            nb_total_commentaires = chiffres[-1]\n",
    "            nb_pages = math.ceil(nb_total_commentaires / nb_commentaires_par_page)\n",
    "            self.nb_total_commentaires = nb_total_commentaires\n",
    "            self.nb_pages = nb_pages\n",
    "            self.nb_commentaires_par_page = nb_commentaires_par_page\n",
    "            return nb_commentaires_par_page, nb_total_commentaires, nb_pages\n",
    "        else:\n",
    "            return None, None, None\n",
    "\n",
    "    def scraper_infos_restaurant(self):\n",
    "        try:\n",
    "            nom = self.driver.find_element(By.XPATH, \"//h1[@class='biGQs _P egaXP rRtyp']\").text\n",
    "            adresse = self.driver.find_element(By.XPATH, \"//div[contains(text(), 'Emplacement et coordonnées')]/following::span[contains(@class, 'biGQs _P pZUbB hmDzD')][1]\").text\n",
    "            note_globale = re.search(r\"(\\d+,\\d+)\", self.driver.find_elements(By.XPATH, \"//div[@class='biGQs _P vvmrG']\")[0].text).group(1)\n",
    "            WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//span//button[@class='ypcsE _S wSSLS']\"))).click()\n",
    "            horaires = [\n",
    "                f\"{lines[0]} : {' - '.join(lines[1:])}\"\n",
    "                for e in self.driver.find_elements(\"xpath\", \"//div[@class='VFyGJ Pi']\")\n",
    "                if len(lines := e.text.splitlines()) >= 2\n",
    "            ]\n",
    "\n",
    "            time.sleep(3)\n",
    "            WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//span//button[@class='ypcsE _S wSSLS']\"))).click()\n",
    "\n",
    "            notes = self.driver.find_elements(By.XPATH, \"//div[@class='khxWm f e Q3']/div/div\")\n",
    "            note_cuisine = re.search(r'<title[^>]*>([\\d.,]+) sur [\\d.,]+', notes[1].get_attribute(\"innerHTML\")).group(1)\n",
    "            note_service = re.search(r'<title[^>]*>([\\d.,]+) sur [\\d.,]+', notes[3].get_attribute(\"innerHTML\")).group(1)\n",
    "            note_rapportqualiteprix = re.search(r'<title[^>]*>([\\d.,]+) sur [\\d.,]+', notes[5].get_attribute(\"innerHTML\")).group(1)\n",
    "            note_ambiance = re.search(r'<title[^>]*>([\\d.,]+) sur [\\d.,]+', notes[7].get_attribute(\"innerHTML\")).group(1)\n",
    "            classement_element = self.driver.find_element(By.XPATH, \"//div[contains(@class, 'biGQs _P pZUbB hmDzD')]//b/span\").text.strip()\n",
    "            classement = (re.search(r'\\d+', classement_element).group())\n",
    "\n",
    "            WebDriverWait(self.driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"button[class='UikNM _G B- _S _W _T c G_ wSSLS ACvVd']\"))).click()\n",
    "            time.sleep(2)\n",
    "\n",
    "            try:\n",
    "                infos_pratiques = self.driver.find_element(By.XPATH, \"//div[contains(@class, 'Wf') and ./div[contains(text(), 'Infos pratiques')]]/following-sibling::div[contains(@class, 'biGQs')]\").text.strip()\n",
    "            except Exception:\n",
    "                infos_pratiques = \"Non renseigné\"\n",
    "\n",
    "            try:\n",
    "                fourchette_prix = self.driver.find_element(By.XPATH, \"//div[contains(@class, 'Wf') and ./div[contains(text(), 'FOURCHETTE DE PRIX')]]/following-sibling::div[contains(@class, 'biGQs _P pZUbB alXOW oCpZu GzNcM nvOhm UTQMg ZTpaU W hmDzD')]\").text.strip().replace(\"€\", \"\").replace(\"\\xa0\", \"\")\n",
    "            except Exception:\n",
    "                fourchette_prix = \"Non renseigné\"\n",
    "\n",
    "            try:\n",
    "                types_cuisines = [item.strip() for item in self.driver.find_element(By.XPATH, \"//div[contains(@class, 'Wf') and ./div[contains(text(), 'CUISINES')]]/following-sibling::div[contains(@class, 'biGQs _P pZUbB alXOW oCpZu GzNcM nvOhm UTQMg ZTpaU W hmDzD')]\").text.strip().split(\",\")]\n",
    "            except Exception:\n",
    "                types_cuisines = \"Non renseigné\"\n",
    "\n",
    "            try:\n",
    "                regimes = [item.strip() for item in self.driver.find_element(By.XPATH, \"//div[contains(@class, 'Wf') and ./div[contains(text(), 'Régimes spéciaux')]]/following-sibling::div[contains(@class, 'biGQs _P pZUbB alXOW oCpZu GzNcM nvOhm UTQMg ZTpaU W hmDzD')]\").text.strip().split(\",\")]\n",
    "            except Exception:\n",
    "                regimes = \"Non renseigné\"\n",
    "\n",
    "            try:\n",
    "                repas = [item.strip() for item in self.driver.find_element(By.XPATH, \"//div[contains(@class, 'Wf') and ./div[contains(text(), 'Repas')]]/following-sibling::div[contains(@class, 'biGQs _P pZUbB alXOW eWlDX GzNcM ATzgx UTQMg TwpTY hmDzD')]\").text.strip().split(\",\")]\n",
    "            except Exception:\n",
    "                repas = \"Non renseigné\"\n",
    "\n",
    "            try:\n",
    "                fonctionnalites = [item.strip() for item in self.driver.find_element(By.XPATH, \"//div[contains(@class, 'Wf') and ./div[contains(text(), 'FONCTIONNALITÉS')]]/following-sibling::div[contains(@class, 'biGQs')]\").text.strip().split(\",\")]\n",
    "            except Exception:\n",
    "                fonctionnalites = \"Non renseigné\"\n",
    "\n",
    "            time.sleep(5)\n",
    "            self.driver.find_element(By.XPATH, \"//button[@aria-label='Fermer']\").click()\n",
    "            time.sleep(2)\n",
    "\n",
    "            try:\n",
    "                google_maps_link = self.driver.find_element(By.XPATH,\"//div[@class='akmhy e j']//a[@class='BMQDV _F Gv wSSLS SwZTJ FGwzt ukgoS']\").get_attribute(\"href\")\n",
    "                if \"@\" in google_maps_link:\n",
    "                    coordinates = google_maps_link.split(\"@\")[1].split(\",\")[:2]\n",
    "                    latitude, longitude = coordinates[0], coordinates[1]\n",
    "                else:\n",
    "                    latitude, longitude = \"Non renseigné\", \"Non renseigné\"\n",
    "                    print(\"Coordonnées introuvables dans le lien.\")\n",
    "            except NoSuchElementException:\n",
    "                latitude, longitude = \"Non renseigné\", \"Non renseigné\"\n",
    "                print(\"Lien Google Maps introuvable.\")\n",
    "\n",
    "            return {\n",
    "                \"nom\": nom,\n",
    "                \"adresse\": adresse,\n",
    "                \"classement\": classement,\n",
    "                \"horaires\": horaires,\n",
    "                \"note_globale\": note_globale,\n",
    "                \"note_cuisine\": note_cuisine,\n",
    "                \"note_service\": note_service,\n",
    "                \"note_rapportqualiteprix\": note_rapportqualiteprix,\n",
    "                \"note_ambiance\": note_ambiance,\n",
    "                \"infos_pratiques\": infos_pratiques,\n",
    "                \"repas\": repas,\n",
    "                \"regimes\": regimes,\n",
    "                \"fonctionnalites\": fonctionnalites,\n",
    "                \"fourchette_prix\": fourchette_prix,\n",
    "                \"types_cuisines\": types_cuisines,\n",
    "                \"latitude\": latitude,\n",
    "                \"longitude\": longitude\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'extraction des informations du restaurant : {e}\")\n",
    "            return {}\n",
    "\n",
    "    def scraper_page(self):\n",
    "        data = []\n",
    "        pseudos = self.driver.find_elements(By.XPATH, \"//span[@class='biGQs _P fiohW fOtGX']\")\n",
    "        titres = self.driver.find_elements(By.XPATH, \"//div[@class='biGQs _P fiohW qWPrE ncFvv fOtGX']\")\n",
    "        etoiles = self.driver.find_elements(By.XPATH, \"//div[@class='OSBmi J k']\")\n",
    "        nb_etoiles = [re.search(r'(\\d+),', etoile.get_attribute(\"textContent\")).group(1) for etoile in etoiles]\n",
    "        dates = [re.search(r\"\\d{1,2}\\s\\w+\\s\\d{4}\", elem.text.strip()).group(0) for elem in self.driver.find_elements(By.XPATH, \"//div[contains(@class, 'biGQs _P pZUbB ncFvv osNWb')]\")]\n",
    "        experiences = self.driver.find_elements(By.XPATH, \"//span[@class='DlAxN']\")\n",
    "        reviews = self.driver.find_elements(By.XPATH, \"//div[@data-test-target='review-body']//span[@class='JguWG' and not(ancestor::div[contains(@class, 'csNQI')])]\")\n",
    "\n",
    "        for i in range(len(titres)):\n",
    "            avis = {\n",
    "                \"pseudo\": pseudos[i].text if i < len(pseudos) else \"\",\n",
    "                \"titre_review\": titres[i].text if i < len(titres) else \"\",\n",
    "                \"nb_etoiles\": nb_etoiles[i] if i < len(nb_etoiles) else \"\",\n",
    "                \"date\": dates[i] if i < len(dates) else \"\",\n",
    "                \"experience\": experiences[i].text if i < len(experiences) else \"\",\n",
    "                \"review\": reviews[i].text if i < len(reviews) else \"\"\n",
    "            }\n",
    "            data.append(avis)\n",
    "        return data\n",
    "\n",
    "    def scraper_toutes_pages(self, nb_pages):\n",
    "        all_data = []\n",
    "        actions = ActionChains(self.driver)\n",
    "\n",
    "        for page in range(1, nb_pages + 1):\n",
    "            print(f\"Scraping de la page {page}...\")\n",
    "            time.sleep(5)\n",
    "            try:\n",
    "                data = self.scraper_page()\n",
    "                print(f\"Données collectées pour la page {page} : {len(data)} avis\")\n",
    "                all_data.extend(data)\n",
    "\n",
    "                next_button = WebDriverWait(self.driver, 50).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, \"//a[@aria-label='Page suivante']\"))\n",
    "                )\n",
    "\n",
    "                self.driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_button)\n",
    "                time.sleep(5)\n",
    "                actions.move_to_element(next_button).click().perform()\n",
    "\n",
    "                print(\"Page suivante chargée.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Erreur rencontrée à la page {page} : {e}\")\n",
    "                break\n",
    "\n",
    "        return all_data\n",
    "\n",
    "    def test_scraping(self, nbPages_texte):\n",
    "        avis = []\n",
    "        infos_restaurant = {\n",
    "            \"nom\": \"Non disponible\",\n",
    "            \"adresse\": \"Non disponible\",\n",
    "            \"classement\": \"Non disponible\",\n",
    "            \"horaires\": [],\n",
    "            \"note_globale\": \"Non disponible\",\n",
    "            \"note_cuisine\": \"Non disponible\",\n",
    "            \"note_service\": \"Non disponible\",\n",
    "            \"note_rapportqualiteprix\": \"Non disponible\",\n",
    "            \"note_ambiance\": \"Non disponible\",\n",
    "            \"repas\": \"Non disponible\",\n",
    "            \"infos_pratiques\": \"Non disponible\",\n",
    "            \"regimes\": [],\n",
    "            \"fonctionnalites\": \"Non disponible\",\n",
    "            \"fourchette_prix\": \"Non disponible\",\n",
    "            \"types_cuisines\": [],\n",
    "            \"latitude\" : \"Non disponible\",\n",
    "            \"longitude\" : \"Non disponible\",\n",
    "        }\n",
    "        try:\n",
    "            infos_restaurant = self.scraper_infos_restaurant()\n",
    "            nb_commentaires_par_page, nb_total_commentaires, nb_pages = self.extraire_infos(nbPages_texte)\n",
    "\n",
    "            average_time_per_page = 15\n",
    "            estimated_total_time = average_time_per_page * nb_pages\n",
    "            estimated_total_time_minutes = math.ceil(estimated_total_time / 60)\n",
    "            print(f\"Temps estimé pour terminer le scraping : {estimated_total_time_minutes} minutes.\\n\")\n",
    "\n",
    "            avis = self.scraper_toutes_pages(nb_pages)\n",
    "            print(f\"Scraping terminé. Total d'avis collectés : {len(avis)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur générale : {e}\")\n",
    "\n",
    "        restaurant_data = {\n",
    "            \"nom\": infos_restaurant[\"nom\"],\n",
    "            \"adresse\": infos_restaurant[\"adresse\"],\n",
    "            \"classement\": infos_restaurant[\"classement\"],\n",
    "            \"horaires\": infos_restaurant[\"horaires\"],\n",
    "            \"note_globale\": infos_restaurant[\"note_globale\"],\n",
    "            \"note_cuisine\": infos_restaurant[\"note_cuisine\"],\n",
    "            \"note_service\": infos_restaurant[\"note_service\"],\n",
    "            \"note_rapportqualiteprix\": infos_restaurant[\"note_rapportqualiteprix\"],\n",
    "            \"note_ambiance\": infos_restaurant[\"note_ambiance\"],\n",
    "            \"infos_pratiques\": infos_restaurant[\"infos_pratiques\"],\n",
    "            \"repas\": infos_restaurant[\"repas\"],\n",
    "            \"regimes\": infos_restaurant[\"regimes\"],\n",
    "            \"fourchette_prix\": infos_restaurant[\"fourchette_prix\"],\n",
    "            \"fonctionnalités\": infos_restaurant[\"fonctionnalites\"],\n",
    "            \"type_cuisines\": infos_restaurant[\"types_cuisines\"],\n",
    "            \"latitude\": infos_restaurant[\"latitude\"],\n",
    "            \"longitude\": infos_restaurant[\"longitude\"],\n",
    "            \"avis\": avis\n",
    "        }\n",
    "\n",
    "        return restaurant_data\n",
    "\n",
    "    def scrapper(self):\n",
    "        found = False\n",
    "        attempts = 0\n",
    "        max_attempts = 20\n",
    "\n",
    "        while not found and attempts < max_attempts:\n",
    "            self.driver = self.create_driver()\n",
    "            try:\n",
    "                self.driver.get(self.url)\n",
    "                time.sleep(3)\n",
    "                self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "                time.sleep(3)\n",
    "                self.handle_cookies()\n",
    "\n",
    "                if self.find_restaurant_name():\n",
    "                    found = True\n",
    "            except NoSuchElementException:\n",
    "                print(f\"Nom non trouvé, tentative {attempts + 1}/{max_attempts}. Redémarrage...\")\n",
    "                attempts += 1\n",
    "                self.cleanup()\n",
    "                time.sleep(10)\n",
    "\n",
    "        if not found:\n",
    "            print(\"Échec : le nom n'a pas été trouvé après plusieurs tentatives.\")\n",
    "            self.cleanup()\n",
    "        else:\n",
    "            print(\"Le nom a été trouvé avec succès. Le navigateur reste ouvert.\")\n",
    "            nbPages_texte = self.driver.find_element(\"xpath\", \"//div[@class='Ci']\").text\n",
    "            data = self.test_scraping(nbPages_texte)\n",
    "            self.cleanup()\n",
    "            return data\n",
    "\n",
    "    def cleanup(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            time.sleep(2)\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        self.cleanup()\n",
    "\n",
    "    def save_data(self, data):\n",
    "        pass\n",
    "\n",
    "    \n",
    "# def main():\n",
    "#     url = \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d5539701-Reviews-L_Institut_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"\n",
    "#     scraper = RestaurantScraper(url)\n",
    "#     data = scraper.scrapper()\n",
    "#     print(data)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agastache_Restaurant.json', 'Aromatic_Restaurant.json', 'BLO_Restaurant.json', 'Bouchon_Les_Lyonnais.json', 'Chez_Antonin.json', 'Chez_Micheline.json', 'Copains_Copines_Sur_la_Colline.json', 'Frazarin_Bistrot_Franco_Italien.json', 'LActeur.json', 'Laffreux_Jojo.json', 'La_Criee.json', 'La_Table_de_Max.json', 'Les_3_Dômes.json', 'Le_Conde.json', 'Le_Desjeuneur.json', 'Le_Grand_Réfectoire.json', 'Le_Palais_Saint_Jean.json', 'Le_Vieux_Lyon.json', 'LInstitution.json', 'LInstitut_Restaurant.json', 'Mama_Restaurant_Lyon.json', 'Monsieur_P.json', 'Restaurant_Le_Musée.json', 'Restaurant_Lounge_N133.json', 'Restaurant_Opaline.json']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 140\u001b[0m\n\u001b[0;32m    134\u001b[0m         insert_data(data)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# import os\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# print(os.getcwd())\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[43minsert_json_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 134\u001b[0m, in \u001b[0;36minsert_json_data\u001b[1;34m(data_dir)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m json_files:\n\u001b[0;32m    133\u001b[0m     data \u001b[38;5;241m=\u001b[39m read_json_file(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 134\u001b[0m     \u001b[43minsert_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 120\u001b[0m, in \u001b[0;36minsert_data\u001b[1;34m(dict_data)\u001b[0m\n\u001b[0;32m    118\u001b[0m         avis_entry \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mFaitAvis(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdim_avis\u001b[38;5;241m.\u001b[39mmodel_dump())\n\u001b[0;32m    119\u001b[0m         db\u001b[38;5;241m.\u001b[39madd(avis_entry)\n\u001b[1;32m--> 120\u001b[0m         \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m         db\u001b[38;5;241m.\u001b[39mrefresh(avis_entry)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\orm\\session.py:2028\u001b[0m, in \u001b[0;36mSession.commit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trans \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2026\u001b[0m     trans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_autobegin_t()\n\u001b[1;32m-> 2028\u001b[0m \u001b[43mtrans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:2\u001b[0m, in \u001b[0;36mcommit\u001b[1;34m(self, _to_root)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\orm\\state_changes.py:139\u001b[0m, in \u001b[0;36m_StateChange.declare_states.<locals>._go\u001b[1;34m(fn, self, *arg, **kw)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_state \u001b[38;5;241m=\u001b[39m _StateChangeStates\u001b[38;5;241m.\u001b[39mCHANGE_IN_PROGRESS\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 139\u001b[0m     ret_value \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\orm\\session.py:1313\u001b[0m, in \u001b[0;36mSessionTransaction.commit\u001b[1;34m(self, _to_root)\u001b[0m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SessionTransactionState\u001b[38;5;241m.\u001b[39mPREPARED:\n\u001b[0;32m   1312\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expect_state(SessionTransactionState\u001b[38;5;241m.\u001b[39mPREPARED):\n\u001b[1;32m-> 1313\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnested:\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m conn, trans, should_commit, autoclose \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\n\u001b[0;32m   1317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connections\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m   1318\u001b[0m     ):\n",
      "File \u001b[1;32m<string>:2\u001b[0m, in \u001b[0;36m_prepare_impl\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\orm\\state_changes.py:139\u001b[0m, in \u001b[0;36m_StateChange.declare_states.<locals>._go\u001b[1;34m(fn, self, *arg, **kw)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_state \u001b[38;5;241m=\u001b[39m _StateChangeStates\u001b[38;5;241m.\u001b[39mCHANGE_IN_PROGRESS\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 139\u001b[0m     ret_value \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\orm\\session.py:1288\u001b[0m, in \u001b[0;36mSessionTransaction._prepare_impl\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39m_is_clean():\n\u001b[0;32m   1287\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1288\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mFlushError(\n\u001b[0;32m   1291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOver 100 subsequent flushes have occurred within \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession.commit() - is an after_flush() hook \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1293\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreating new objects?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1294\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\orm\\session.py:4352\u001b[0m, in \u001b[0;36mSession.flush\u001b[1;34m(self, objects)\u001b[0m\n\u001b[0;32m   4350\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   4351\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flushing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 4352\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4353\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   4354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flushing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\orm\\session.py:4488\u001b[0m, in \u001b[0;36mSession._flush\u001b[1;34m(self, objects)\u001b[0m\n\u001b[0;32m   4486\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   4487\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m util\u001b[38;5;241m.\u001b[39msafe_reraise():\n\u001b[1;32m-> 4488\u001b[0m         transaction\u001b[38;5;241m.\u001b[39mrollback(_capture_exception\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:146\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[1;34m(self, type_, value, traceback)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_value\u001b[38;5;241m.\u001b[39mwith_traceback(exc_tb)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exc_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# remove potential circular references\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\orm\\session.py:4448\u001b[0m, in \u001b[0;36mSession._flush\u001b[1;34m(self, objects)\u001b[0m\n\u001b[0;32m   4446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_on_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   4447\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 4448\u001b[0m     \u001b[43mflush_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4449\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   4450\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_on_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\orm\\unitofwork.py:466\u001b[0m, in \u001b[0;36mUOWTransaction.execute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m topological\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdependencies, postsort_actions):\n\u001b[1;32m--> 466\u001b[0m         \u001b[43mrec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\orm\\unitofwork.py:642\u001b[0m, in \u001b[0;36mSaveUpdateAll.execute\u001b[1;34m(self, uow)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;129m@util\u001b[39m\u001b[38;5;241m.\u001b[39mpreload_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlalchemy.orm.persistence\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, uow):\n\u001b[1;32m--> 642\u001b[0m     \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreloaded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morm_persistence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_obj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43muow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstates_for_mapper_hierarchy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43muow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\orm\\persistence.py:93\u001b[0m, in \u001b[0;36msave_obj\u001b[1;34m(base_mapper, states, uowtransaction, single)\u001b[0m\n\u001b[0;32m     81\u001b[0m     update \u001b[38;5;241m=\u001b[39m _collect_update_commands(\n\u001b[0;32m     82\u001b[0m         uowtransaction, table, states_to_update\n\u001b[0;32m     83\u001b[0m     )\n\u001b[0;32m     85\u001b[0m     _emit_update_statements(\n\u001b[0;32m     86\u001b[0m         base_mapper,\n\u001b[0;32m     87\u001b[0m         uowtransaction,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m         update,\n\u001b[0;32m     91\u001b[0m     )\n\u001b[1;32m---> 93\u001b[0m     \u001b[43m_emit_insert_statements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43muowtransaction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43minsert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m _finalize_insert_update_commands(\n\u001b[0;32m    102\u001b[0m     base_mapper,\n\u001b[0;32m    103\u001b[0m     uowtransaction,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m     ),\n\u001b[0;32m    120\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\orm\\persistence.py:1048\u001b[0m, in \u001b[0;36m_emit_insert_statements\u001b[1;34m(base_mapper, uowtransaction, mapper, table, insert, bookkeeping, use_orm_insert_stmt, execution_options)\u001b[0m\n\u001b[0;32m   1045\u001b[0m records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(records)\n\u001b[0;32m   1046\u001b[0m multiparams \u001b[38;5;241m=\u001b[39m [rec[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m records]\n\u001b[1;32m-> 1048\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution_options\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bookkeeping:\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[0;32m   1053\u001b[0m         (\n\u001b[0;32m   1054\u001b[0m             state,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1063\u001b[0m         last_inserted_params,\n\u001b[0;32m   1064\u001b[0m     ) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(records, result\u001b[38;5;241m.\u001b[39mcontext\u001b[38;5;241m.\u001b[39mcompiled_parameters):\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1418\u001b[0m, in \u001b[0;36mConnection.execute\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(statement) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNO_OPTIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\sql\\elements.py:515\u001b[0m, in \u001b[0;36mClauseElement._execute_on_connection\u001b[1;34m(self, connection, distilled_params, execution_options)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, Executable)\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_clauseelement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistilled_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_options\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObjectNotExecutableError(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1640\u001b[0m, in \u001b[0;36mConnection._execute_clauseelement\u001b[1;34m(self, elem, distilled_parameters, execution_options)\u001b[0m\n\u001b[0;32m   1628\u001b[0m compiled_cache: Optional[CompiledCacheType] \u001b[38;5;241m=\u001b[39m execution_options\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   1629\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiled_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_compiled_cache\n\u001b[0;32m   1630\u001b[0m )\n\u001b[0;32m   1632\u001b[0m compiled_sql, extracted_params, cache_hit \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_compile_w_cache(\n\u001b[0;32m   1633\u001b[0m     dialect\u001b[38;5;241m=\u001b[39mdialect,\n\u001b[0;32m   1634\u001b[0m     compiled_cache\u001b[38;5;241m=\u001b[39mcompiled_cache,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1638\u001b[0m     linting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\u001b[38;5;241m.\u001b[39mcompiler_linting \u001b[38;5;241m|\u001b[39m compiler\u001b[38;5;241m.\u001b[39mWARN_LINTING,\n\u001b[0;32m   1639\u001b[0m )\n\u001b[1;32m-> 1640\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_compiled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1645\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiled_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextracted_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_hit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_hit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1651\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_events:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_execute(\n\u001b[0;32m   1654\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1655\u001b[0m         elem,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1659\u001b[0m         ret,\n\u001b[0;32m   1660\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1846\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_insertmany_context(dialect, context)\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1986\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1983\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39m_setup_result_proxy()\n\u001b[0;32m   1985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1986\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1987\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1988\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1990\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\engine\\base.py:2358\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[0;32m   2356\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2357\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2358\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m   2359\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2360\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reentrant_error\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\engine\\base.py:1967\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1965\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1967\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1968\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[0;32m   1972\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[0;32m   1973\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1974\u001b[0m         cursor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1978\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[0;32m   1979\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\sqlalchemy\\engine\\default.py:941\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 941\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from utils import database\n",
    "from model import models, schemas\n",
    "import os\n",
    "import datetime\n",
    "import uuid\n",
    "import locale\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "\n",
    "# Configurer la locale en français\n",
    "locale.setlocale(locale.LC_TIME, 'fr_FR.UTF-8')\n",
    "\n",
    "# def get_db():\n",
    "#     db = database.SessionLocal()\n",
    "#     try:\n",
    "#         yield db\n",
    "#     finally:\n",
    "#         db.close()\n",
    "\n",
    "# models.Base.metadata.create_all(bind=database.engine)\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def get_data_list(data_dir='./data'):\n",
    "    json_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]\n",
    "    return json_files\n",
    "\n",
    "def parse_date(date_str):\n",
    "    # French to English month mappings\n",
    "    fr_to_en = {\n",
    "        'janvier': 'January',\n",
    "        'février': 'February',\n",
    "        'mars': 'March',\n",
    "        'avril': 'April',\n",
    "        'mai': 'May',\n",
    "        'juin': 'June',\n",
    "        'juillet': 'July',\n",
    "        'août': 'August',\n",
    "        'septembre': 'September',\n",
    "        'octobre': 'October',\n",
    "        'novembre': 'November',\n",
    "        'décembre': 'December'\n",
    "    }\n",
    "    \n",
    "    day, month, year = date_str.split(' ')\n",
    "    \n",
    "    month_en = fr_to_en[month.lower()]\n",
    "    \n",
    "    date_en = f\"{day} {month_en} {year}\"\n",
    "    \n",
    "    # Parse the English date string\n",
    "    return parser.parse(date_en, dayfirst=True)\n",
    "\n",
    "def insert_data(dict_data):\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        # Insert location\n",
    "        id_location = str(uuid.uuid4())\n",
    "        dict_location = {\n",
    "            'id_location': id_location,\n",
    "            'longitude': dict_data['longitude'],\n",
    "            'latitude': dict_data['latitude'],\n",
    "            'adresse': dict_data['adresse']\n",
    "        }\n",
    "        dim_location = schemas.DimLocation(**dict_location)\n",
    "        location = models.DimLocation(**dim_location.model_dump())\n",
    "        db.add(location)\n",
    "        db.commit()\n",
    "        db.refresh(location)\n",
    "\n",
    "        # Insert restaurant\n",
    "        id_restaurant = str(uuid.uuid4())\n",
    "        dict_restaurant = {\n",
    "            'id_restaurant': id_restaurant,\n",
    "            'nom': dict_data['nom'],\n",
    "            'id_location': id_location\n",
    "        }\n",
    "        dim_restaurant = schemas.DimRestaurant(**dict_restaurant)\n",
    "        restaurant = models.DimRestaurant(**dim_restaurant.model_dump())\n",
    "        db.add(restaurant)\n",
    "        db.commit()\n",
    "        db.refresh(restaurant)\n",
    "\n",
    "        # Insert avis\n",
    "        for avis in dict_data['avis']:\n",
    "            # Insert date\n",
    "            id_date = str(uuid.uuid4())\n",
    "            date_temp = parse_date(avis['date'])\n",
    "\n",
    "            jour_temp ,mois_temp , annee_temp = avis['date'].split(' ')\n",
    "\n",
    "            dict_time = {\n",
    "                'id_date': id_date,\n",
    "                'date': date_temp,\n",
    "                'mois': str(mois_temp),\n",
    "                'annee': str(annee_temp),\n",
    "                'jour': str(jour_temp),\n",
    "            }\n",
    "            dim_date = schemas.DimDate(**dict_time)\n",
    "            date_entry = models.DimDate(**dim_date.model_dump())\n",
    "            db.add(date_entry)\n",
    "            db.commit()\n",
    "            db.refresh(date_entry)\n",
    "\n",
    "            # Insert avis\n",
    "            id_avis = str(uuid.uuid4())\n",
    "            dict_avis = {\n",
    "                'id_avis': id_avis,\n",
    "                'id_restaurant': id_restaurant,\n",
    "                'id_date': id_date,\n",
    "                'note': avis['nb_etoiles']\n",
    "            }\n",
    "            dim_avis = schemas.FaitAvis(**dict_avis)\n",
    "            avis_entry = models.FaitAvis(**dim_avis.model_dump())\n",
    "            db.add(avis_entry)\n",
    "            db.commit()\n",
    "            db.refresh(avis_entry)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        db.rollback()\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "def insert_json_data(data_dir='./data'):\n",
    "    json_files = get_data_list(data_dir)\n",
    "    print(json_files)\n",
    "    for file in json_files:\n",
    "        data = read_json_file(f'{data_dir}/{file}')\n",
    "        insert_data(data)\n",
    "\n",
    "# import os\n",
    "\n",
    "# print(os.getcwd())\n",
    "\n",
    "insert_json_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil in c:\\users\\ediad\\.conda\\envs\\nlpproject\\lib\\site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ediad\\.conda\\envs\\nlpproject\\lib\\site-packages (from python-dateutil) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "type(str(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import datetime\n",
    "import uuid\n",
    "import locale\n",
    "from dateutil import parser\n",
    "from utils import database\n",
    "from model import models, schemas\n",
    "from functools import lru_cache\n",
    "\n",
    "# Configurer la locale en français\n",
    "locale.setlocale(locale.LC_TIME, 'fr_FR.UTF-8')\n",
    "\n",
    "# Créer les tables uniquement si nécessaire\n",
    "models.Base.metadata.create_all(bind=database.engine)\n",
    "\n",
    "# Fonction pour ouvrir la base de données\n",
    "def get_db():\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "# Charger un fichier JSON\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Obtenir la liste des fichiers JSON\n",
    "def get_data_list(data_dir='./data'):\n",
    "    return [f for f in os.listdir(data_dir) if f.endswith('.json')]\n",
    "\n",
    "# Préparer les correspondances de mois (optimisé avec cache)\n",
    "@lru_cache(maxsize=None)\n",
    "def get_month_mapping():\n",
    "    return {\n",
    "        'janvier': 'January', 'février': 'February', 'mars': 'March',\n",
    "        'avril': 'April', 'mai': 'May', 'juin': 'June', 'juillet': 'July',\n",
    "        'août': 'August', 'septembre': 'September', 'octobre': 'October',\n",
    "        'novembre': 'November', 'décembre': 'December'\n",
    "    }\n",
    "\n",
    "# Parser les dates en utilisant les correspondances\n",
    "def parse_date(date_str):\n",
    "    fr_to_en = get_month_mapping()\n",
    "    day, month, year = date_str.split(' ')\n",
    "    month_en = fr_to_en[month.lower()]\n",
    "    date_en = f\"{day} {month_en} {year}\"\n",
    "    return parser.parse(date_en, dayfirst=True)\n",
    "\n",
    "# Insérer des données en base de données (optimisé pour les batchs)\n",
    "def insert_data(dict_data):\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        # Insérer la localisation\n",
    "        id_location = str(uuid.uuid4())\n",
    "        dict_location = {\n",
    "            'id_location': id_location,\n",
    "            'longitude': dict_data['longitude'],\n",
    "            'latitude': dict_data['latitude'],\n",
    "            'adresse': dict_data['adresse']\n",
    "        }\n",
    "        location = models.DimLocation(**schemas.DimLocation(**dict_location).model_dump())\n",
    "        db.add(location)\n",
    "\n",
    "        # Insérer le restaurant\n",
    "        id_restaurant = str(uuid.uuid4())\n",
    "        dict_restaurant = {\n",
    "            'id_restaurant': id_restaurant,\n",
    "            'nom': dict_data['nom'],\n",
    "            'id_location': id_location\n",
    "        }\n",
    "        restaurant = models.DimRestaurant(**schemas.DimRestaurant(**dict_restaurant).model_dump())\n",
    "        db.add(restaurant)\n",
    "\n",
    "        # Préparer les entrées pour les avis et les dates\n",
    "        avis_entries = []\n",
    "        date_entries = []\n",
    "\n",
    "        for avis in dict_data['avis']:\n",
    "            # Insérer la date\n",
    "            id_date = str(uuid.uuid4())\n",
    "            date_temp = parse_date(avis['date'])\n",
    "            jour_temp, mois_temp, annee_temp = avis['date'].split(' ')\n",
    "            dict_time = {\n",
    "                'id_date': id_date,\n",
    "                'date': date_temp,\n",
    "                'mois': str(mois_temp),\n",
    "                'annee': str(annee_temp),\n",
    "                'jour': str(jour_temp),\n",
    "            }\n",
    "            date_entry = models.DimDate(**schemas.DimDate(**dict_time).model_dump())\n",
    "            date_entries.append(date_entry)\n",
    "\n",
    "            # Insérer l'avis\n",
    "            id_avis = str(uuid.uuid4())\n",
    "            dict_avis = {\n",
    "                'id_avis': id_avis,\n",
    "                'id_restaurant': id_restaurant,\n",
    "                'id_date': id_date,\n",
    "                'note': avis['nb_etoiles']\n",
    "            }\n",
    "            avis_entry = models.FaitAvis(**schemas.FaitAvis(**dict_avis).model_dump())\n",
    "            avis_entries.append(avis_entry)\n",
    "\n",
    "        # Exécuter les insertions groupées\n",
    "        db.add_all(date_entries)\n",
    "        db.add_all(avis_entries)\n",
    "        db.commit()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur : {e}\")\n",
    "        db.rollback()\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "# Charger tous les fichiers JSON en mémoire\n",
    "def load_all_json(data_dir='./data'):\n",
    "    data_list = []\n",
    "    for file in get_data_list(data_dir):\n",
    "        data = read_json_file(f'{data_dir}/{file}')\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "# Insérer les données des fichiers JSON\n",
    "def insert_json_data(data_dir='./data'):\n",
    "    all_data = load_all_json(data_dir)\n",
    "    for data in all_data:\n",
    "        insert_data(data)\n",
    "\n",
    "# Lancer l'importation\n",
    "if __name__ == \"__main__\":\n",
    "    insert_json_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from util import database\n",
    "from model import models, schemas\n",
    "import os\n",
    "import datetime\n",
    "import uuid\n",
    "import locale\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "\n",
    "# Configurer la locale en français\n",
    "locale.setlocale(locale.LC_TIME, 'fr_FR.UTF-8')\n",
    "\n",
    "def get_db():\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "models.Base.metadata.create_all(bind=database.engine)\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def get_data_list(data_dir='./data'):\n",
    "    json_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]\n",
    "    return json_files\n",
    "\n",
    "def parse_date(date_str):\n",
    "    # French to English month mappings\n",
    "    fr_to_en = {\n",
    "        'janvier': 'January',\n",
    "        'février': 'February',\n",
    "        'mars': 'March',\n",
    "        'avril': 'April',\n",
    "        'mai': 'May',\n",
    "        'juin': 'June',\n",
    "        'juillet': 'July',\n",
    "        'août': 'August',\n",
    "        'septembre': 'September',\n",
    "        'octobre': 'October',\n",
    "        'novembre': 'November',\n",
    "        'décembre': 'December'\n",
    "    }\n",
    "    \n",
    "    day, month, year = date_str.split(' ')\n",
    "    \n",
    "    month_en = fr_to_en[month.lower()]\n",
    "    \n",
    "    date_en = f\"{day} {month_en} {year}\"\n",
    "    \n",
    "    # Parse the English date string\n",
    "    return parser.parse(date_en, dayfirst=True)\n",
    "\n",
    "def insert_data(dict_data):\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        # Insert location\n",
    "        id_location = str(uuid.uuid4())\n",
    "        dict_location = {\n",
    "            'id_location': id_location,\n",
    "            'longitude': dict_data['longitude'],\n",
    "            'latitude': dict_data['latitude'],\n",
    "            'adresse': dict_data['adresse']\n",
    "        }\n",
    "        dim_location = schemas.DimLocation(**dict_location)\n",
    "        location = models.DimLocation(**dim_location.model_dump())\n",
    "        db.add(location)\n",
    "        db.commit()\n",
    "        db.refresh(location)\n",
    "\n",
    "        # Insert restaurant\n",
    "        id_restaurant = str(uuid.uuid4())\n",
    "        dict_restaurant = {\n",
    "            'id_restaurant': id_restaurant,\n",
    "            'nom': dict_data['nom'],\n",
    "            'id_location': id_location\n",
    "        }\n",
    "        dim_restaurant = schemas.DimRestaurant(**dict_restaurant)\n",
    "        restaurant = models.DimRestaurant(**dim_restaurant.model_dump())\n",
    "        db.add(restaurant)\n",
    "        db.commit()\n",
    "        db.refresh(restaurant)\n",
    "\n",
    "        # Insert avis\n",
    "        for avis in dict_data['avis']:\n",
    "            # Insert date\n",
    "            id_date = str(uuid.uuid4())\n",
    "            date_temp = parse_date(avis['date'])\n",
    "\n",
    "            jour_temp ,mois_temp , annee_temp = avis['date'].split(' ')\n",
    "\n",
    "            dict_time = {\n",
    "                'id_date': id_date,\n",
    "                'date': date_temp,\n",
    "                'mois': str(mois_temp),\n",
    "                'annee': str(annee_temp),\n",
    "                'jour': str(jour_temp),\n",
    "            }\n",
    "            dim_date = schemas.DimDate(**dict_time)\n",
    "            date_entry = models.DimDate(**dim_date.model_dump())\n",
    "            db.add(date_entry)\n",
    "            db.commit()\n",
    "            db.refresh(date_entry)\n",
    "\n",
    "            # Insert avis\n",
    "            id_avis = str(uuid.uuid4())\n",
    "            dict_avis = {\n",
    "                'id_avis': id_avis,\n",
    "                'id_restaurant': id_restaurant,\n",
    "                'id_date': id_date,\n",
    "                'note': avis['nb_etoiles']\n",
    "            }\n",
    "            dim_avis = schemas.FaitAvis(**dict_avis)\n",
    "            avis_entry = models.FaitAvis(**dim_avis.model_dump())\n",
    "            db.add(avis_entry)\n",
    "            db.commit()\n",
    "            db.refresh(avis_entry)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        db.rollback()\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "def insert_json_data(data_dir='./data'):\n",
    "    json_files = get_data_list(data_dir)\n",
    "    for file in json_files:\n",
    "        data = read_json_file(f'{data_dir}/{file}')\n",
    "        insert_data(data)\n",
    "\n",
    "insert_json_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a39d4beb-b19d-47ce-b377-dac26207b498\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('a39d4beb-b19d-47ce-b377-dac26207b498'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "b695c8bc-41b0-4ddd-9438-30b3e5ad25af\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('b695c8bc-41b0-4ddd-9438-30b3e5ad25af'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "1e435dbc-77a8-4cad-9ee1-1b0065eefcee\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('1e435dbc-77a8-4cad-9ee1-1b0065eefcee'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "b0934ba0-70d9-4979-b840-86203c81d762\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('b0934ba0-70d9-4979-b840-86203c81d762'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "be3752c6-fc7f-44c1-b44a-f1ff5744a95f\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('be3752c6-fc7f-44c1-b44a-f1ff5744a95f'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "d06d0ce8-d7cd-4cc4-81b4-cad38cf3b536\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('d06d0ce8-d7cd-4cc4-81b4-cad38cf3b536'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "32460775-1269-463a-a6ba-fe192a2d685d\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('32460775-1269-463a-a6ba-fe192a2d685d'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "5685925b-5489-4ecc-bd66-64377b019c8d\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('5685925b-5489-4ecc-bd66-64377b019c8d'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "e72ea6d1-9d27-49d2-98e6-09edd1d0bec5\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('e72ea6d1-9d27-49d2-98e6-09edd1d0bec5'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "a67c14a7-ba85-48c7-842a-b1360b86523b\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('a67c14a7-ba85-48c7-842a-b1360b86523b'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "c48c6ee8-cbdb-484a-a9a0-499d0f7d2202\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('c48c6ee8-cbdb-484a-a9a0-499d0f7d2202'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "70fdc969-0e8d-47a7-8c5d-729539cf9126\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('70fdc969-0e8d-47a7-8c5d-729539cf9126'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "3e99a9b6-80dd-418b-908e-a7156ea4afb3\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('3e99a9b6-80dd-418b-908e-a7156ea4afb3'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "a33bc95f-e920-400c-9855-f18b199f4d46\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('a33bc95f-e920-400c-9855-f18b199f4d46'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "445db52d-16c8-43bb-927b-7dce76f54e40\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('445db52d-16c8-43bb-927b-7dce76f54e40'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "a446c742-f5de-4837-9e87-fc697247a051\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('a446c742-f5de-4837-9e87-fc697247a051'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "c895e8d6-345f-412d-81d2-0fd8374f365c\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('c895e8d6-345f-412d-81d2-0fd8374f365c'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "6fea887d-8cc5-4ac3-ba42-074383859914\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('6fea887d-8cc5-4ac3-ba42-074383859914'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "1c8e9233-53cb-404c-90b5-f2bd4561a242\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('1c8e9233-53cb-404c-90b5-f2bd4561a242'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "94b5bb30-9c3b-4460-bde2-f0335a43c247\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('94b5bb30-9c3b-4460-bde2-f0335a43c247'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "8ac0667b-5d60-4229-a780-08fb39610a86\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('8ac0667b-5d60-4229-a780-08fb39610a86'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "db5635c2-57c5-49df-8c7b-a42e62f21c3e\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('db5635c2-57c5-49df-8c7b-a42e62f21c3e'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "cd57abf9-b35e-42a6-811e-956156374a72\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('cd57abf9-b35e-42a6-811e-956156374a72'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "03d0ec11-3ad6-4e6e-85ea-eb3f3c6974f3\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('03d0ec11-3ad6-4e6e-85ea-eb3f3c6974f3'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n",
      "77979bd2-7cc7-49d3-937d-e76b53f04730\n",
      "1 validation error for DimLocation\n",
      "id_location\n",
      "  Input should be a valid string [type=string_type, input_value=UUID('77979bd2-7cc7-49d3-937d-e76b53f04730'), input_type=UUID]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/string_type\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from util import database\n",
    "from model import models, schemas\n",
    "import os\n",
    "import datetime\n",
    "import uuid\n",
    "\n",
    "\n",
    "def get_db():\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "models.Base.metadata.create_all(bind= database.engine)\n",
    "\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def  get_data_list(data_dir = './data'):\n",
    "    # data_dir = './data'\n",
    "    json_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]\n",
    "    return json_files\n",
    "\n",
    "def insert_data(dict_data):\n",
    "    db = database.SessionLocal()\n",
    "\n",
    "    try:\n",
    "        # Insert location\n",
    "        id_location = uuid.uuid4()\n",
    "        print(id_location)\n",
    "        dict_location = {\n",
    "            'id_location': id_location,  # Provide a default value if id_location is missing\n",
    "            'longitude': dict_data['longitude'],\n",
    "            'latitude': dict_data['latitude'],\n",
    "            'adresse': dict_data['adresse']\n",
    "        }\n",
    "        dim_location = schemas.DimLocation(**dict_location)\n",
    "        location = models.DimLocation(**dim_location.dict())\n",
    "        db.add(location)\n",
    "        db.commit()\n",
    "        db.refresh(location)\n",
    "\n",
    "        # Insert restaurant\n",
    "        id_restaurant = uuid.uuid4()\n",
    "        dict_restaurant = {\n",
    "            'id_restaurant': id_restaurant,\n",
    "            'nom': dict_data['nom'],\n",
    "            'id_location': id_location\n",
    "        }\n",
    "        dim_restaurant = schemas.DimRestaurant(**dict_restaurant)\n",
    "        restaurant = models.DimRestaurant(**dim_restaurant.dict())\n",
    "        db.add(restaurant)\n",
    "        db.commit()\n",
    "        db.refresh(restaurant)\n",
    "\n",
    "        # Insert avis\n",
    "        for avis  in  dict_data['avis']:\n",
    "            \n",
    "            #insert date\n",
    "            id_date = uuid.uuid4()\n",
    "            date_temp = datetime.datetime.strptime(avis['date'])\n",
    "            jour_semaine = date_temp.weekday()\n",
    "            mois_temp = date_temp.strftime('%m')\n",
    "            annee_temp = date_temp.strftime('%Y')\n",
    "            dict_time = {\n",
    "                'id_date': id_date,\n",
    "                'date': date_temp,\n",
    "                'mois': mois_temp,\n",
    "                'annee': annee_temp,\n",
    "                'jour_semaine': jour_semaine,\n",
    "            }\n",
    "            print(jour_semaine)\n",
    "            #insert avis\n",
    "            id_avis = uuid.uuid4()\n",
    "            nombre_etoile = avis['nombre_etoile']\n",
    "            dict_avis = {\n",
    "                'id_avis': id_avis,\n",
    "                'avis': avis\n",
    "            }\n",
    "            dim_avis = schemas.DimAvis(**dict_avis)\n",
    "            avis = models.DimAvis(**dim_avis.dict())\n",
    "            db.add(avis)\n",
    "            db.commit()\n",
    "            db.refresh(avis)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        dict_time = {\n",
    "            'id_time': id_time,\n",
    "            'date': dict_data['date'],\n",
    "            'jour': dict_data['jour'],\n",
    "            'heure': dict_data['heure']\n",
    "        }\n",
    "        dim_time = schemas.DimTime(**dict_time)\n",
    "        time = models.DimTime(**dim_time.dict())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # db.rollback()\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "\n",
    "def insert_json_data(data_dir = './data'):\n",
    "    json_files = get_data_list(data_dir)\n",
    "    for file in json_files:\n",
    "        data = read_json_file(f'{data_dir}/{file}')\n",
    "        insert_data(data)\n",
    "        \n",
    "insert_json_data( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "877473145"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = random.randint(1, 1000000000)\n",
    "id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<model.models.DimLocation object at 0x0000022483382160>, <model.models.DimLocation object at 0x0000022483382280>, <model.models.DimLocation object at 0x00000224830D42E0>, <model.models.DimLocation object at 0x00000224830D45B0>, <model.models.DimLocation object at 0x00000224830D4130>, <model.models.DimLocation object at 0x00000224830D4370>, <model.models.DimLocation object at 0x00000224830D4610>, <model.models.DimLocation object at 0x00000224830D40A0>, <model.models.DimLocation object at 0x00000224830D4520>, <model.models.DimLocation object at 0x00000224830D4340>, <model.models.DimLocation object at 0x00000224830D4B20>, <model.models.DimLocation object at 0x0000022481F45970>, <model.models.DimLocation object at 0x0000022481F45670>, <model.models.DimLocation object at 0x0000022481F45490>, <model.models.DimLocation object at 0x0000022481F45580>, <model.models.DimLocation object at 0x0000022481F45B80>, <model.models.DimLocation object at 0x0000022481F45880>, <model.models.DimLocation object at 0x0000022481F45AC0>, <model.models.DimLocation object at 0x0000022481F45130>, <model.models.DimLocation object at 0x0000022481F458E0>, <model.models.DimLocation object at 0x0000022481F45AF0>, <model.models.DimLocation object at 0x0000022481F45250>, <model.models.DimLocation object at 0x0000022481F45A00>, <model.models.DimLocation object at 0x0000022481F45700>, <model.models.DimLocation object at 0x0000022481F459A0>, <model.models.DimLocation object at 0x0000022481F45850>]\n",
      "134 Rue Duguesclin, 69006 Lyon France\n",
      "134 Rue Duguesclin, 69006 Lyon France\n",
      "15 Rue du Chariot d'Or, 69004 Lyon France\n",
      "37 Rue de la Charite, 69002 Lyon France\n",
      "19 Rue de la Bombarde Angle Rue Tramassac, 69005 Lyon France\n",
      "Halles de Lyon 102 Cours Lafayette, 69003 Lyon France\n",
      "14 Place Carnot, 69002 Lyon France\n",
      "3 Rue Duviard, 69004 Lyon France\n",
      "23 Rue De Condé, 69002 Lyon France\n",
      "5 rue Charles Dullin, 69002 Lyon France\n",
      "61 Rue de la Part-Dieu, 69003 Lyon France\n",
      "112 Cr Charlemagne, 69002 Lyon France\n",
      "46 Avenue Jean Jaurès, 69007 Lyon France\n",
      "20 Quai Dr Gailleton Hotel Sofitel Lyon Bellecour, 69002 Lyon France\n",
      "26 rue de Conde, 69002 Lyon France\n",
      "3 rue des Pierres Plantees, 69001 Lyon France\n",
      "3 Cour Saint Henri Grand Hôtel Dieu, 69002 Lyon France\n",
      "40 Rue Saint Jean 69005, 69005 Lyon France\n",
      "44 Rue Saint-Jean, 69005 Lyon France\n",
      "24 Rue de la Republique, 69002 Lyon France\n",
      "20 Place Bellecour, 69002 Lyon France\n",
      "13 Rue Domer, 69007 Lyon France\n",
      "8 Place des Célestins Rue Pazzi, 69002 Lyon France\n",
      "2 Rue des Forces, 69002 Lyon France\n",
      "133 Rue Bugeaud, 69006 Lyon France\n",
      "8 Rue Pailleron, 69004 Lyon France\n"
     ]
    }
   ],
   "source": [
    "#afficher  les  données de  la  table  DimLocation\n",
    "def get_all_locations():\n",
    "    db = database.SessionLocal()\n",
    "    locations = db.query(models.DimLocation).all()\n",
    "    db.close()\n",
    "    return locations\n",
    "\n",
    "data  = get_all_locations()\n",
    "print(data)\n",
    "for d in data:\n",
    "    print(d.adresse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'create_engine' from 'sqlalchemy' (c:\\Users\\ediad\\Documents\\NLP\\TripAdvisor-NLP-Analysis\\server\\app\\sqlalchemy\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m database\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# from .database import SessionLocal\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# from . import models, schemas, crud\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models, schemas, crud\n",
      "File \u001b[1;32mc:\\Users\\ediad\\Documents\\NLP\\TripAdvisor-NLP-Analysis\\server\\app\\utils\\database.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_engine\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeclarative\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m declarative_base\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01morm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sessionmaker\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'create_engine' from 'sqlalchemy' (c:\\Users\\ediad\\Documents\\NLP\\TripAdvisor-NLP-Analysis\\server\\app\\sqlalchemy\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from util import database\n",
    "# from .database import SessionLocal\n",
    "# from . import models, schemas, crud\n",
    "from models import models, schemas, crud\n",
    "# from sqlalchemy.orm import Session\n",
    "# Create all tables if they don't exist\n",
    "# Base.metadata.create_all(bind=engine)\n",
    "\n",
    "def get_db():\n",
    "    db = database.SessionLocal()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        db.close()\n",
    "\n",
    "models.Base.metadata.create_all(bind=database.engine)\n",
    "# def read_json_file(file_path):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         data = json.load(file)\n",
    "#     return data\n",
    "\n",
    "# def insert_data_from_json(db: SessionLocal, data: dict):\n",
    "#     # Insert locations\n",
    "#     for location in data.get('locations', []):\n",
    "#         location_schema = schemas.DimLocationCreate(**location)\n",
    "#         crud.create_location(db, location_schema)\n",
    "\n",
    "#     # Insert restaurants\n",
    "#     for restaurant in data.get('restaurants', []):\n",
    "#         restaurant_schema = schemas.DimRestaurantCreate(**restaurant)\n",
    "#         crud.create_restaurant(db, restaurant_schema)\n",
    "\n",
    "#     # Insert dates\n",
    "#     for date in data.get('dates', []):\n",
    "#         date_schema = schemas.DimDateCreate(**date)\n",
    "#         crud.create_date(db, date_schema)\n",
    "\n",
    "#     # Insert reviews\n",
    "#     for review in data.get('reviews', []):\n",
    "#         review_schema = schemas.FaitAvisCreate(**review)\n",
    "#         crud.create_review(db, review_schema)\n",
    "\n",
    "# def main():\n",
    "#     db = SessionLocal()\n",
    "#     try:\n",
    "#         data = read_json_file('data.json')\n",
    "#         insert_data_from_json(db, data)\n",
    "#     finally:\n",
    "#         db.close()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agastache_Restaurant.json\n",
      "dict_keys(['nom', 'adresse', 'classement', 'horaires', 'note_globale', 'note_cuisine', 'note_service', 'note_rapportqualiteprix', 'note_ambiance', 'infos_pratiques', 'repas', 'regimes', 'fourchette_prix', 'fonctionnalités', 'type_cuisines', 'latitude', 'longitude', 'nb_avis', 'nbExcellent', 'nbTrèsbon', 'nbMoyen', 'nbMédiocre', 'nbHorrible', 'avis'])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from schemas import DimDateBase\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "data_dir = './data'\n",
    "json_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]\n",
    "\n",
    "\n",
    "\n",
    "for  file  in  json_files[:1]:\n",
    "    # dim_date = DimDateBase()\n",
    "\n",
    "    print(file)\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    data = read_json_file(file_path)\n",
    "    print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agastache_Restaurant.json\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 26\u001b[0m\n\u001b[0;32m     21\u001b[0m data \u001b[38;5;241m=\u001b[39m read_json_file(file_path)\n\u001b[0;32m     22\u001b[0m date \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m: data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnom\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m }\n\u001b[1;32m---> 26\u001b[0m dim_date \u001b[38;5;241m=\u001b[39m \u001b[43mDimDateBase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(date)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# print(data ['avis'])\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# print(len(data))\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from schemas import DimDateBase\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.load(file)\n",
    "    \n",
    "data_dir = '../data'\n",
    "json_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]\n",
    "\n",
    "\n",
    "\n",
    "for  file  in  json_files[:1]:\n",
    "    # dim_date = DimDateBase()\n",
    "\n",
    "    print(file)\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    data = read_json_file(file_path)\n",
    "    date = {\n",
    "        'date': data['nom']\n",
    "\n",
    "    }\n",
    "    dim_date = DimDateBase()\n",
    "\n",
    "    print(date)\n",
    "    # print(data ['avis'])\n",
    "    # print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date=datetime.date(2023, 10, 1) mois=10 annee=2023\n"
     ]
    }
   ],
   "source": [
    "from schemas import DimDateCreate, DimAuteurCreate, FaitAvisCreate, DimDate, DimAuteur, FaitAvis\n",
    "\n",
    "# Example function to create a new date entry\n",
    "def create_date_entry(date_str: str, mois_str: str, annee_str: str):\n",
    "    date_entry = DimDateCreate(date=date_str, mois=mois_str, annee=annee_str)\n",
    "    # Perform further operations, such as saving to the database\n",
    "    return date_entry\n",
    "\n",
    "# # Example function to create a new author entry\n",
    "# def create_author_entry(auteur: str, email: Optional[str] = None):\n",
    "#     author_entry = DimAuteurCreate(auteur=auteur, email=email)\n",
    "#     # Perform further operations, such as saving to the database\n",
    "#     return author_entry\n",
    "\n",
    "# # Example function to create a new review entry\n",
    "# def create_review_entry(id_restaurant: int, id_date: int, id_auteur: int, note: int, commentaire: Optional[str] = None, nb_commentaire: Optional[int] = None):\n",
    "#     review_entry = FaitAvisCreate(\n",
    "#         id_restaurant=id_restaurant,\n",
    "#         id_date=id_date,\n",
    "#         id_auteur=id_auteur,\n",
    "#         note=note,\n",
    "#         commentaire=commentaire,\n",
    "#         nb_commentaire=nb_commentaire\n",
    "#     )\n",
    "#     # Perform further operations, such as saving to the database\n",
    "#     return review_entry\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    date_entry = create_date_entry(\"2023-10-01\", \"10\", \"2023\")\n",
    "    print(date_entry)\n",
    "\n",
    "    # author_entry = create_author_entry(\"John Doe\", \"john.doe@example.com\")\n",
    "    # print(author_entry)\n",
    "\n",
    "    # review_entry = create_review_entry(1, 1, 1, 5, \"Great restaurant!\", 10)\n",
    "    # print(review_entry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
