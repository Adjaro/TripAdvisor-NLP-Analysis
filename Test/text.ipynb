{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Scraping page 1\n",
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.tripadvisor.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "WARNING:__main__:No restaurants found on page 1\n",
      "INFO:__main__:Scraping page 2\n",
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.tripadvisor.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.tripadvisor.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "WARNING:__main__:No restaurants found on page 2\n",
      "INFO:__main__:Scraping page 3\n",
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.tripadvisor.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\ediad\\.conda\\envs\\nlpProject\\lib\\site-packages\\urllib3\\connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.tripadvisor.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "WARNING:__main__:No restaurants found on page 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No restaurants were found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint, choice\n",
    "import logging\n",
    "from fake_useragent import UserAgent\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "def create_session():\n",
    "    session = requests.Session()\n",
    "    retry = Retry(total=3, backoff_factor=1, status_forcelist=[500, 502, 503, 504])\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def get_headers():\n",
    "    ua = UserAgent()\n",
    "    return {\n",
    "        'User-Agent': ua.random,\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'TE': 'Trailers'\n",
    "    }\n",
    "\n",
    "def get_page_content(url, logger, session):\n",
    "    try:\n",
    "        # Add longer timeout and verify=False for testing\n",
    "        response = session.get(\n",
    "            url, \n",
    "            headers=get_headers(),\n",
    "            timeout=30,\n",
    "            verify=False  # Note: In production, keep verify=True\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching URL {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_restaurants(city_url, num_pages=5):\n",
    "    logger = setup_logging()\n",
    "    restaurants_data = []\n",
    "    session = create_session()\n",
    "    \n",
    "    for page in range(num_pages):\n",
    "        try:\n",
    "            if page == 0:\n",
    "                page_url = city_url\n",
    "            else:\n",
    "                page_url = city_url.replace('.html', f'-oa{page*30}.html')\n",
    "                \n",
    "            logger.info(f\"Scraping page {page + 1}\")\n",
    "            \n",
    "            # Add longer delay between requests\n",
    "            time.sleep(randint(5, 10))\n",
    "            \n",
    "            content = get_page_content(page_url, logger, session)\n",
    "            if not content:\n",
    "                continue\n",
    "                \n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            restaurant_divs = soup.find_all('div', class_='listing')\n",
    "            \n",
    "            if not restaurant_divs:\n",
    "                restaurant_divs = soup.find_all('div', class_='location-meta-block')  # Alternative class\n",
    "            \n",
    "            if not restaurant_divs:\n",
    "                logger.warning(f\"No restaurants found on page {page + 1}\")\n",
    "                continue\n",
    "            \n",
    "            for div in restaurant_divs:\n",
    "                restaurant = parse_restaurant(div, logger)\n",
    "                if restaurant:\n",
    "                    restaurants_data.append(restaurant)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing page {page + 1}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(restaurants_data)\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.tripadvisor.com/Restaurants-g187265-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"\n",
    "    df = scrape_restaurants(base_url, num_pages=3)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No restaurants were found\")\n",
    "        return\n",
    "        \n",
    "    df.to_csv('lyon_restaurants.csv', index=False)\n",
    "    print(f\"Successfully scraped {len(df)} restaurants\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:WDM:====== WebDriver manager ======\n",
      "INFO:WDM:Get LATEST chromedriver version for google-chrome\n",
      "INFO:WDM:Get LATEST chromedriver version for google-chrome\n",
      "INFO:WDM:Driver [C:\\Users\\ediad\\.wdm\\drivers\\chromedriver\\win64\\131.0.6778.108\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "INFO:__main__:Scraping page 1\n",
      "ERROR:__main__:Error during scraping: Message: \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No restaurants were found\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "\n",
    "def setup_logging():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "def setup_driver():\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    return driver\n",
    "\n",
    "def parse_restaurant(restaurant_element, logger):\n",
    "    try:\n",
    "        return {\n",
    "            'name': restaurant_element.find_element(By.CSS_SELECTOR, '[data-test=\"title\"]').text.strip(),\n",
    "            # 'rating': restaurant_element.find_element(By.CSS_SELECTOR, '[data-test=\"rating-review\"]').get_attribute('aria-label').split()[0] if restaurant_element.find_elements(By.CSS_SELECTOR, '[data-test=\"rating-review\"]') else 'N/A',\n",
    "            # 'reviews': restaurant_element.find_element(By.CSS_SELECTOR, '[data-test=\"review-count\"]').text.strip('()') if restaurant_element.find_elements(By.CSS_SELECTOR, '[data-test=\"review-count\"]') else '0',\n",
    "            # 'cuisine': restaurant_element.find_element(By.CSS_SELECTOR, '[data-test=\"cuisine\"]').text if restaurant_element.find_elements(By.CSS_SELECTOR, '[data-test=\"cuisine\"]') else 'N/A',\n",
    "            # 'price_range': restaurant_element.find_element(By.CSS_SELECTOR, '[data-test=\"price-range\"]').text if restaurant_element.find_elements(By.CSS_SELECTOR, '[data-test=\"price-range\"]') else 'N/A',\n",
    "            # 'address': restaurant_element.find_element(By.CSS_SELECTOR, '[data-test=\"address\"]').text if restaurant_element.find_elements(By.CSS_SELECTOR, '[data-test=\"address\"]') else 'N/A'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing restaurant: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_restaurants(city_url, num_pages=5):\n",
    "    logger = setup_logging()\n",
    "    restaurants_data = []\n",
    "    driver = setup_driver()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    \n",
    "    try:\n",
    "        for page in range(num_pages):\n",
    "            page_url = f\"{city_url.replace('.html', '')}-oa{page*30}.html\" if page > 0 else city_url\n",
    "            logger.info(f\"Scraping page {page + 1}\")\n",
    "            \n",
    "            driver.get(page_url)\n",
    "            time.sleep(5)  # Allow page to load\n",
    "            \n",
    "            # Wait for restaurants to load\n",
    "            restaurant_elements = wait.until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"[data-test='restaurant-item']\"))\n",
    "            )\n",
    "            \n",
    "            if not restaurant_elements:\n",
    "                logger.warning(f\"No restaurants found on page {page + 1}\")\n",
    "                continue\n",
    "                \n",
    "            for element in restaurant_elements:\n",
    "                restaurant = parse_restaurant(element, logger)\n",
    "                if restaurant:\n",
    "                    restaurants_data.append(restaurant)\n",
    "            \n",
    "            time.sleep(randint(3, 5))\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during scraping: {str(e)}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return pd.DataFrame(restaurants_data)\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.tripadvisor.com/Restaurants-g187265-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"\n",
    "    df = scrape_restaurants(base_url, num_pages=3)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"No restaurants were found\")\n",
    "        return\n",
    "        \n",
    "    df.to_csv('lyon_restaurants.csv', index=False)\n",
    "    print(f\"Successfully scraped {len(df)} restaurants\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Scraping page: https://www.tripadvisor.fr/Restaurants-g187265-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n",
      "INFO:__main__:Scraping page: https://www.tripadvisor.fr/Restaurants-g187265-Lyon_Rhone_Auvergne_Rhone_Alpes-oa30.html\n",
      "INFO:__main__:Scraping page: https://www.tripadvisor.fr/Restaurants-g187265-Lyon_Rhone_Auvergne_Rhone_Alpes-oa60.html\n",
      "ERROR:__main__:No restaurants found\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TripAdvisorScraper:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.ua = UserAgent()\n",
    "        self.session = requests.Session()\n",
    "\n",
    "    def get_headers(self):\n",
    "        return {\n",
    "            \"User-Agent\": self.ua.random,\n",
    "            \"Accept-Language\": \"fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"DNT\": \"1\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\"\n",
    "        }\n",
    "\n",
    "    def get_page_content(self, url, retries=3):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self.session.get(\n",
    "                    url, \n",
    "                    headers=self.get_headers(), \n",
    "                    timeout=15\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                return BeautifulSoup(response.text, 'html.parser')\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                if attempt == retries - 1:\n",
    "                    return None\n",
    "                time.sleep(randint(2, 5))\n",
    "\n",
    "    def parse_restaurant(self, restaurant):\n",
    "        print(restaurant)\n",
    "        try:\n",
    "            data = {\n",
    "                \"nom\": restaurant.find('a', class_='Lwqic Cj b').text.strip() if restaurant.find('a', class_='Lwqic Cj b') else \"N/A\",\n",
    "                # \"adresse\": restaurant.find('span', class_='fHibO').text.strip() if restaurant.find('span', class_='fHibO') else \"N/A\",\n",
    "                # \"note\": restaurant.find('svg', class_='UctUV')['aria-label'].split()[0] if restaurant.find('svg', class_='UctUV') else \"N/A\",\n",
    "                # \"nb_avis\": restaurant.find('span', class_='IiChw').text.strip('()') if restaurant.find('span', class_='IiChw') else \"0\",\n",
    "                # \"cuisine\": ', '.join([c.text.strip() for c in restaurant.find_all('span', class_='DsyBj DxyfE')]) if restaurant.find_all('span', class_='DsyBj DxyfE') else \"N/A\",\n",
    "                # \"prix\": restaurant.find('span', class_='DsyBj DxyfE Gi z').text.strip() if restaurant.find('span', class_='DsyBj DxyfE Gi z') else \"N/A\"\n",
    "            }\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing restaurant: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_page(self, page_number=0):\n",
    "        url = self.base_url if page_number == 0 else self.base_url.replace('.html', f'-oa{page_number*30}.html')\n",
    "        logger.info(f\"Scraping page: {url}\")\n",
    "        \n",
    "        soup = self.get_page_content(url)\n",
    "        if not soup:\n",
    "            return []\n",
    "\n",
    "        restaurants = []\n",
    "        restaurant_divs = soup.find_all('div', class_='nJbYN Wh S2 H2 f')\n",
    "        \n",
    "        for restaurant_div in restaurant_divs:\n",
    "            data = self.parse_restaurant(restaurant_div)\n",
    "            if data:\n",
    "                restaurants.append(data)\n",
    "            time.sleep(randint(1, 3))  # Respect rate limiting\n",
    "\n",
    "        return restaurants\n",
    "\n",
    "    def scrape_restaurants(self, num_pages=3):\n",
    "        all_restaurants = []\n",
    "        \n",
    "        for page in range(num_pages):\n",
    "            restaurants = self.scrape_page(page)\n",
    "            if restaurants:\n",
    "                all_restaurants.extend(restaurants)\n",
    "            time.sleep(randint(3, 5))  # Delay between pages\n",
    "            \n",
    "        return pd.DataFrame(all_restaurants)\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.tripadvisor.fr/Restaurants-g187265-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"\n",
    "    scraper = TripAdvisorScraper(base_url)\n",
    "    \n",
    "    try:\n",
    "        df = scraper.scrape_restaurants(num_pages=3)\n",
    "        if not df.empty:\n",
    "            df.to_csv('restaurants_lyon.csv', index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"Successfully scraped {len(df)} restaurants\")\n",
    "            logger.info(f\"Data saved to restaurants_lyon.csv\")\n",
    "        else:\n",
    "            logger.error(\"No restaurants found\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Scraping failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Scraping de la page : https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.tripadvisor.fr:443\n",
      "DEBUG:urllib3.connectionpool:https://www.tripadvisor.fr:443 \"GET /RestaurantSearch?geo=187265&sortOrder=popularity&o=a0 HTTP/11\" 200 None\n",
      "DEBUG:__main__:Page récupérée avec succès : https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a0\n",
      "ERROR:__main__:Aucun restaurant trouvé.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Configuration du logger\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TripAdvisorScraper:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.ua = UserAgent()\n",
    "        self.session = requests.Session()\n",
    "\n",
    "    def get_headers(self):\n",
    "        \"\"\"Génère des en-têtes HTTP réalistes pour simuler un vrai navigateur.\"\"\"\n",
    "        return {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\",\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,/;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-Site\": \"none\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "            \"Cache-Control\": \"max-age=0\",\n",
    "        }\n",
    "\n",
    "    def get_page_content(self, url, retries=3):\n",
    "        \"\"\"Récupère le contenu d'une page avec gestion des erreurs et des délais.\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self.session.get(url, headers=self.get_headers(), timeout=30)\n",
    "                response.raise_for_status()\n",
    "                logger.debug(f\"Page récupérée avec succès : {url}\")\n",
    "                time.sleep(randint(2, 4))  # Respect des limites\n",
    "                return BeautifulSoup(response.text, 'html.parser')\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Tentative {attempt + 1} échouée : {str(e)}\")\n",
    "                if attempt == retries - 1:\n",
    "                    return None\n",
    "                time.sleep(randint(5, 10))\n",
    "\n",
    "    def parse_restaurant(self, restaurant):\n",
    "        \"\"\"Analyse les informations d'un restaurant dans une carte.\"\"\"\n",
    "        try:\n",
    "            name_elem = restaurant.find('a', {\"data-test\": \"restaurant-name\"})\n",
    "            address_elem = restaurant.find('div', class_='vQlTa')\n",
    "            rating_elem = restaurant.find('svg', {'aria-label': True})\n",
    "            reviews_elem = restaurant.find('span', {'data-test': 'reviews-count'})\n",
    "            price_elem = restaurant.find('span', {'data-test': 'price-range'})\n",
    "\n",
    "            data = {\n",
    "                \"nom\": name_elem.text.strip() if name_elem else \"N/A\",\n",
    "                \"adresse\": address_elem.text.strip() if address_elem else \"N/A\",\n",
    "                \"note\": rating_elem['aria-label'].split()[0] if rating_elem else \"N/A\",\n",
    "                \"nb_avis\": reviews_elem.text.strip().split()[0] if reviews_elem else \"0\",\n",
    "                \"prix\": price_elem.text.strip() if price_elem else \"N/A\",\n",
    "                \"url\": urljoin(self.base_url, name_elem['href']) if name_elem and name_elem.has_attr('href') else \"N/A\"\n",
    "            }\n",
    "            logger.debug(f\"Restaurant analysé : {data}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de l'analyse d'un restaurant : {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_next_page_url(self, page_number):\n",
    "        \"\"\"Génère l'URL pour paginer les résultats.\"\"\"\n",
    "        offset = page_number * 30  # TripAdvisor utilise un offset par tranches de 30\n",
    "        return f\"{self.base_url}&o=a{offset}\"\n",
    "\n",
    "    def scrape_page(self, page_number=0):\n",
    "        \"\"\"Récupère les données des restaurants d'une page donnée.\"\"\"\n",
    "        url = self.get_next_page_url(page_number)\n",
    "        logger.info(f\"Scraping de la page : {url}\")\n",
    "        soup = self.get_page_content(url)\n",
    "        # print(soup)\n",
    "        if not soup:\n",
    "            logger.warning(f\"Pas de contenu récupéré pour la page {page_number}\")\n",
    "            return []\n",
    "\n",
    "        restaurants = []\n",
    "        restaurant_divs = soup.find_all('div', {\"data-test\": \"restaurant-item\"})\n",
    "\n",
    "        for restaurant_div in restaurant_divs:\n",
    "            data = self.parse_restaurant(restaurant_div)\n",
    "            if data:\n",
    "                restaurants.append(data)\n",
    "            time.sleep(randint(1, 2))  # Délai entre les analyses\n",
    "\n",
    "        return restaurants\n",
    "\n",
    "    def scrape_restaurants(self, num_pages=3):\n",
    "        \"\"\"Récupère les informations de plusieurs pages de restaurants.\"\"\"\n",
    "        all_restaurants = []\n",
    "        for page in range(num_pages):\n",
    "            restaurants = self.scrape_page(page)\n",
    "            if restaurants:\n",
    "                all_restaurants.extend(restaurants)\n",
    "                logger.info(f\"Page {page + 1} : {len(restaurants)} restaurants récupérés.\")\n",
    "            time.sleep(randint(3, 5))  # Délai entre les pages\n",
    "        return pd.DataFrame(all_restaurants)\n",
    "\n",
    "def main():\n",
    "    # URL de recherche des restaurants à Lyon\n",
    "    base_url = \"https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity\"\n",
    "    scraper = TripAdvisorScraper(base_url)\n",
    "    \n",
    "    try:\n",
    "        df = scraper.scrape_restaurants(num_pages=1)\n",
    "        if not df.empty:\n",
    "            df.to_csv('restaurants_lyon.csv', index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"Scraping réussi : {len(df)} restaurants enregistrés dans 'restaurants_lyon.csv'\")\n",
    "        else:\n",
    "            logger.error(\"Aucun restaurant trouvé.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Échec du scraping : {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Scraping de la page : https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.tripadvisor.fr:443\n",
      "DEBUG:urllib3.connectionpool:https://www.tripadvisor.fr:443 \"GET /RestaurantSearch?geo=187265&sortOrder=popularity&o=a0 HTTP/11\" 200 None\n",
      "DEBUG:__main__:Page récupérée avec succès : https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a0\n",
      "ERROR:__main__:Aucun restaurant trouvé.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Configuration du logger\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TripAdvisorScraper:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.ua = UserAgent()\n",
    "        self.session = requests.Session()\n",
    "\n",
    "    def get_headers(self):\n",
    "        \"\"\"Génère des en-têtes HTTP réalistes pour simuler un vrai navigateur.\"\"\"\n",
    "        return {\n",
    "            # \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\",\n",
    "            \"User-Agent\": self.ua.random,\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,/;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-Site\": \"none\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "            \"Cache-Control\": \"max-age=0\",\n",
    "        }\n",
    "\n",
    "    def get_page_content(self, url, retries=3):\n",
    "        \"\"\"Récupère le contenu d'une page avec gestion des erreurs et des délais.\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self.session.get(url, headers=self.get_headers(), timeout=30)\n",
    "                response.raise_for_status()\n",
    "                logger.debug(f\"Page récupérée avec succès : {url}\")\n",
    "                time.sleep(randint(2, 4))  # Respect des limites\n",
    "                return BeautifulSoup(response.text, 'html.parser')\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Tentative {attempt + 1} échouée : {str(e)}\")\n",
    "                if attempt == retries - 1:\n",
    "                    return None\n",
    "                time.sleep(randint(5, 10))\n",
    "\n",
    "    def parse_restaurant(self, restaurant):\n",
    "        \"\"\"Analyse les informations d'un restaurant dans une carte.\"\"\"\n",
    "        try:\n",
    "            name_elem = restaurant.find('a', {\"data-test\": \"restaurant-name\"})\n",
    "            address_elem = restaurant.find('div', class_='vQlTa')\n",
    "            rating_elem = restaurant.find('svg', {'aria-label': True})\n",
    "            reviews_elem = restaurant.find('span', {'data-test': 'reviews-count'})\n",
    "            price_elem = restaurant.find('span', {'data-test': 'price-range'})\n",
    "\n",
    "            data = {\n",
    "                \"nom\": name_elem.text.strip() if name_elem else \"N/A\",\n",
    "                \"adresse\": address_elem.text.strip() if address_elem else \"N/A\",\n",
    "                \"note\": rating_elem['aria-label'].split()[0] if rating_elem else \"N/A\",\n",
    "                \"nb_avis\": reviews_elem.text.strip().split()[0] if reviews_elem else \"0\",\n",
    "                \"prix\": price_elem.text.strip() if price_elem else \"N/A\",\n",
    "                \"url\": urljoin(self.base_url, name_elem['href']) if name_elem and name_elem.has_attr('href') else \"N/A\"\n",
    "            }\n",
    "            logger.debug(f\"Restaurant analysé : {data}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de l'analyse d'un restaurant : {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_next_page_url(self, page_number):\n",
    "        \"\"\"Génère l'URL pour paginer les résultats.\"\"\"\n",
    "        offset = page_number * 30  # TripAdvisor utilise un offset par tranches de 30\n",
    "        return f\"{self.base_url}&o=a{offset}\"\n",
    "\n",
    "    def scrape_page(self, page_number=0):\n",
    "        \"\"\"Récupère les données des restaurants d'une page donnée.\"\"\"\n",
    "        url = self.get_next_page_url(page_number)\n",
    "        logger.info(f\"Scraping de la page : {url}\")\n",
    "        soup = self.get_page_content(url)\n",
    "        # print(soup)\n",
    "        with open('page_content.html', 'w', encoding='utf-8') as file:\n",
    "            file.write(str(soup))\n",
    "        \n",
    "\n",
    "        if not soup:\n",
    "            logger.warning(f\"Pas de contenu récupéré pour la page {page_number}\")\n",
    "            return []\n",
    "\n",
    "        restaurants = []\n",
    "        restaurant_divs = soup.find_all('div', {\"data-test\": \"restaurant-item\"})\n",
    "\n",
    "        for restaurant_div in restaurant_divs:\n",
    "            data = self.parse_restaurant(restaurant_div)\n",
    "            if data:\n",
    "                restaurants.append(data)\n",
    "            time.sleep(randint(1, 2))  # Délai entre les analyses\n",
    "\n",
    "        return restaurants\n",
    "\n",
    "    def scrape_restaurants(self, num_pages=3):\n",
    "        \"\"\"Récupère les informations de plusieurs pages de restaurants.\"\"\"\n",
    "        all_restaurants = []\n",
    "        for page in range(num_pages):\n",
    "            restaurants = self.scrape_page(page)\n",
    "            if restaurants:\n",
    "                all_restaurants.extend(restaurants)\n",
    "                logger.info(f\"Page {page + 1} : {len(restaurants)} restaurants récupérés.\")\n",
    "            time.sleep(randint(3, 5))  # Délai entre les pages\n",
    "        return pd.DataFrame(all_restaurants)\n",
    "\n",
    "def main():\n",
    "    # URL de recherche des restaurants à Lyon\n",
    "    base_url = \"https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity\"\n",
    "    scraper = TripAdvisorScraper(base_url)\n",
    "    \n",
    "    try:\n",
    "        df = scraper.scrape_restaurants(num_pages=1)\n",
    "        if not df.empty:\n",
    "            df.to_csv('restaurants_lyon.csv', index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"Scraping réussi : {len(df)} restaurants enregistrés dans 'restaurants_lyon.csv'\")\n",
    "        else:\n",
    "            logger.error(\"Aucun restaurant trouvé.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Échec du scraping : {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 20:31:28,862 - INFO - Scraping de la page : https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a0\n",
      "2024-12-18 20:31:28,866 - ERROR - Tentative 1 échouée pour https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a0 : Failed to parse: http://proxy1:port\n",
      "2024-12-18 20:31:38,868 - ERROR - Tentative 2 échouée pour https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a0 : Failed to parse: http://proxy2:port\n",
      "2024-12-18 20:31:46,889 - ERROR - Tentative 3 échouée pour https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a0 : Failed to parse: http://proxy3:port\n",
      "2024-12-18 20:31:46,889 - WARNING - Pas de contenu récupéré pour la page 0\n",
      "2024-12-18 20:31:57,895 - INFO - Scraping de la page : https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a30\n",
      "2024-12-18 20:31:57,904 - ERROR - Tentative 1 échouée pour https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a30 : Failed to parse: http://proxy1:port\n",
      "2024-12-18 20:32:06,920 - ERROR - Tentative 2 échouée pour https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a30 : Failed to parse: http://proxy2:port\n",
      "2024-12-18 20:32:14,926 - ERROR - Tentative 3 échouée pour https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a30 : Failed to parse: http://proxy3:port\n",
      "2024-12-18 20:32:14,926 - WARNING - Pas de contenu récupéré pour la page 1\n",
      "2024-12-18 20:32:22,933 - INFO - Scraping de la page : https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a60\n",
      "2024-12-18 20:32:22,943 - ERROR - Tentative 1 échouée pour https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a60 : Failed to parse: http://proxy1:port\n",
      "2024-12-18 20:32:28,965 - ERROR - Tentative 2 échouée pour https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a60 : Failed to parse: http://proxy2:port\n",
      "2024-12-18 20:32:36,977 - ERROR - Tentative 3 échouée pour https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a60 : Failed to parse: http://proxy3:port\n",
      "2024-12-18 20:32:36,977 - WARNING - Pas de contenu récupéré pour la page 2\n",
      "2024-12-18 20:32:48,983 - ERROR - Aucun restaurant trouvé.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "from itertools import cycle\n",
    "\n",
    "# Configuration du logger\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TripAdvisorScraper:\n",
    "    def __init__(self, base_url, min_delay=5, max_delay=10, retries=3, proxies=None):\n",
    "        self.base_url = base_url\n",
    "        self.ua = UserAgent()\n",
    "        self.session = requests.Session()\n",
    "        self.min_delay = min_delay\n",
    "        self.max_delay = max_delay\n",
    "        self.retries = retries\n",
    "        self.proxies = cycle(proxies) if proxies else None  # Rotation des proxies\n",
    "\n",
    "    def get_headers(self):\n",
    "        \"\"\"Génère des en-têtes HTTP réalistes pour simuler un vrai navigateur.\"\"\"\n",
    "        return {\n",
    "            \"User-Agent\": self.ua.random,\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "        }\n",
    "\n",
    "    def get_page_content(self, url):\n",
    "        \"\"\"Récupère le contenu HTML d'une page avec gestion des erreurs.\"\"\"\n",
    "        for attempt in range(self.retries):\n",
    "            try:\n",
    "                proxy = next(self.proxies) if self.proxies else None\n",
    "                proxies = {\"http\": proxy, \"https\": proxy} if proxy else None\n",
    "\n",
    "                response = self.session.get(url, headers=self.get_headers(), proxies=proxies, timeout=30)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                if \"captcha\" in response.url.lower():\n",
    "                    logger.error(\"Redirection vers une page CAPTCHA. Impossible de continuer.\")\n",
    "                    return None\n",
    "\n",
    "                logger.debug(f\"Page récupérée avec succès : {url}\")\n",
    "                time.sleep(randint(self.min_delay, self.max_delay))  # Respect des délais\n",
    "                return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                logger.error(f\"Tentative {attempt + 1} échouée pour {url} : {e}\")\n",
    "                if attempt == self.retries - 1:\n",
    "                    return None\n",
    "                time.sleep(randint(5, 10))\n",
    "\n",
    "    def parse_restaurant(self, restaurant):\n",
    "        \"\"\"Analyse les informations d'un restaurant dans une carte.\"\"\"\n",
    "        try:\n",
    "            name_elem = restaurant.find('a', {\"data-test\": \"restaurant-name\"})\n",
    "            address_elem = restaurant.find('div', class_='vQlTa')\n",
    "            rating_elem = restaurant.find('svg', {'aria-label': True})\n",
    "            reviews_elem = restaurant.find('span', {'data-test': 'reviews-count'})\n",
    "            price_elem = restaurant.find('span', {'data-test': 'price-range'})\n",
    "\n",
    "            data = {\n",
    "                \"nom\": name_elem.text.strip() if name_elem else \"N/A\",\n",
    "                \"adresse\": address_elem.text.strip() if address_elem else \"N/A\",\n",
    "                \"note\": rating_elem['aria-label'].split()[0] if rating_elem else \"N/A\",\n",
    "                \"nb_avis\": reviews_elem.text.strip().split()[0] if reviews_elem else \"0\",\n",
    "                \"prix\": price_elem.text.strip() if price_elem else \"N/A\",\n",
    "                \"url\": urljoin(self.base_url, name_elem['href']) if name_elem and name_elem.has_attr('href') else \"N/A\",\n",
    "            }\n",
    "            logger.debug(f\"Restaurant analysé : {data}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de l'analyse d'un restaurant : {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_next_page_url(self, page_number):\n",
    "        \"\"\"Génère l'URL pour paginer les résultats.\"\"\"\n",
    "        offset = page_number * 30  # TripAdvisor utilise un offset par tranches de 30\n",
    "        return f\"{self.base_url}&o=a{offset}\"\n",
    "\n",
    "    def scrape_page(self, page_number=0):\n",
    "        \"\"\"Récupère les données des restaurants d'une page donnée.\"\"\"\n",
    "        url = self.get_next_page_url(page_number)\n",
    "        logger.info(f\"Scraping de la page : {url}\")\n",
    "        soup = self.get_page_content(url)\n",
    "\n",
    "        if not soup:\n",
    "            logger.warning(f\"Pas de contenu récupéré pour la page {page_number}\")\n",
    "            return []\n",
    "\n",
    "        restaurants = []\n",
    "        restaurant_divs = soup.find_all('div', {\"data-test\": \"restaurant-item\"})\n",
    "\n",
    "        for restaurant_div in restaurant_divs:\n",
    "            data = self.parse_restaurant(restaurant_div)\n",
    "            if data:\n",
    "                restaurants.append(data)\n",
    "            time.sleep(randint(1, 2))  # Délai entre les analyses\n",
    "\n",
    "        return restaurants\n",
    "\n",
    "    def scrape_restaurants(self, num_pages=3):\n",
    "        \"\"\"Récupère les informations de plusieurs pages de restaurants.\"\"\"\n",
    "        all_restaurants = []\n",
    "        for page in range(num_pages):\n",
    "            restaurants = self.scrape_page(page)\n",
    "            if restaurants:\n",
    "                all_restaurants.extend(restaurants)\n",
    "                logger.info(f\"Page {page + 1} : {len(restaurants)} restaurants récupérés.\")\n",
    "            time.sleep(randint(self.min_delay + 1, self.max_delay + 2))  # Délai entre les pages\n",
    "        return pd.DataFrame(all_restaurants)\n",
    "\n",
    "def main():\n",
    "    # URL de recherche des restaurants à Lyon\n",
    "    base_url = \"https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity\"\n",
    "    # Liste de proxies à utiliser (à remplacer par vos propres proxies)\n",
    "    proxies = [\"http://proxy1:port\", \"http://proxy2:port\", \"http://proxy3:port\"]\n",
    "\n",
    "    scraper = TripAdvisorScraper(base_url, proxies=proxies)\n",
    "\n",
    "    try:\n",
    "        df = scraper.scrape_restaurants(num_pages=1)\n",
    "        if not df.empty:\n",
    "            df.to_csv('restaurants_lyon.csv', index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"Scraping réussi : {len(df)} restaurants enregistrés dans 'restaurants_lyon.csv'\")\n",
    "        else:\n",
    "            logger.error(\"Aucun restaurant trouvé.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Échec du scraping : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 22:43:03,086 - INFO - Scraping page 1\n",
      "2024-12-18 22:43:03,096 - DEBUG - Starting new HTTPS connection (1): www.tripadvisor.fr:443\n",
      "2024-12-18 22:43:03,689 - DEBUG - https://www.tripadvisor.fr:443 \"GET /RestaurantSearch?geo=187265&sortOrder=popularity&o=a0 HTTP/11\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"YtrWs\" data-test-target=\"restaurants-list\"></div>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 22:43:04,777 - INFO - Found 1 restaurants on page 1\n",
      "2024-12-18 22:43:11,790 - INFO - Successfully scraped 1 restaurants\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent, FakeUserAgentError\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TripAdvisorScraper:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.setup_user_agent()\n",
    "        self.session = requests.Session()\n",
    "        self.min_delay = 3\n",
    "        self.max_delay = 7\n",
    "\n",
    "    def setup_user_agent(self):\n",
    "        try:\n",
    "            self.ua = UserAgent()\n",
    "        except FakeUserAgentError:\n",
    "            logger.warning(\"FakeUserAgent failed, using fallback\")\n",
    "            self.ua = None\n",
    "\n",
    "    def get_headers(self):\n",
    "        default_ua = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
    "        return {\n",
    "            \"User-Agent\": self.ua.random if self.ua else default_ua,\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,/;q=0.8\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Upgrade-Insecure-Requests\": \"1\",\n",
    "            \"Sec-Fetch-Dest\": \"document\",\n",
    "            \"Sec-Fetch-Mode\": \"navigate\",\n",
    "            \"Sec-Fetch-Site\": \"none\",\n",
    "            \"Sec-Fetch-User\": \"?1\",\n",
    "            \"Cache-Control\": \"max-age=0\",\n",
    "        }\n",
    "\n",
    "    def make_request(self, url, retries=3):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self.session.get(\n",
    "                    url,\n",
    "                    headers=self.get_headers(),\n",
    "                    timeout=30\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Request failed (attempt {attempt + 1}): {str(e)}\")\n",
    "                if attempt == retries - 1:\n",
    "                    raise\n",
    "                time.sleep(randint(5, 10))\n",
    "\n",
    "    def parse_restaurant(self, element):\n",
    "        \"\"\"Extrait les informations d'un restaurant.\"\"\"\n",
    "        try:\n",
    "            name_elem = element.find('a', class_='Lwqic Cj b')\n",
    "            return {\n",
    "                'Nom': name_elem.text.strip() if name_elem else \"N/A\",\n",
    "                'URL': urljoin(self.base_url, name_elem['href']) if name_elem and name_elem.has_attr('href') else \"N/A\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de l'analyse d'un restaurant : {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_page(self, page_number):\n",
    "        url = f\"{self.base_url}&o=a{page_number * 30}\"\n",
    "        logger.info(f\"Scraping page {page_number + 1}\")\n",
    "        \n",
    "        try:\n",
    "            content = self.make_request(url)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "            \n",
    "            restaurant_divs = soup.find_all('div', class_='YtrWs')\n",
    "            print(soup.find_all('div', {\"data-test-target\": \"restaurants-list\"}))\n",
    "            if not restaurant_divs:\n",
    "                logger.warning(f\"No restaurants found on page {page_number + 1}\")\n",
    "                return []\n",
    "\n",
    "            restaurants = []\n",
    "            for div in restaurant_divs:\n",
    "                data = self.parse_restaurant(div)\n",
    "                if data:\n",
    "                    restaurants.append(data)\n",
    "                time.sleep(randint(1, 2))\n",
    "\n",
    "            return restaurants\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping page {page_number + 1}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def scrape_restaurants(self, num_pages=1):\n",
    "        all_restaurants = []\n",
    "        empty_pages = 0\n",
    "        \n",
    "        for page in range(num_pages):\n",
    "            restaurants = self.scrape_page(page)\n",
    "            if restaurants:\n",
    "                all_restaurants.extend(restaurants)\n",
    "                empty_pages = 0\n",
    "                logger.info(f\"Found {len(restaurants)} restaurants on page {page + 1}\")\n",
    "            else:\n",
    "                empty_pages += 1\n",
    "                if empty_pages >= 2:\n",
    "                    logger.info(\"Stopping due to consecutive empty pages\")\n",
    "                    break\n",
    "            time.sleep(randint(self.min_delay, self.max_delay))\n",
    "            \n",
    "        return pd.DataFrame(all_restaurants)\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity\"\n",
    "    scraper = TripAdvisorScraper(base_url)\n",
    "    \n",
    "    try:\n",
    "        df = scraper.scrape_restaurants(num_pages=1)\n",
    "        if not df.empty:\n",
    "            df.to_csv('restaurants_lyon.csv', index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"Successfully scraped {len(df)} restaurants\")\n",
    "        else:\n",
    "            logger.error(\"No restaurants found\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Scraping failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 22:56:27,994 - INFO - Scraping de la page 1 : https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a0\n",
      "2024-12-18 22:56:28,003 - DEBUG - Starting new HTTPS connection (1): www.tripadvisor.fr:443\n",
      "2024-12-18 22:56:28,706 - DEBUG - https://www.tripadvisor.fr:443 \"GET /RestaurantSearch?geo=187265&sortOrder=popularity&o=a0 HTTP/11\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d15114321-Reviews-L_affreux_Jojo-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">1<!-- -->. <!-- -->L'affreux Jojo</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d18626103-Reviews-Table_Partage-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">2<!-- -->. <!-- -->Table &amp; Partage</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d23110895-Reviews-Frazarin-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">3<!-- -->. <!-- -->Frazarin Bistrot Franco Italien</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d2027277-Reviews-La_Bouteillerie-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">4<!-- -->. <!-- -->La Bouteillerie</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d19896976-Reviews-Empanadas_Club-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">5<!-- -->. <!-- -->Empanadas Club</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d20287839-Reviews-Agastache_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">6<!-- -->. <!-- -->Agastache Restaurant</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d949361-Reviews-Le_Casse_Museau-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">7<!-- -->. <!-- -->Le Casse Museau</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d15087146-Reviews-La_Table_d_Ambre-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">8<!-- -->. <!-- -->La Table d’Ambre</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d17541665-Reviews-La_Source-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">9<!-- -->. <!-- -->La Source</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d5539701-Reviews-L_Institut_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">10<!-- -->. <!-- -->L'Institut Restaurant</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d12874430-Reviews-Le_Comptoir_Des_Cousins-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">11<!-- -->. <!-- -->Le Comptoir Des Cousins</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d718105-Reviews-L_Alexandrin-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">12<!-- -->. <!-- -->L'Alexandrin</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d15639122-Reviews-La_Table_de_Max-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">13<!-- -->. <!-- -->La Table de Max</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d14895883-Reviews-Rose_de_Damas-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">14<!-- -->. <!-- -->Rose de Damas</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d2507320-Reviews-Chez_Antonin-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">15<!-- -->. <!-- -->Chez Antonin</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d15670272-Reviews-Trattino-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">16<!-- -->. <!-- -->Trattino</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d1331945-Reviews-La_Gargotte-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">17<!-- -->. <!-- -->La Gargotte</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d4993538-Reviews-Le_Boeuf_D_argent-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">18<!-- -->. <!-- -->Le Boeuf D'argent</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d19402701-Reviews-Antolio-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">19<!-- -->. <!-- -->Antolio</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d11816506-Reviews-Restaurant_Lounge_N133-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">20<!-- -->. <!-- -->Restaurant Lounge N133</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d18930236-Reviews-Souk_Souk-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">21<!-- -->. <!-- -->Souk Souk</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d4173079-Reviews-Le_Poivron_Bleu-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">22<!-- -->. <!-- -->Le Poivron Bleu</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d13110017-Reviews-Restaurant_Opaline-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">23<!-- -->. <!-- -->Restaurant Opaline</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d17816279-Reviews-Court_Bouillon-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">24<!-- -->. <!-- -->Court Bouillon</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d6453660-Reviews-Burgundy_By_Matthieu-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">25<!-- -->. <!-- -->Burgundy By Matthieu</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d7015032-Reviews-Copains_Copines_Sur_la_Colline-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">26<!-- -->. <!-- -->Copains Copines Sur la Colline</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d8805775-Reviews-Le_Supreme-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">27<!-- -->. <!-- -->Le Suprême</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d6496648-Reviews-L_Atelier_des_Augustins-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">28<!-- -->. <!-- -->L'Atelier des Augustins</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d8478076-Reviews-Les_Saveurs_du_Bistrot-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">29<!-- -->. <!-- -->Les Saveurs du Bistrot</a></span></div>, <div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d1024908-Reviews-Le_Gourmet_de_Seze-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">30<!-- -->. <!-- -->Le Gourmet de Séze</a></span></div>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 22:57:12,138 - INFO - Page 1 : 30 restaurants trouvés.\n",
      "2024-12-18 22:57:17,145 - INFO - Scraping terminé avec succès : 30 restaurants trouvés.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent, FakeUserAgentError\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint\n",
    "import logging\n",
    "\n",
    "# Configuration du logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TripAdvisorScraper:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.setup_user_agent()\n",
    "        self.session = requests.Session()\n",
    "        self.min_delay = 2\n",
    "        self.max_delay = 5\n",
    "\n",
    "    def setup_user_agent(self):\n",
    "        \"\"\"Initialise le User-Agent avec gestion des erreurs.\"\"\"\n",
    "        try:\n",
    "            self.ua = UserAgent()\n",
    "        except FakeUserAgentError:\n",
    "            logger.warning(\"Impossible d'utiliser FakeUserAgent. Utilisation d'un User-Agent par défaut.\")\n",
    "            self.ua = None\n",
    "\n",
    "    def get_headers(self):\n",
    "        \"\"\"Crée des en-têtes HTTP pour simuler un navigateur.\"\"\"\n",
    "        default_ua = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        return {\n",
    "            \"User-Agent\": self.ua.random if self.ua else default_ua,\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Cache-Control\": \"no-cache\"\n",
    "        }\n",
    "\n",
    "    def make_request(self, url, retries=3):\n",
    "        \"\"\"Effectue une requête GET avec gestion des erreurs.\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self.session.get(url, headers=self.get_headers(), timeout=30)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Requête échouée (tentative {attempt + 1}): {str(e)}\")\n",
    "                if attempt == retries - 1:\n",
    "                    raise\n",
    "                time.sleep(randint(3, 7))\n",
    "\n",
    "    def parse_restaurant(self, element):\n",
    "        \"\"\"Extrait les informations d'un restaurant à partir d'un élément HTML.\"\"\"\n",
    "        try:\n",
    "            name_elem = element.find('a', {\"data-test\": \"restaurant-name\"})\n",
    "            rating_elem = element.find('svg', {'aria-label': True})\n",
    "            reviews_elem = element.find('span', {'data-test': 'reviews-count'})\n",
    "            price_elem = element.find('span', {'data-test': 'price-range'})\n",
    "            \n",
    "            return {\n",
    "                'name': name_elem.text.strip() if name_elem else \"N/A\",\n",
    "                'rating': rating_elem['aria-label'].split()[0] if rating_elem else \"N/A\",\n",
    "                'reviews': reviews_elem.text.strip().split()[0] if reviews_elem else \"0\",\n",
    "                'price': price_elem.text.strip() if price_elem else \"N/A\",\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de l'analyse d'un restaurant : {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_page(self, page_number):\n",
    "        \"\"\"Récupère les informations des restaurants sur une page spécifique.\"\"\"\n",
    "        url = f\"{self.base_url}&o=a{page_number * 30}\"\n",
    "        logger.info(f\"Scraping de la page {page_number + 1} : {url}\")\n",
    "        \n",
    "        try:\n",
    "            content = self.make_request(url)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            with open(f'page_{page_number + 1}.html', 'w', encoding='utf-8') as file:\n",
    "                file.write(str(soup))\n",
    "\n",
    "            \n",
    "            # Extraction des restaurants (sélecteur basé sur la classe RfBGI)\n",
    "            restaurant_divs = soup.find_all('div', class_='RfBGI')\n",
    "            print(restaurant_divs)\n",
    "            if not restaurant_divs:\n",
    "                logger.warning(f\"Aucun restaurant trouvé sur la page {page_number + 1}\")\n",
    "                return []\n",
    "\n",
    "            restaurants = []\n",
    "            for div in restaurant_divs:\n",
    "                data = self.parse_restaurant(div)\n",
    "                if data:\n",
    "                    restaurants.append(data)\n",
    "                time.sleep(randint(1, 2))  # Petite pause entre les éléments\n",
    "\n",
    "            return restaurants\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du scraping de la page {page_number + 1} : {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def scrape_all_pages(self, max_pages=10):\n",
    "        \"\"\"Récupère les informations des restaurants sur plusieurs pages.\"\"\"\n",
    "        all_restaurants = []\n",
    "        empty_pages = 0\n",
    "\n",
    "        for page_number in range(max_pages):\n",
    "            restaurants = self.scrape_page(page_number)\n",
    "            if restaurants:\n",
    "                all_restaurants.extend(restaurants)\n",
    "                empty_pages = 0\n",
    "                logger.info(f\"Page {page_number + 1} : {len(restaurants)} restaurants trouvés.\")\n",
    "            else:\n",
    "                empty_pages += 1\n",
    "                if empty_pages >= 2:\n",
    "                    logger.info(\"Arrêt du scraping après 2 pages vides consécutives.\")\n",
    "                    break\n",
    "            time.sleep(randint(self.min_delay, self.max_delay))  # Pause entre les pages\n",
    "            \n",
    "        return pd.DataFrame(all_restaurants)\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity\"\n",
    "    scraper = TripAdvisorScraper(base_url)\n",
    "\n",
    "    try:\n",
    "        df = scraper.scrape_all_pages(max_pages=1)  # Scrape jusqu'à 5 pages\n",
    "        if not df.empty:\n",
    "            df.to_csv('restaurants_lyon.csv', index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"Scraping terminé avec succès : {len(df)} restaurants trouvés.\")\n",
    "        else:\n",
    "            logger.error(\"Aucun restaurant trouvé.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Échec du scraping : {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
