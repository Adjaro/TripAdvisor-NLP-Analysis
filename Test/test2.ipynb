{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import logging\n",
    "import time\n",
    "from random import randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lien = \"https://www.tripadvisor.fr/Search?q=Bikube&geo=187265&ssrc=a&searchNearby=false&searchSessionId=00094ad53a7c5a29.ssid&offset=0\"\n",
    "\n",
    "headers = {\n",
    "            \"User-Agent\": UserAgent.random, \n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Cache-Control\": \"no-cache\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = requests.get(lien, headers={'User-Agent': UserAgent().chrome})\n",
    "soup = BeautifulSoup(content.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tripadvisor.html', 'w') as f:\n",
    "    f.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = soup.find('main')\n",
    "with open('tripadvisor.html', 'w') as f:\n",
    "    f.write(main.prettify())\n",
    "# print(main.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# data-test-attribute=\"all-results-section\"\n",
    "# class =SVuzf e\n",
    "all_results_section = soup.find_all('div', class_='SVuzf e')\n",
    "print(all_results_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant Details:\n",
      "name: Restaurant Bergamote\n",
      "address: 123 Rue de Gerland, 69007 Lyon France\n",
      "phone: +33 4 78 72 64 32\n",
      "reviews: 159 avis\n",
      "category: 159 avis\n",
      "price_range: Nº 30 sur 3 175 restaurants à Lyon\n",
      "status: Fermé aujourd'hui\n",
      "\n",
      "Found 0 comments\n",
      "\n",
      "Data exported to CSV files\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import logging\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_soup_from_html(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "def scrape_restaurant_details(soup):\n",
    "    restaurant_data = {}\n",
    "    \n",
    "    # Basic Info\n",
    "    name = soup.find(\"h1\", class_=\"biGQs _P hzzSG rRtyp\")\n",
    "    address = soup.find(\"span\", {\"data-automation\": \"restaurantsMapLinkOnName\"})\n",
    "    phone = soup.find(\"a\", href=lambda x: x and x.startswith(\"tel:\"))\n",
    "    reviews = soup.find(\"span\", class_=\"biGQs _P pZUbB KxBGd\")\n",
    "    \n",
    "    restaurant_data[\"name\"] = name.text.strip() if name else \"N/A\"\n",
    "    restaurant_data[\"address\"] = address.text.strip() if address else \"N/A\"\n",
    "    restaurant_data[\"phone\"] = phone.text.strip() if phone else \"N/A\"\n",
    "    restaurant_data[\"reviews\"] = reviews.text.strip() if reviews else \"N/A\"\n",
    "    \n",
    "    # Category and Price\n",
    "    categories = soup.find_all(\"span\", class_=\"biGQs _P pZUbB KxBGd\")\n",
    "    if categories and len(categories) >= 2:\n",
    "        restaurant_data[\"category\"] = categories[0].text.strip()\n",
    "        restaurant_data[\"price_range\"] = categories[1].text.strip()\n",
    "    else:\n",
    "        restaurant_data[\"category\"] = \"N/A\"\n",
    "        restaurant_data[\"price_range\"] = \"N/A\"\n",
    "    \n",
    "    # Status\n",
    "    status = soup.find(\"div\", class_=\"biGQs _P fOtGX\")\n",
    "    restaurant_data[\"status\"] = status.text.strip() if status else \"N/A\"\n",
    "    \n",
    "    return restaurant_data\n",
    "\n",
    "def scrape_comments(soup):\n",
    "    comments_data = []\n",
    "    comment_containers = soup.find_all(\"div\", class_=\"review-container\")\n",
    "    \n",
    "    for container in comment_containers:\n",
    "        try:\n",
    "            title = container.find(\"a\", class_=\"title\")\n",
    "            content = container.find(\"p\", class_=\"partial_entry\")\n",
    "            rating_tag = container.find(\"span\", class_=\"ui_bubble_rating\")\n",
    "            date_tag = container.find(\"span\", class_=\"ratingDate\")\n",
    "            \n",
    "            comment = {\n",
    "                \"title\": title.text.strip() if title else \"N/A\",\n",
    "                \"content\": content.text.strip() if content else \"N/A\",\n",
    "                \"rating\": int(rating_tag[\"class\"][1].split(\"_\")[-1])/10 if rating_tag else None,\n",
    "                \"date\": date_tag[\"title\"] if date_tag else \"N/A\"\n",
    "            }\n",
    "            comments_data.append(comment)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting comment: {e}\")\n",
    "            \n",
    "    return comments_data\n",
    "\n",
    "def main():\n",
    "    file_path = \"test.html\"\n",
    "    soup = get_soup_from_html(file_path)\n",
    "    \n",
    "    # Get restaurant details\n",
    "    restaurant_details = scrape_restaurant_details(soup)\n",
    "    print(\"Restaurant Details:\")\n",
    "    for key, value in restaurant_details.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "        \n",
    "    # Get comments\n",
    "    comments = scrape_comments(soup)\n",
    "    print(f\"\\nFound {len(comments)} comments\")\n",
    "    \n",
    "    # Export to CSV\n",
    "    pd.DataFrame([restaurant_details]).to_csv(\"restaurant_details.csv\", \n",
    "                                            index=False, \n",
    "                                            encoding=\"utf-8-sig\")\n",
    "    pd.DataFrame(comments).to_csv(\"restaurant_comments.csv\", \n",
    "                                 index=False, \n",
    "                                 encoding=\"utf-8-sig\")\n",
    "    \n",
    "    print(\"\\nData exported to CSV files\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Détails du restaurant :\n",
      "Name: N/A\n",
      "Address: N/A\n",
      "Phone: N/A\n",
      "Reviews: N/A\n",
      "Category: N/A\n",
      "Price_range: N/A\n",
      "Status: N/A\n",
      "\n",
      "Commentaires :\n",
      "\n",
      "Les détails du restaurant ont été exportés dans 'restaurant_details.csv'.\n",
      "Les commentaires ont été exportés dans 'restaurant_comments.csv'.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import logging\n",
    "import time\n",
    "from random import randint\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_headers():\n",
    "    ua = UserAgent()\n",
    "    return {\n",
    "        \"User-Agent\": ua.random,\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Cache-Control\": \"max-age=0\",\n",
    "    }\n",
    "\n",
    "def make_request(url, retries=3):\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=get_headers())\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"Request failed: {str(e)}\")\n",
    "            time.sleep(randint(1, 3))\n",
    "    return None\n",
    "\n",
    "def scrape_restaurant_details(soup):\n",
    "    restaurant_data = {}\n",
    "\n",
    "    name = soup.find(\"h1\", class_=\"biGQs _P hzzSG rRtyp\")\n",
    "    restaurant_data[\"name\"] = name.text.strip() if name else \"N/A\"\n",
    "\n",
    "    address = soup.find(\"span\", {\"data-automation\": \"restaurantsMapLinkOnName\"})\n",
    "    restaurant_data[\"address\"] = address.text.strip() if address else \"N/A\"\n",
    "\n",
    "    phone = soup.find(\"a\", href=lambda x: x and x.startswith(\"tel:\"))\n",
    "    restaurant_data[\"phone\"] = phone.text.strip() if phone else \"N/A\"\n",
    "\n",
    "    reviews = soup.find(\"span\", class_=\"biGQs _P pZUbB KxBGd\")\n",
    "    restaurant_data[\"reviews\"] = reviews.text.strip() if reviews else \"N/A\"\n",
    "\n",
    "    categories = soup.find_all(\"span\", class_=\"biGQs _P pZUbB KxBGd\")\n",
    "    if categories and len(categories) >= 2:\n",
    "        restaurant_data[\"category\"] = categories[0].text.strip()\n",
    "        restaurant_data[\"price_range\"] = categories[1].text.strip()\n",
    "    else:\n",
    "        restaurant_data[\"category\"] = \"N/A\"\n",
    "        restaurant_data[\"price_range\"] = \"N/A\"\n",
    "\n",
    "    status = soup.find(\"div\", class_=\"biGQs _P fOtGX\")\n",
    "    restaurant_data[\"status\"] = status.text.strip() if status else \"N/A\"\n",
    "\n",
    "    return restaurant_data\n",
    "\n",
    "def scrape_comments(soup):\n",
    "    comments_data = []\n",
    "\n",
    "    comment_containers = soup.find_all(\"div\", class_=\"review-container\")\n",
    "\n",
    "    for container in comment_containers:\n",
    "        try:\n",
    "            title = container.find(\"a\", class_=\"title\").text.strip()\n",
    "            content = container.find(\"p\", class_=\"partial_entry\").text.strip()\n",
    "            rating_tag = container.find(\"span\", class_=\"ui_bubble_rating\")\n",
    "            rating = int(rating_tag[\"class\"][1].split(\"_\")[-1]) / 10 if rating_tag else None\n",
    "            date_tag = container.find(\"span\", class_=\"ratingDate\")\n",
    "            date = date_tag[\"title\"] if date_tag else \"N/A\"\n",
    "\n",
    "            comments_data.append({\n",
    "                \"title\": title,\n",
    "                \"content\": content,\n",
    "                \"rating\": rating,\n",
    "                \"date\": date\n",
    "            })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de l'extraction d'un commentaire : {e}\")\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "def main():\n",
    "    url = \"https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a0\"\n",
    "    content = make_request(url)\n",
    "    if not content:\n",
    "        logger.error(\"Failed to retrieve the page content\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    restaurant_details = scrape_restaurant_details(soup)\n",
    "    comments = scrape_comments(soup)\n",
    "\n",
    "    print(\"Détails du restaurant :\")\n",
    "    for key, value in restaurant_details.items():\n",
    "        print(f\"{key.capitalize()}: {value}\")\n",
    "\n",
    "    print(\"\\nCommentaires :\")\n",
    "    for comment in comments:\n",
    "        print(f\"- {comment['title']} ({comment['rating']} étoiles) : {comment['content']}\")\n",
    "\n",
    "    details_df = pd.DataFrame([restaurant_details])\n",
    "    details_df.to_csv(\"restaurant_details.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    comments_df = pd.DataFrame(comments)\n",
    "    comments_df.to_csv(\"restaurant_comments.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"\\nLes détails du restaurant ont été exportés dans 'restaurant_details.csv'.\")\n",
    "    print(\"Les commentaires ont été exportés dans 'restaurant_comments.csv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Détails du restaurant :\n",
      "Name: Restaurant Bergamote\n",
      "Address: 123 Rue de Gerland, 69007 Lyon France\n",
      "Phone: +33 4 78 72 64 32\n",
      "Reviews: 159 avis\n",
      "Category: 159 avis\n",
      "Price_range: Nº 30 sur 3 175 restaurants à Lyon\n",
      "Status: Fermé aujourd'hui\n",
      "\n",
      "Commentaires :\n",
      "\n",
      "Les détails du restaurant ont été exportés dans 'restaurant_details.csv'.\n",
      "Les commentaires ont été exportés dans 'restaurant_comments.csv'.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_restaurant_details(file_path):\n",
    "    # Charger le fichier HTML local\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "    \n",
    "    # Initialiser un dictionnaire pour stocker les données générales\n",
    "    restaurant_data = {}\n",
    "\n",
    "    # Récupérer le nom du restaurant\n",
    "    name = soup.find(\"h1\", class_=\"biGQs _P hzzSG rRtyp\")\n",
    "    restaurant_data[\"name\"] = name.text.strip() if name else \"N/A\"\n",
    "\n",
    "    # Récupérer l'adresse\n",
    "    address = soup.find(\"span\", {\"data-automation\": \"restaurantsMapLinkOnName\"})\n",
    "    restaurant_data[\"address\"] = address.text.strip() if address else \"N/A\"\n",
    "\n",
    "    # Récupérer le numéro de téléphone\n",
    "    phone = soup.find(\"a\", href=lambda x: x and x.startswith(\"tel:\"))\n",
    "    restaurant_data[\"phone\"] = phone.text.strip() if phone else \"N/A\"\n",
    "\n",
    "    # Récupérer les avis (nombre et note moyenne)\n",
    "    reviews = soup.find(\"span\", class_=\"biGQs _P pZUbB KxBGd\")\n",
    "    restaurant_data[\"reviews\"] = reviews.text.strip() if reviews else \"N/A\"\n",
    "\n",
    "    # Récupérer la catégorie de cuisine et la gamme de prix\n",
    "    categories = soup.find_all(\"span\", class_=\"biGQs _P pZUbB KxBGd\")\n",
    "    if categories and len(categories) >= 2:\n",
    "        restaurant_data[\"category\"] = categories[0].text.strip()\n",
    "        restaurant_data[\"price_range\"] = categories[1].text.strip()\n",
    "    else:\n",
    "        restaurant_data[\"category\"] = \"N/A\"\n",
    "        restaurant_data[\"price_range\"] = \"N/A\"\n",
    "\n",
    "    # Récupérer le statut (ouvert ou fermé)\n",
    "    status = soup.find(\"div\", class_=\"biGQs _P fOtGX\")\n",
    "    restaurant_data[\"status\"] = status.text.strip() if status else \"N/A\"\n",
    "\n",
    "    return restaurant_data\n",
    "\n",
    "def scrape_comments(file_path):\n",
    "    # Charger le fichier HTML local\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "    \n",
    "    # Initialiser une liste pour stocker les commentaires\n",
    "    comments_data = []\n",
    "\n",
    "    # Localiser les conteneurs des commentaires\n",
    "    comment_containers = soup.find_all(\"div\", class_=\"review-container\")\n",
    "\n",
    "    for container in comment_containers:\n",
    "        try:\n",
    "            # Titre du commentaire\n",
    "            title = container.find(\"a\", class_=\"title\").text.strip()\n",
    "\n",
    "            # Contenu du commentaire\n",
    "            content = container.find(\"p\", class_=\"partial_entry\").text.strip()\n",
    "\n",
    "            # Note (exemple : \"bubble_50\" -> 5 étoiles)\n",
    "            rating_tag = container.find(\"span\", class_=\"ui_bubble_rating\")\n",
    "            rating = int(rating_tag[\"class\"][1].split(\"_\")[-1]) / 10 if rating_tag else None\n",
    "\n",
    "            # Date de l'avis\n",
    "            date_tag = container.find(\"span\", class_=\"ratingDate\")\n",
    "            date = date_tag[\"title\"] if date_tag else \"N/A\"\n",
    "\n",
    "            # Ajouter le commentaire au tableau\n",
    "            comments_data.append({\n",
    "                \"title\": title,\n",
    "                \"content\": content,\n",
    "                \"rating\": rating,\n",
    "                \"date\": date\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'extraction d'un commentaire : {e}\")\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "def main():\n",
    "    # Spécifiez le chemin vers le fichier HTML\n",
    "    file_path = \"test.html\"\n",
    "    \n",
    "    # Scraper les détails du restaurant\n",
    "    restaurant_details = scrape_restaurant_details(file_path)\n",
    "    \n",
    "    # Scraper les commentaires\n",
    "    comments = scrape_comments(file_path)\n",
    "    \n",
    "    # Afficher les résultats du restaurant\n",
    "    print(\"Détails du restaurant :\")\n",
    "    for key, value in restaurant_details.items():\n",
    "        print(f\"{key.capitalize()}: {value}\")\n",
    "    \n",
    "    # Afficher les commentaires\n",
    "    print(\"\\nCommentaires :\")\n",
    "    for comment in comments:\n",
    "        print(f\"- {comment['title']} ({comment['rating']} étoiles) : {comment['content']}\")\n",
    "\n",
    "    # Exporter les résultats dans des fichiers CSV\n",
    "    details_df = pd.DataFrame([restaurant_details])\n",
    "    details_df.to_csv(\"restaurant_details.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    comments_df = pd.DataFrame(comments)\n",
    "    comments_df.to_csv(\"restaurant_comments.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"\\nLes détails du restaurant ont été exportés dans 'restaurant_details.csv'.\")\n",
    "    print(\"Les commentaires ont été exportés dans 'restaurant_comments.csv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Restaurant Bergamote\n",
      "Address: 123 Rue de Gerland, 69007 Lyon France\n",
      "Phone: +33 4 78 72 64 32\n",
      "Reviews: 159 avis\n",
      "Category: 159 avis\n",
      "Price_range: Nº 30 sur 3 175 restaurants à Lyon\n",
      "Status: Fermé aujourd'hui\n",
      "\n",
      "Les détails ont été exportés dans 'restaurant_details.csv'.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def scrape_restaurant_details(file_path):\n",
    "    # Charger le fichier HTML local\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        soup = BeautifulSoup(file, \"html.parser\")\n",
    "    \n",
    "    # Initialiser un dictionnaire pour stocker les données\n",
    "    restaurant_data = {}\n",
    "\n",
    "    # Récupérer le nom du restaurant\n",
    "    name = soup.find(\"h1\", class_=\"biGQs _P hzzSG rRtyp\")\n",
    "    restaurant_data[\"name\"] = name.text.strip() if name else \"N/A\"\n",
    "\n",
    "    # Récupérer l'adresse\n",
    "    address = soup.find(\"span\", {\"data-automation\": \"restaurantsMapLinkOnName\"})\n",
    "    restaurant_data[\"address\"] = address.text.strip() if address else \"N/A\"\n",
    "\n",
    "    # Récupérer le numéro de téléphone\n",
    "    phone = soup.find(\"a\", href=lambda x: x and x.startswith(\"tel:\"))\n",
    "    restaurant_data[\"phone\"] = phone.text.strip() if phone else \"N/A\"\n",
    "\n",
    "    # Récupérer les avis (nombre et note moyenne)\n",
    "    reviews = soup.find(\"span\", class_=\"biGQs _P pZUbB KxBGd\")\n",
    "    restaurant_data[\"reviews\"] = reviews.text.strip() if reviews else \"N/A\"\n",
    "\n",
    "    # Récupérer la catégorie de cuisine et la gamme de prix\n",
    "    categories = soup.find_all(\"span\", class_=\"biGQs _P pZUbB KxBGd\")\n",
    "    if categories and len(categories) >= 2:\n",
    "        restaurant_data[\"category\"] = categories[0].text.strip()\n",
    "        restaurant_data[\"price_range\"] = categories[1].text.strip()\n",
    "    else:\n",
    "        restaurant_data[\"category\"] = \"N/A\"\n",
    "        restaurant_data[\"price_range\"] = \"N/A\"\n",
    "\n",
    "    # Récupérer le statut (ouvert ou fermé)\n",
    "    status = soup.find(\"div\", class_=\"biGQs _P fOtGX\")\n",
    "    restaurant_data[\"status\"] = status.text.strip() if status else \"N/A\"\n",
    "\n",
    "    return restaurant_data\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Spécifiez le chemin vers le fichier HTML\n",
    "    file_path = \"test.html\"\n",
    "    \n",
    "    # Scraper les détails du restaurant\n",
    "    restaurant_details = scrape_restaurant_details(file_path)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    for key, value in restaurant_details.items():\n",
    "        print(f\"{key.capitalize()}: {value}\")\n",
    "    \n",
    "    # Exporter les résultats dans un fichier CSV\n",
    "    df = pd.DataFrame([restaurant_details])\n",
    "    df.to_csv(\"restaurant_details.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"\\nLes détails ont été exportés dans 'restaurant_details.csv'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 12:21:46,195 - INFO - Scraping des résultats à l'offset 0 : https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a0\n",
      "2024-12-22 12:22:39,419 - INFO - Offset 0 : 31 restaurants trouvés.\n",
      "2024-12-22 12:22:42,426 - INFO - Scraping terminé avec succès : 31 restaurants trouvés.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent, FakeUserAgentError\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TripAdvisorScraper:\n",
    "    def __init__(self, base_url):\n",
    "        \"\"\"\n",
    "        Initialise le scraper.\n",
    "        :param base_url: URL de base pour le scraping\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.setup_user_agent()\n",
    "        self.session = requests.Session()\n",
    "        self.min_delay = 2\n",
    "        self.max_delay = 5\n",
    "\n",
    "    def setup_user_agent(self):\n",
    "        \"\"\"Initialise le User-Agent avec gestion des erreurs.\"\"\"\n",
    "        try:\n",
    "            self.ua = UserAgent()\n",
    "        except FakeUserAgentError:\n",
    "            logger.warning(\"Impossible d'utiliser FakeUserAgent. Utilisation d'un User-Agent par défaut.\")\n",
    "            self.ua = None\n",
    "\n",
    "    def get_headers(self):\n",
    "        \"\"\"Crée des en-têtes HTTP pour simuler un navigateur.\"\"\"\n",
    "        default_ua = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        return {\n",
    "            \"User-Agent\": self.ua.random if self.ua else default_ua,\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Cache-Control\": \"no-cache\"\n",
    "        }\n",
    "\n",
    "    def build_url(self, offset):\n",
    "        \"\"\"Construit l'URL pour une page donnée.\"\"\"\n",
    "        return f\"{self.base_url}&o=a{offset}\"\n",
    "\n",
    "    def make_request(self, url, retries=3):\n",
    "        \"\"\"Effectue une requête GET avec gestion des erreurs et mise à jour du User-Agent.\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                headers = self.get_headers()\n",
    "                response = self.session.get(url, headers=headers, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Requête échouée (tentative {attempt + 1}) : {str(e)}\")\n",
    "                if attempt == retries - 1:\n",
    "                    raise\n",
    "                time.sleep(randint(3, 7))\n",
    "\n",
    "    def parse_restaurant(self, element):\n",
    "        \"\"\"Extrait les informations d'un restaurant.\"\"\"\n",
    "        try:\n",
    "            name_elem = element.find('a', class_='Lwqic Cj b')\n",
    "            return {\n",
    "                'Nom': name_elem.text.strip() if name_elem else \"N/A\",\n",
    "                'URL': urljoin(self.base_url, name_elem['href']) if name_elem and name_elem.has_attr('href') else \"N/A\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de l'analyse d'un restaurant : {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_page(self, offset):\n",
    "        \"\"\"Récupère les informations des restaurants sur une page spécifique.\"\"\"\n",
    "        url = self.build_url(offset)\n",
    "        logger.info(f\"Scraping des résultats à l'offset {offset} : {url}\")\n",
    "\n",
    "        try:\n",
    "            content = self.make_request(url)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Extraction des restaurants\n",
    "            restaurant_divs = soup.find_all('div', class_='RfBGI')\n",
    "            restaurants = []\n",
    "\n",
    "            for div in restaurant_divs:\n",
    "                data = self.parse_restaurant(div)\n",
    "                if data:\n",
    "                    restaurants.append(data)\n",
    "                time.sleep(randint(1, 2))\n",
    "\n",
    "            return restaurants\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du scraping de l'offset {offset} : {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def scrape_all_pages(self, max_pages=50):\n",
    "        \"\"\"Récupère les informations des restaurants sur plusieurs pages.\"\"\"\n",
    "        all_restaurants = []\n",
    "        empty_pages = 0\n",
    "\n",
    "        for page_number in range(max_pages):\n",
    "            offset = page_number * 30\n",
    "            restaurants = self.scrape_page(offset)\n",
    "            if restaurants:\n",
    "                all_restaurants.extend(restaurants)\n",
    "                empty_pages = 0\n",
    "                logger.info(f\"Offset {offset} : {len(restaurants)} restaurants trouvés.\")\n",
    "            else:\n",
    "                empty_pages += 1\n",
    "                if empty_pages >= 2:\n",
    "                    logger.info(\"Arrêt du scraping après 2 pages vides consécutives.\")\n",
    "                    break\n",
    "            time.sleep(randint(self.min_delay, self.max_delay))  # Pause entre les pages\n",
    "            \n",
    "        return pd.DataFrame(all_restaurants)\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity\"\n",
    "    scraper = TripAdvisorScraper(base_url)\n",
    "\n",
    "    try:\n",
    "        df = scraper.scrape_all_pages(max_pages=1)  # Scrape jusqu'à 50 pages\n",
    "        if not df.empty:\n",
    "            df.to_csv('restaurants_lyon.csv', index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"Scraping terminé avec succès : {len(df)} restaurants trouvés.\")\n",
    "        else:\n",
    "            logger.error(\"Aucun restaurant trouvé.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Échec du scraping : {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 12:04:21,021 - INFO - Scraping des résultats à l'offset 0 : https://www.tripadvisor.fr/Search?q=Bouchon+Lyonnais&geo=187265&ssrc=e&searchNearby=false&offset=0\n",
      "2024-12-22 12:04:21,364 - ERROR - Erreur lors du scraping de l'offset 0 : 'NoneType' object is not callable\n",
      "2024-12-22 12:04:24,368 - INFO - Scraping des résultats à l'offset 30 : https://www.tripadvisor.fr/Search?q=Bouchon+Lyonnais&geo=187265&ssrc=e&searchNearby=false&offset=30\n",
      "2024-12-22 12:04:24,678 - ERROR - Erreur lors du scraping de l'offset 30 : 'NoneType' object is not callable\n",
      "2024-12-22 12:04:24,678 - INFO - Arrêt du scraping après 2 pages vides consécutives.\n",
      "2024-12-22 12:04:24,678 - ERROR - Aucun restaurant trouvé.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent, FakeUserAgentError\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint\n",
    "import logging\n",
    "\n",
    "# Configuration du logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TripAdvisorScraper:\n",
    "    def __init__(self, base_url, restaurant_name=None):\n",
    "        \"\"\"\n",
    "        Initialise le scraper.\n",
    "        :param base_url: URL de base pour le scraping\n",
    "        :param restaurant_name: Nom du restaurant pour filtrer les résultats (facultatif)\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.restaurant_name = restaurant_name\n",
    "        self.setup_user_agent()\n",
    "        self.session = requests.Session()\n",
    "        self.min_delay = 2\n",
    "        self.max_delay = 5\n",
    "\n",
    "    def setup_user_agent(self):\n",
    "        \"\"\"Initialise le User-Agent avec gestion des erreurs.\"\"\"\n",
    "        try:\n",
    "            self.ua = UserAgent()\n",
    "        except FakeUserAgentError:\n",
    "            logger.warning(\"Impossible d'utiliser FakeUserAgent. Utilisation d'un User-Agent par défaut.\")\n",
    "            self.ua = None\n",
    "\n",
    "    def get_headers(self):\n",
    "        \"\"\"Crée des en-têtes HTTP pour simuler un navigateur.\"\"\"\n",
    "        default_ua = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        return {\n",
    "            \"User-Agent\": self.ua.random if self.ua else default_ua,\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Cache-Control\": \"no-cache\"\n",
    "        }\n",
    "\n",
    "    def build_url(self, offset):\n",
    "        \"\"\"Construit l'URL pour une page donnée avec un nom de restaurant.\"\"\"\n",
    "        if self.restaurant_name:\n",
    "            # Ajout du nom du restaurant et des paramètres supplémentaires\n",
    "            query = self.restaurant_name.replace(' ', '+')\n",
    "            return f\"{self.base_url}?q={query}&geo=187265&ssrc=e&searchNearby=false&offset={offset}\"\n",
    "        else:\n",
    "            raise ValueError(\"Un nom de restaurant est requis pour cette recherche.\")\n",
    "\n",
    "    def make_request(self, url, retries=3):\n",
    "        \"\"\"Effectue une requête GET avec gestion des erreurs.\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self.session.get(url, headers=self.get_headers(), timeout=30)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Requête échouée (tentative {attempt + 1}) : {str(e)}\")\n",
    "                if attempt == retries - 1:\n",
    "                    raise\n",
    "                time.sleep(randint(3, 7))\n",
    "\n",
    "    def scrape_page(self, offset):\n",
    "        \"\"\"Récupère les informations des restaurants sur une page spécifique.\"\"\"\n",
    "        url = self.build_url(offset)\n",
    "        logger.info(f\"Scraping des résultats à l'offset {offset} : {url}\")\n",
    "\n",
    "        try:\n",
    "            content = self.make_request(url)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            main_div = soup.find('main')\n",
    "            \n",
    "            # main_div = BeautifulSoup(main_div, 'html.parser')\n",
    "            # Extraction des restaurants\n",
    "            restaurant_divs = main_div.find_all('div', class_='kgrOn o')\n",
    "           \n",
    "            restaurants = []\n",
    "\n",
    "            for div in restaurant_divs:\n",
    "                a_tag = div.find('a', class_='BMQDV _F Gv wSSLS SwZTJ')\n",
    "                if a_tag:\n",
    "                    href = a_tag['href']\n",
    "                    name = ''.join(a_tag.stripped_strings)\n",
    "                    # Supprimer les numéros en début de nom, s'il y en a\n",
    "                    name = name.split(\". \", 1)[-1]\n",
    "                    restaurants.append({'name': name, 'href': href})\n",
    "\n",
    "            return restaurants\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du scraping de l'offset {offset} : {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def scrape_all_pages(self, max_pages=2):\n",
    "        \"\"\"Récupère les informations des restaurants sur plusieurs pages.\"\"\"\n",
    "        all_restaurants = []\n",
    "        empty_pages = 0\n",
    "\n",
    "        for page_number in range(max_pages):\n",
    "            offset = page_number * 30\n",
    "            restaurants = self.scrape_page(offset)\n",
    "            if restaurants:\n",
    "                all_restaurants.extend(restaurants)\n",
    "                empty_pages = 0\n",
    "                logger.info(f\"Offset {offset} : {len(restaurants)} restaurants trouvés.\")\n",
    "            else:\n",
    "                empty_pages += 1\n",
    "                if empty_pages >= 2:\n",
    "                    logger.info(\"Arrêt du scraping après 2 pages vides consécutives.\")\n",
    "                    break\n",
    "            time.sleep(randint(self.min_delay, self.max_delay))  # Pause entre les pages\n",
    "            \n",
    "        return pd.DataFrame(all_restaurants)\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.tripadvisor.fr/Search\"\n",
    "    restaurant_name = \"Bouchon Lyonnais\"  # Nom du restaurant pour filtrer\n",
    "    scraper = TripAdvisorScraper(base_url, restaurant_name)\n",
    "\n",
    "    try:\n",
    "        df = scraper.scrape_all_pages(max_pages=10)  # Scrape jusqu'à 10 pages\n",
    "        if not df.empty:\n",
    "            df.to_csv('restaurants_lyon.csv', index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"Scraping terminé avec succès : {len(df)} restaurants trouvés.\")\n",
    "        else:\n",
    "            logger.error(\"Aucun restaurant trouvé.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Échec du scraping : {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom : Bikube Lyon - Restaurant, Lien : /Restaurant_Review-g187265-d28100759-Reviews-Bikube_Lyon_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n",
      "Nom : Angelo | Italian Restaurant, Lien : /Restaurant_Review-g187265-d16216238-Reviews-Angelo_Italian_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n",
      "Nom : 1.L'affreux Jojo, Lien : /Restaurant_Review-g187265-d15114321-Reviews-L_affreux_Jojo-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n",
      "Nom : 2.Table & Partage, Lien : /Restaurant_Review-g187265-d18626103-Reviews-Table_Partage-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n",
      "Nom : 3.Frazarin Bistrot Franco Italien, Lien : /Restaurant_Review-g187265-d23110895-Reviews-Frazarin-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n",
      "Nom : 4.La Bouteillerie, Lien : /Restaurant_Review-g187265-d2027277-Reviews-La_Bouteillerie-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n",
      "Nom : 5.Empanadas Club, Lien : /Restaurant_Review-g187265-d19896976-Reviews-Empanadas_Club-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Le HTML à analyser\n",
    "html_content = '''\n",
    "<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d28100759-Reviews-Bikube_Lyon_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">Bikube Lyon - Restaurant</a></span></div>\n",
    "<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d16216238-Reviews-Angelo_Italian_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">Angelo | Italian Restaurant</a></span></div>\n",
    "<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d15114321-Reviews-L_affreux_Jojo-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">1<!-- -->. <!-- -->L'affreux Jojo</a></span></div>\n",
    "<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d18626103-Reviews-Table_Partage-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">2<!-- -->. <!-- -->Table &amp; Partage</a></span></div>\n",
    "<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d23110895-Reviews-Frazarin-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">3<!-- -->. <!-- -->Frazarin Bistrot Franco Italien</a></span></div>\n",
    "<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d2027277-Reviews-La_Bouteillerie-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">4<!-- -->. <!-- -->La Bouteillerie</a></span></div>\n",
    "<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d19896976-Reviews-Empanadas_Club-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">5<!-- -->. <!-- -->Empanadas Club</a></span></div>\n",
    "'''\n",
    "\n",
    "# Création de l'objet BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extraction des liens et des noms\n",
    "restaurants = []\n",
    "for a_tag in soup.find_all('a', class_='Lwqic Cj b'):\n",
    "    href = a_tag['href']\n",
    "    # Supprime les éventuels commentaires ou éléments inutiles dans le texte\n",
    "    name = ''.join(a_tag.stripped_strings)\n",
    "    restaurants.append({'name': name, 'href': href})\n",
    "\n",
    "# Affichage des résultats\n",
    "for restaurant in restaurants:\n",
    "    print(f\"Nom : {restaurant['name']}, Lien : {restaurant['href']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
