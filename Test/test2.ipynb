{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 12:19:59,843 - INFO - Scraping des résultats à l'offset 0 : https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a0\n",
      "2024-12-22 12:20:44,310 - INFO - Offset 0 : 31 restaurants trouvés.\n",
      "2024-12-22 12:20:48,313 - INFO - Scraping des résultats à l'offset 30 : https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a30\n",
      "2024-12-22 12:21:33,306 - INFO - Offset 30 : 31 restaurants trouvés.\n",
      "2024-12-22 12:21:38,317 - INFO - Scraping des résultats à l'offset 60 : https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity&o=a60\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent, FakeUserAgentError\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint\n",
    "import logging\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TripAdvisorScraper:\n",
    "    def __init__(self, base_url):\n",
    "        \"\"\"\n",
    "        Initialise le scraper.\n",
    "        :param base_url: URL de base pour le scraping\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.setup_user_agent()\n",
    "        self.session = requests.Session()\n",
    "        self.min_delay = 2\n",
    "        self.max_delay = 5\n",
    "\n",
    "    def setup_user_agent(self):\n",
    "        \"\"\"Initialise le User-Agent avec gestion des erreurs.\"\"\"\n",
    "        try:\n",
    "            self.ua = UserAgent()\n",
    "        except FakeUserAgentError:\n",
    "            logger.warning(\"Impossible d'utiliser FakeUserAgent. Utilisation d'un User-Agent par défaut.\")\n",
    "            self.ua = None\n",
    "\n",
    "    def get_headers(self):\n",
    "        \"\"\"Crée des en-têtes HTTP pour simuler un navigateur.\"\"\"\n",
    "        default_ua = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        return {\n",
    "            \"User-Agent\": self.ua.random if self.ua else default_ua,\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Cache-Control\": \"no-cache\"\n",
    "        }\n",
    "\n",
    "    def build_url(self, offset):\n",
    "        \"\"\"Construit l'URL pour une page donnée.\"\"\"\n",
    "        return f\"{self.base_url}&o=a{offset}\"\n",
    "\n",
    "    def make_request(self, url, retries=3):\n",
    "        \"\"\"Effectue une requête GET avec gestion des erreurs et mise à jour du User-Agent.\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                headers = self.get_headers()\n",
    "                response = self.session.get(url, headers=headers, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Requête échouée (tentative {attempt + 1}) : {str(e)}\")\n",
    "                if attempt == retries - 1:\n",
    "                    raise\n",
    "                time.sleep(randint(3, 7))\n",
    "\n",
    "    def parse_restaurant(self, element):\n",
    "        \"\"\"Extrait les informations d'un restaurant.\"\"\"\n",
    "        try:\n",
    "            name_elem = element.find('a', class_='Lwqic Cj b')\n",
    "            return {\n",
    "                'Nom': name_elem.text.strip() if name_elem else \"N/A\",\n",
    "                'URL': urljoin(self.base_url, name_elem['href']) if name_elem and name_elem.has_attr('href') else \"N/A\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors de l'analyse d'un restaurant : {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_page(self, offset):\n",
    "        \"\"\"Récupère les informations des restaurants sur une page spécifique.\"\"\"\n",
    "        url = self.build_url(offset)\n",
    "        logger.info(f\"Scraping des résultats à l'offset {offset} : {url}\")\n",
    "\n",
    "        try:\n",
    "            content = self.make_request(url)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # Extraction des restaurants\n",
    "            restaurant_divs = soup.find_all('div', class_='RfBGI')\n",
    "            restaurants = []\n",
    "\n",
    "            for div in restaurant_divs:\n",
    "                data = self.parse_restaurant(div)\n",
    "                if data:\n",
    "                    restaurants.append(data)\n",
    "                time.sleep(randint(1, 2))\n",
    "\n",
    "            return restaurants\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du scraping de l'offset {offset} : {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def scrape_all_pages(self, max_pages=50):\n",
    "        \"\"\"Récupère les informations des restaurants sur plusieurs pages.\"\"\"\n",
    "        all_restaurants = []\n",
    "        empty_pages = 0\n",
    "\n",
    "        for page_number in range(max_pages):\n",
    "            offset = page_number * 30\n",
    "            restaurants = self.scrape_page(offset)\n",
    "            if restaurants:\n",
    "                all_restaurants.extend(restaurants)\n",
    "                empty_pages = 0\n",
    "                logger.info(f\"Offset {offset} : {len(restaurants)} restaurants trouvés.\")\n",
    "            else:\n",
    "                empty_pages += 1\n",
    "                if empty_pages >= 2:\n",
    "                    logger.info(\"Arrêt du scraping après 2 pages vides consécutives.\")\n",
    "                    break\n",
    "            time.sleep(randint(self.min_delay, self.max_delay))  # Pause entre les pages\n",
    "            \n",
    "        return pd.DataFrame(all_restaurants)\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.tripadvisor.fr/RestaurantSearch?geo=187265&sortOrder=popularity\"\n",
    "    scraper = TripAdvisorScraper(base_url)\n",
    "\n",
    "    try:\n",
    "        df = scraper.scrape_all_pages(max_pages=1)  # Scrape jusqu'à 50 pages\n",
    "        if not df.empty:\n",
    "            df.to_csv('restaurants_lyon.csv', index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"Scraping terminé avec succès : {len(df)} restaurants trouvés.\")\n",
    "        else:\n",
    "            logger.error(\"Aucun restaurant trouvé.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Échec du scraping : {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 12:04:21,021 - INFO - Scraping des résultats à l'offset 0 : https://www.tripadvisor.fr/Search?q=Bouchon+Lyonnais&geo=187265&ssrc=e&searchNearby=false&offset=0\n",
      "2024-12-22 12:04:21,364 - ERROR - Erreur lors du scraping de l'offset 0 : 'NoneType' object is not callable\n",
      "2024-12-22 12:04:24,368 - INFO - Scraping des résultats à l'offset 30 : https://www.tripadvisor.fr/Search?q=Bouchon+Lyonnais&geo=187265&ssrc=e&searchNearby=false&offset=30\n",
      "2024-12-22 12:04:24,678 - ERROR - Erreur lors du scraping de l'offset 30 : 'NoneType' object is not callable\n",
      "2024-12-22 12:04:24,678 - INFO - Arrêt du scraping après 2 pages vides consécutives.\n",
      "2024-12-22 12:04:24,678 - ERROR - Aucun restaurant trouvé.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent, FakeUserAgentError\n",
    "import pandas as pd\n",
    "import time\n",
    "from random import randint\n",
    "import logging\n",
    "\n",
    "# Configuration du logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TripAdvisorScraper:\n",
    "    def __init__(self, base_url, restaurant_name=None):\n",
    "        \"\"\"\n",
    "        Initialise le scraper.\n",
    "        :param base_url: URL de base pour le scraping\n",
    "        :param restaurant_name: Nom du restaurant pour filtrer les résultats (facultatif)\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.restaurant_name = restaurant_name\n",
    "        self.setup_user_agent()\n",
    "        self.session = requests.Session()\n",
    "        self.min_delay = 2\n",
    "        self.max_delay = 5\n",
    "\n",
    "    def setup_user_agent(self):\n",
    "        \"\"\"Initialise le User-Agent avec gestion des erreurs.\"\"\"\n",
    "        try:\n",
    "            self.ua = UserAgent()\n",
    "        except FakeUserAgentError:\n",
    "            logger.warning(\"Impossible d'utiliser FakeUserAgent. Utilisation d'un User-Agent par défaut.\")\n",
    "            self.ua = None\n",
    "\n",
    "    def get_headers(self):\n",
    "        \"\"\"Crée des en-têtes HTTP pour simuler un navigateur.\"\"\"\n",
    "        default_ua = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        return {\n",
    "            \"User-Agent\": self.ua.random if self.ua else default_ua,\n",
    "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "            \"Accept-Language\": \"fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "            \"Connection\": \"keep-alive\",\n",
    "            \"Cache-Control\": \"no-cache\"\n",
    "        }\n",
    "\n",
    "    def build_url(self, offset):\n",
    "        \"\"\"Construit l'URL pour une page donnée avec un nom de restaurant.\"\"\"\n",
    "        if self.restaurant_name:\n",
    "            # Ajout du nom du restaurant et des paramètres supplémentaires\n",
    "            query = self.restaurant_name.replace(' ', '+')\n",
    "            return f\"{self.base_url}?q={query}&geo=187265&ssrc=e&searchNearby=false&offset={offset}\"\n",
    "        else:\n",
    "            raise ValueError(\"Un nom de restaurant est requis pour cette recherche.\")\n",
    "\n",
    "    def make_request(self, url, retries=3):\n",
    "        \"\"\"Effectue une requête GET avec gestion des erreurs.\"\"\"\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = self.session.get(url, headers=self.get_headers(), timeout=30)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Requête échouée (tentative {attempt + 1}) : {str(e)}\")\n",
    "                if attempt == retries - 1:\n",
    "                    raise\n",
    "                time.sleep(randint(3, 7))\n",
    "\n",
    "    def scrape_page(self, offset):\n",
    "        \"\"\"Récupère les informations des restaurants sur une page spécifique.\"\"\"\n",
    "        url = self.build_url(offset)\n",
    "        logger.info(f\"Scraping des résultats à l'offset {offset} : {url}\")\n",
    "\n",
    "        try:\n",
    "            content = self.make_request(url)\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            main_div = soup.find('main')\n",
    "            \n",
    "            # main_div = BeautifulSoup(main_div, 'html.parser')\n",
    "            # Extraction des restaurants\n",
    "            restaurant_divs = main_div.find_all('div', class_='kgrOn o')\n",
    "           \n",
    "            restaurants = []\n",
    "\n",
    "            for div in restaurant_divs:\n",
    "                a_tag = div.find('a', class_='BMQDV _F Gv wSSLS SwZTJ')\n",
    "                if a_tag:\n",
    "                    href = a_tag['href']\n",
    "                    name = ''.join(a_tag.stripped_strings)\n",
    "                    # Supprimer les numéros en début de nom, s'il y en a\n",
    "                    name = name.split(\". \", 1)[-1]\n",
    "                    restaurants.append({'name': name, 'href': href})\n",
    "\n",
    "            return restaurants\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erreur lors du scraping de l'offset {offset} : {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def scrape_all_pages(self, max_pages=2):\n",
    "        \"\"\"Récupère les informations des restaurants sur plusieurs pages.\"\"\"\n",
    "        all_restaurants = []\n",
    "        empty_pages = 0\n",
    "\n",
    "        for page_number in range(max_pages):\n",
    "            offset = page_number * 30\n",
    "            restaurants = self.scrape_page(offset)\n",
    "            if restaurants:\n",
    "                all_restaurants.extend(restaurants)\n",
    "                empty_pages = 0\n",
    "                logger.info(f\"Offset {offset} : {len(restaurants)} restaurants trouvés.\")\n",
    "            else:\n",
    "                empty_pages += 1\n",
    "                if empty_pages >= 2:\n",
    "                    logger.info(\"Arrêt du scraping après 2 pages vides consécutives.\")\n",
    "                    break\n",
    "            time.sleep(randint(self.min_delay, self.max_delay))  # Pause entre les pages\n",
    "            \n",
    "        return pd.DataFrame(all_restaurants)\n",
    "\n",
    "def main():\n",
    "    base_url = \"https://www.tripadvisor.fr/Search\"\n",
    "    restaurant_name = \"Bouchon Lyonnais\"  # Nom du restaurant pour filtrer\n",
    "    scraper = TripAdvisorScraper(base_url, restaurant_name)\n",
    "\n",
    "    try:\n",
    "        df = scraper.scrape_all_pages(max_pages=10)  # Scrape jusqu'à 10 pages\n",
    "        if not df.empty:\n",
    "            df.to_csv('restaurants_lyon.csv', index=False, encoding='utf-8-sig')\n",
    "            logger.info(f\"Scraping terminé avec succès : {len(df)} restaurants trouvés.\")\n",
    "        else:\n",
    "            logger.error(\"Aucun restaurant trouvé.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Échec du scraping : {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom : Bikube Lyon - Restaurant, Lien : /Restaurant_Review-g187265-d28100759-Reviews-Bikube_Lyon_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n",
      "Nom : Angelo | Italian Restaurant, Lien : /Restaurant_Review-g187265-d16216238-Reviews-Angelo_Italian_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n",
      "Nom : 1.L'affreux Jojo, Lien : /Restaurant_Review-g187265-d15114321-Reviews-L_affreux_Jojo-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n",
      "Nom : 2.Table & Partage, Lien : /Restaurant_Review-g187265-d18626103-Reviews-Table_Partage-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n",
      "Nom : 3.Frazarin Bistrot Franco Italien, Lien : /Restaurant_Review-g187265-d23110895-Reviews-Frazarin-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n",
      "Nom : 4.La Bouteillerie, Lien : /Restaurant_Review-g187265-d2027277-Reviews-La_Bouteillerie-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n",
      "Nom : 5.Empanadas Club, Lien : /Restaurant_Review-g187265-d19896976-Reviews-Empanadas_Club-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Le HTML à analyser\n",
    "html_content = '''\n",
    "<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d28100759-Reviews-Bikube_Lyon_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">Bikube Lyon - Restaurant</a></span></div>\n",
    "<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d16216238-Reviews-Angelo_Italian_Restaurant-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">Angelo | Italian Restaurant</a></span></div>\n",
    "<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d15114321-Reviews-L_affreux_Jojo-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">1<!-- -->. <!-- -->L'affreux Jojo</a></span></div>\n",
    "<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d18626103-Reviews-Table_Partage-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">2<!-- -->. <!-- -->Table &amp; Partage</a></span></div>\n",
    "<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d23110895-Reviews-Frazarin-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">3<!-- -->. <!-- -->Frazarin Bistrot Franco Italien</a></span></div>\n",
    "<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d2027277-Reviews-La_Bouteillerie-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">4<!-- -->. <!-- -->La Bouteillerie</a></span></div>\n",
    "<div class=\"RfBGI\"><span><a class=\"Lwqic Cj b\" href=\"/Restaurant_Review-g187265-d19896976-Reviews-Empanadas_Club-Lyon_Rhone_Auvergne_Rhone_Alpes.html\" target=\"_blank\">5<!-- -->. <!-- -->Empanadas Club</a></span></div>\n",
    "'''\n",
    "\n",
    "# Création de l'objet BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extraction des liens et des noms\n",
    "restaurants = []\n",
    "for a_tag in soup.find_all('a', class_='Lwqic Cj b'):\n",
    "    href = a_tag['href']\n",
    "    # Supprime les éventuels commentaires ou éléments inutiles dans le texte\n",
    "    name = ''.join(a_tag.stripped_strings)\n",
    "    restaurants.append({'name': name, 'href': href})\n",
    "\n",
    "# Affichage des résultats\n",
    "for restaurant in restaurants:\n",
    "    print(f\"Nom : {restaurant['name']}, Lien : {restaurant['href']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
