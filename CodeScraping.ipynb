{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import undetected_chromedriver as uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les nombres et calculer les pages\n",
    "def extraire_infos(texte):\n",
    "    \"\"\"\n",
    "    Extrait le nombre de commentaires par page, le nombre total de commentaires,\n",
    "    et calcule le nombre de pages à partir d'un texte donné.\n",
    "    \"\"\"\n",
    "    # Nettoyer le texte pour supprimer les espaces insécables\n",
    "    texte = texte.replace(\"\\u202f\", \"\")  # Remplace les espaces insécables par rien\n",
    "\n",
    "    # Extraire les chiffres du texte\n",
    "    chiffres = [int(s) for s in re.findall(r'\\d+', texte)]\n",
    "    \n",
    "    if len(chiffres) >= 2:\n",
    "        nb_commentaires_par_page = chiffres[1]  # Exemple : \"15\" (2e chiffre)\n",
    "        nb_total_commentaires = chiffres[-1]   # Exemple : \"1300\" (dernier chiffre)\n",
    "        nb_pages = math.ceil(nb_total_commentaires / nb_commentaires_par_page)\n",
    "        return nb_commentaires_par_page, nb_total_commentaires, nb_pages\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def scraper_infos_restaurant(driver):\n",
    "    \"\"\"\n",
    "    Scrape les informations globales sur le restaurant.\n",
    "    \"\"\"\n",
    "    nom = driver.find_element(By.XPATH, \"//h1[@class='biGQs _P egaXP rRtyp']\").text  \n",
    "    adresse = driver.find_element(By.XPATH, \"//div[contains(text(), 'Emplacement et coordonnées')]/following::span[contains(@class, 'biGQs _P pZUbB hmDzD')][1]\").text \n",
    "    classement_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'biGQs _P pZUbB hmDzD')]//b/span\").text.strip()\n",
    "    classement = (re.search(r'\\d+', classement_element).group())\n",
    "    fourchette_prix = driver.find_element(By.XPATH, \"//div[contains(@class, 'biGQs _P pZUbB alXOW oCpZu GzNcM nvOhm UTQMg ZTpaU W hmDzD') and contains(text(), '€')]\").text.strip().replace(\"€\", \"\").replace(\"\\xa0\", \"\")\n",
    "    cuisine_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'biGQs _P pZUbB alXOW oCpZu GzNcM nvOhm UTQMg ZTpaU W hmDzD') and not(contains(text(), '€'))]\")\n",
    "    type_cuisine = [c.strip() for c in cuisine_element.text.split(',')]\n",
    "    return {\"nom\": nom, \"adresse\": adresse, \"classement\": classement, \"fourchette_prix\": fourchette_prix, \"type_cuisine\": type_cuisine}\n",
    "\n",
    "# Fonction pour scraper les avis d'une page\n",
    "def scraper_page(driver):\n",
    "    \"\"\"\n",
    "    Récupère les avis d'une seule page.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    # Récupération des éléments sur la page\n",
    "    pseudos = driver.find_elements(By.XPATH, \"//span[@class='biGQs _P fiohW fOtGX']\")\n",
    "    titres = driver.find_elements(By.XPATH, \"//div[@class='biGQs _P fiohW qWPrE ncFvv fOtGX']\")\n",
    "    etoiles = driver.find_elements(By.XPATH, \"//div[@class='OSBmi J k']\")\n",
    "    nb_etoiles = [re.search(r'(\\d+),', etoile.get_attribute(\"textContent\")).group(1) for etoile in etoiles]\n",
    "    dates = [driver.execute_script(\"return arguments[0].childNodes[0].textContent;\", elem).strip() for elem in driver.find_elements(By.XPATH, \"//div[@class='aVuQn']\")]\n",
    "    experiences = driver.find_elements(By.XPATH, \"//span[@class='DlAxN']\")\n",
    "    reviews = driver.find_elements(By.XPATH, \"//div[@data-test-target='review-body']//span[@class='JguWG' and not(ancestor::div[contains(@class, 'csNQI')])]\")\n",
    "\n",
    "    for i in range(len(titres)):\n",
    "        avis = {\n",
    "            \"pseudo\": pseudos[i].text if i < len(pseudos) else \"\",\n",
    "            \"titre_review\": titres[i].text if i < len(titres) else \"\",\n",
    "            \"nb_etoiles\": nb_etoiles[i] if i < len(nb_etoiles) else \"\",\n",
    "            \"date\": dates[i] if i < len(dates) else \"\",\n",
    "            \"experience\": experiences[i].text if i < len(experiences) else \"\",\n",
    "            \"review\": reviews[i].text if i < len(reviews) else \"\"\n",
    "        }\n",
    "        data.append(avis)\n",
    "    return data\n",
    "\n",
    "# Fonction pour scraper les avis de toutes les pages\n",
    "def scraper_toutes_pages(driver, nb_pages):\n",
    "    \"\"\"\n",
    "    Scrape les avis de toutes les pages en utilisant la fonction `scraper_page`.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    actions = ActionChains(driver)\n",
    "     \n",
    "    for page in range(1, nb_pages + 1):\n",
    "        print(f\"Scraping de la page {page}...\")\n",
    "        time.sleep(5) \n",
    "        try:\n",
    "            # Recharger les avis dynamiquement pour chaque page\n",
    "            data = scraper_page(driver)\n",
    "            print(f\"Données collectées pour la page {page} : {len(data)} avis\")\n",
    "            all_data.extend(data)\n",
    "\n",
    "            # Navigation vers la page suivante\n",
    "            next_button = WebDriverWait(driver, 50).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//a[@aria-label='Page suivante']\"))\n",
    "            )\n",
    "\n",
    "            # Scroll et clic\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_button)\n",
    "            time.sleep(5)\n",
    "            actions.move_to_element(next_button).click().perform()\n",
    "\n",
    "\n",
    "            print(\"Page suivante chargée.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur rencontrée à la page {page} : {e}\")\n",
    "            break  # Arrêter la boucle, mais conserver les données collectées jusqu'ici\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def test_scraping(driver, nbPages_texte):\n",
    "    \"\"\"\n",
    "    Teste l'ensemble du processus de scraping :\n",
    "    - Extraction d'informations globales sur le restaurant\n",
    "    - Scraping des avis sur toutes les pages\n",
    "    - Regroupement des données\n",
    "    \"\"\"\n",
    "    avis = []  # Initialiser pour éviter les erreurs\n",
    "    infos_restaurant = {\n",
    "        \"nom\": \"Non disponible\",\n",
    "        \"adresse\": \"Non disponible\",\n",
    "        \"classement\": \"Non disponible\",\n",
    "        \"fourchette_prix\": \"Non disponible\",\n",
    "        \"type_cuisine\": []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Étape 1 : Extraire les infos globales\n",
    "        infos_restaurant = scraper_infos_restaurant(driver)\n",
    "        print(f\"Nom : {infos_restaurant['nom']}\")\n",
    "        print(f\"Adresse : {infos_restaurant['adresse']}\")\n",
    "        print(f\"Classement : {infos_restaurant['classement']}\")\n",
    "        print(f\"Fourchette de prix : {infos_restaurant['fourchette_prix']}\")\n",
    "        print(f\"Type de cuisine : {infos_restaurant['type_cuisine']}\")\n",
    "\n",
    "        # Étape 2 : Extraire les infos pour les pages d'avis\n",
    "        nb_commentaires_par_page, nb_total_commentaires, nb_pages = extraire_infos(nbPages_texte)\n",
    "        print(f\"Nombre de pages : {nb_pages}\")\n",
    "\n",
    "        # **Estimation du temps total** :\n",
    "        average_time_per_page = 15  # Temps moyen par page en secondes\n",
    "        estimated_total_time = average_time_per_page * nb_pages\n",
    "        # Arrondir en minutes\n",
    "        estimated_total_time_minutes = math.ceil(estimated_total_time / 60)\n",
    "        print(f\"Temps estimé pour terminer le scraping : {estimated_total_time_minutes} minutes.\\n\")\n",
    "\n",
    "        # Étape 3 : Scraper les avis\n",
    "        avis = scraper_toutes_pages(driver, nb_pages)\n",
    "        print(f\"Scraping terminé. Total d'avis collectés : {len(avis)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur générale : {e}\")\n",
    "\n",
    "    # Étape 4 : Regrouper les données, même partielles\n",
    "    restaurant_data = {\n",
    "        \"nom\": infos_restaurant[\"nom\"],\n",
    "        \"adresse\": infos_restaurant[\"adresse\"],\n",
    "        \"classement\": infos_restaurant[\"classement\"],\n",
    "        \"fourchette_prix\": infos_restaurant[\"fourchette_prix\"],\n",
    "        \"type_cuisine\": infos_restaurant[\"type_cuisine\"],\n",
    "        \"avis\": avis  # Liste des avis\n",
    "    }\n",
    "\n",
    "    return restaurant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page ouverte avec un User-Agent réaliste\n"
     ]
    }
   ],
   "source": [
    "# Service pour ChromeDriver\n",
    "# Modifier avec le bon chemin\n",
    "service = Service('C:/Users/Ihnhn/Desktop/M2 SISE/NLP/Projet/chromedriver.exe')\n",
    "# Step 3: Rotate user agents \n",
    "user_agents = [\n",
    "    # Add your list of user agents here\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "  \n",
    "]\n",
    "# Configuration du navigateur\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"start-maximized\")\n",
    "\n",
    "# select random user agent\n",
    "user_agent = random.choice(user_agents)\n",
    "# pass in selected user agent as an argument\n",
    "options.add_argument(f'--user-agent={user_agent}')\n",
    "# Lancement du navigateur\n",
    "driver = uc.Chrome(options=options, service=service)\n",
    "\n",
    "# Ouvrez TripAdvisor\n",
    "#driver.get(\"https://www.tripadvisor.fr/Restaurant_Review-g187265-d3727154-Reviews-Les_Terrasses_de_Lyon-Lyon_Rhone_Auvergne_Rhone_Alpes.html\")\n",
    "#driver.get(\"https://www.tripadvisor.fr/Restaurant_Review-g187265-d23110895-Reviews-Frazarin-Lyon_Rhone_Auvergne_Rhone_Alpes.html\")\n",
    "driver.get(\"https://www.tripadvisor.fr/Restaurant_Review-g187265-d2281210-Reviews-Bouchon_Les_Lyonnais-Lyon_Rhone_Auvergne_Rhone_Alpes.html\")\n",
    "# Exécution de JavaScript pour rendre Selenium indétectable\n",
    "driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "time.sleep(3)\n",
    "click_cookies = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"button[id='onetrust-reject-all-handler']\"))).click()\n",
    "\n",
    "print(\"Page ouverte avec un User-Agent réaliste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbPages_texte = driver.find_element(\"xpath\", \"//div[@class='Ci']\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom : Bouchon Les Lyonnais\n",
      "Adresse : 19 Rue de la Bombarde Angle Rue Tramassac, 69005 Lyon France\n",
      "Classement : 187\n",
      "Fourchette de prix : Italienne • -\n",
      "Type de cuisine : ['Française', 'Européenne']\n",
      "Nombre de pages : 87\n",
      "Temps estimé pour terminer le scraping : 22 minutes.\n",
      "\n",
      "Scraping de la page 1...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[338], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Bouchon_Les_Lyonnais \u001b[38;5;241m=\u001b[39m \u001b[43mtest_scraping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbPages_texte\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[337], line 135\u001b[0m, in \u001b[0;36mtest_scraping\u001b[1;34m(driver, nbPages_texte)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemps estimé pour terminer le scraping : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimated_total_time_minutes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# Étape 3 : Scraper les avis\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     avis \u001b[38;5;241m=\u001b[39m \u001b[43mscraper_toutes_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_pages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping terminé. Total d\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavis collectés : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(avis)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[337], line 72\u001b[0m, in \u001b[0;36mscraper_toutes_pages\u001b[1;34m(driver, nb_pages)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, nb_pages \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScraping de la page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;66;03m# Recharger les avis dynamiquement pour chaque page\u001b[39;00m\n\u001b[0;32m     75\u001b[0m         data \u001b[38;5;241m=\u001b[39m scraper_page(driver)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Bouchon_Les_Lyonnais = test_scraping(driver, nbPages_texte)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
