{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import undetected_chromedriver as uc\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les nombres et calculer les pages\n",
    "def extraire_infos(texte):\n",
    "    \"\"\"\n",
    "    Extrait le nombre de commentaires par page, le nombre total de commentaires,\n",
    "    et calcule le nombre de pages à partir d'un texte donné.\n",
    "    \"\"\"\n",
    "    # Nettoyer le texte pour supprimer les espaces insécables\n",
    "    texte = texte.replace(\"\\u202f\", \"\")  # Remplace les espaces insécables par rien\n",
    "\n",
    "    # Extraire les chiffres du texte\n",
    "    chiffres = [int(s) for s in re.findall(r'\\d+', texte)]\n",
    "    \n",
    "    if len(chiffres) >= 2:\n",
    "        nb_commentaires_par_page = chiffres[1]  # Exemple : \"15\" (2e chiffre)\n",
    "        nb_total_commentaires = chiffres[-1]   # Exemple : \"1300\" (dernier chiffre)\n",
    "        nb_pages = math.ceil(nb_total_commentaires / nb_commentaires_par_page)\n",
    "        return nb_commentaires_par_page, nb_total_commentaires, nb_pages\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "  \n",
    "def scraper_infos_restaurant(driver):\n",
    "    \"\"\"\n",
    "    Scrape les informations globales sur le restaurant.\n",
    "    \"\"\"\n",
    "    nom = driver.find_element(By.XPATH, \"//h1[@class='biGQs _P egaXP rRtyp']\").text \n",
    "    adresse = driver.find_element(By.XPATH, \"//div[contains(text(), 'Emplacement et coordonnées')]/following::span[contains(@class, 'biGQs _P pZUbB hmDzD')][1]\").text \n",
    "    note_globale = re.search(r\"(\\d+,\\d+)\", driver.find_elements(By.XPATH, \"//div[@class='biGQs _P vvmrG']\")[0].text).group(1)\n",
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//span//button[@class='ypcsE _S wSSLS']\"))).click()\n",
    "    horaires = [\n",
    "        f\"{lines[0]} : {' - '.join(lines[1:])}\"\n",
    "        for e in driver.find_elements(\"xpath\", \"//div[@class='VFyGJ Pi']\")\n",
    "        if len(lines := e.text.splitlines()) >= 2\n",
    "    ]\n",
    "\n",
    "    time.sleep(3)\n",
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//span//button[@class='ypcsE _S wSSLS']\"))).click()\n",
    "\n",
    "    # Localiser les éléments des notes\n",
    "    notes = driver.find_elements(By.XPATH, \"//div[@class='khxWm f e Q3']/div/div\")\n",
    "    # Extraire les notes pour chaque catégorie à partir de l'innerHTML\n",
    "    note_cuisine = re.search(r'<title[^>]*>([\\d.,]+) sur [\\d.,]+', notes[1].get_attribute(\"innerHTML\")).group(1)\n",
    "    note_service = re.search(r'<title[^>]*>([\\d.,]+) sur [\\d.,]+', notes[3].get_attribute(\"innerHTML\")).group(1)\n",
    "    note_rapportqualiteprix = re.search(r'<title[^>]*>([\\d.,]+) sur [\\d.,]+', notes[5].get_attribute(\"innerHTML\")).group(1)\n",
    "    note_ambiance = re.search(r'<title[^>]*>([\\d.,]+) sur [\\d.,]+', notes[7].get_attribute(\"innerHTML\")).group(1)\n",
    "    classement = (re.search(r'\d+', driver.find_element(By.XPATH, "//div[contains(@class, 'biGQs _P pZUbB hmDzD')]//b/span").text.replace("\u202f", "").replace(" ", "")).group())\n",
    "\n",
    "    #click pour avoir détails\n",
    "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"button[class='UikNM _G B- _S _W _T c G_ wSSLS ACvVd']\"))).click()\n",
    "    time.sleep(2)\n",
    "    #infos pratiques\n",
    "    try:\n",
    "        infos_pratiques = driver.find_element(By.XPATH, \"//div[contains(@class, 'Wf') and ./div[contains(text(), 'Infos pratiques')]]/following-sibling::div[contains(@class, 'biGQs')]\").text.strip()\n",
    "    except Exception:\n",
    "        infos_pratiques = \"Non renseigné\"\n",
    "\n",
    "    #fourchette de prix\n",
    "    try:\n",
    "        fourchette_prix = driver.find_element(By.XPATH, \"//div[contains(@class, 'Wf') and ./div[contains(text(), 'FOURCHETTE DE PRIX')]]/following-sibling::div[contains(@class, 'biGQs _P pZUbB alXOW oCpZu GzNcM nvOhm UTQMg ZTpaU W hmDzD')]\").text.strip().replace(\"€\", \"\").replace(\"\\xa0\", \"\")\n",
    "    except Exception:\n",
    "        fourchette_prix = \"Non renseigné\"  # Valeur par défaut\n",
    "    #type cuisines\n",
    "    try:\n",
    "        types_cuisines = [item.strip() for item in driver.find_element(By.XPATH, \"//div[contains(@class, 'Wf') and ./div[contains(text(), 'CUISINES')]]/following-sibling::div[contains(@class, 'biGQs _P pZUbB alXOW oCpZu GzNcM nvOhm UTQMg ZTpaU W hmDzD')]\").text.strip().split(\",\")]\n",
    "    except Exception:\n",
    "        types_cuisines = \"Non renseigné\"\n",
    "    #regimes\n",
    "    try:\n",
    "        regimes = [item.strip() for item in driver.find_element(By.XPATH, \"//div[contains(@class, 'Wf') and ./div[contains(text(), 'Régimes spéciaux')]]/following-sibling::div[contains(@class, 'biGQs _P pZUbB alXOW oCpZu GzNcM nvOhm UTQMg ZTpaU W hmDzD')]\").text.strip().split(\",\")]\n",
    "    except Exception:\n",
    "        regimes = \"Non renseigné\"\n",
    "    #repas\n",
    "    try:\n",
    "        repas = [item.strip() for item in driver.find_element(By.XPATH, \"//div[contains(@class, 'Wf') and ./div[contains(text(), 'Repas')]]/following-sibling::div[contains(@class, 'biGQs _P pZUbB alXOW eWlDX GzNcM ATzgx UTQMg TwpTY hmDzD')]\").text.strip().split(\",\")]\n",
    "    except Exception:\n",
    "        repas = \"Non renseigné\"\n",
    "    #fonctionnalités\n",
    "    try:\n",
    "        fonctionnalites = [item.strip() for item in driver.find_element(By.XPATH, \"//div[contains(@class, 'Wf') and ./div[contains(text(), 'FONCTIONNALITÉS')]]/following-sibling::div[contains(@class, 'biGQs')]\").text.strip().split(\",\")]\n",
    "    except Exception:\n",
    "        fonctionnalites = \"Non renseigné\"\n",
    "    \n",
    "    time.sleep(5)\n",
    "    driver.find_element(By.XPATH, \"//button[@aria-label='Fermer']\").click()\n",
    "    time.sleep(2)\n",
    "\n",
    "    #quartier = driver.find_elements(By.XPATH,\"//div[@class='akmhy e j']//a[@class='BMQDV _F Gv wSSLS SwZTJ FGwzt ukgoS']/span[contains(@class, 'biGQs _P pZUbB hmDzD')]\")\n",
    "    #quartier = quartier[1].text\n",
    "    \n",
    "    try:\n",
    "    # Récupérer l'élément `<a>` contenant le lien Google Maps\n",
    "        google_maps_link = driver.find_element(By.XPATH,\"//div[@class='akmhy e j']//a[@class='BMQDV _F Gv wSSLS SwZTJ FGwzt ukgoS']\").get_attribute(\"href\")\n",
    "        # Extraire les coordonnées géographiques du lien\n",
    "        if \"@\" in google_maps_link:\n",
    "            coordinates = google_maps_link.split(\"@\")[1].split(\",\")[:2]  # Prendre latitude et longitude\n",
    "            latitude, longitude = coordinates[0], coordinates[1]\n",
    "            #print(f\"Latitude : {latitude}, Longitude : {longitude}\")\n",
    "        else:\n",
    "            latitude, longitude = \"Non renseigné\", \"Non renseigné\"\n",
    "            #print(\"Coordonnées introuvables dans le lien.\")\n",
    "    except NoSuchElementException:\n",
    "        latitude, longitude = \"Non renseigné\", \"Non renseigné\"\n",
    "        #print(\"Lien Google Maps introuvable.\")\n",
    "\n",
    "    nb_avis = (driver.find_element(By.XPATH, \"//span[@class='GPKsO']\").get_attribute(\"innerText\").replace(\"\\u202f\", \"\").split()[0])\n",
    "\n",
    "    avis_list = [\n",
    "        cat.text + \" : \" + nb.text.replace(\" \", \"\").replace(\"\\u202f\", \"\")\n",
    "        for cat, nb in zip(\n",
    "            driver.find_elements(By.XPATH, \"//div[@class='biGQs _P fiohW hmDzD']/div[@class='Ygqck o W q']\"),\n",
    "            driver.find_elements(By.XPATH, \"//div[@class='biGQs _P fiohW biKBZ osNWb']\")\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"nom\": nom, \n",
    "        \"adresse\": adresse, \n",
    "        \"classement\": classement, \n",
    "        \"horaires\": horaires,\n",
    "        \"note_globale\": note_globale, \n",
    "        \"note_cuisine\":note_cuisine, \n",
    "        \"note_service\":note_service, \n",
    "        \"note_rapportqualiteprix\":note_rapportqualiteprix, \n",
    "        \"note_ambiance\":note_ambiance,\n",
    "        \"infos_pratiques\":infos_pratiques,\n",
    "        \"repas\":repas, \n",
    "        \"regimes\": regimes,\n",
    "        \"fonctionnalites\":fonctionnalites,\n",
    "        \"fourchette_prix\": fourchette_prix, \n",
    "        \"types_cuisines\": types_cuisines, \n",
    "        \"latitude\": latitude, \n",
    "        \"longitude\": longitude,\n",
    "        \"nb_avis\": nb_avis,\n",
    "        \"avis_list\": avis_list\n",
    "        }\n",
    "\n",
    "# Fonction pour scraper les avis d'une page\n",
    "def scraper_page(driver):\n",
    "    \"\"\"\n",
    "    Récupère les avis d'une seule page.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    # Récupération des éléments sur la page\n",
    "    pseudos = driver.find_elements(By.XPATH, \"//span[@class='biGQs _P fiohW fOtGX']\") \n",
    "    titres = driver.find_elements(By.XPATH, \"//div[@class='biGQs _P fiohW qWPrE ncFvv fOtGX']\")\n",
    "    etoiles = driver.find_elements(By.XPATH, \"//div[@class='OSBmi J k']\")\n",
    "    nb_etoiles = [re.search(r'(\\d+),', etoile.get_attribute(\"textContent\")).group(1) for etoile in etoiles]\n",
    "    dates = [re.search(r\"\\d{1,2}\\s\\w+\\s\\d{4}\", elem.text.strip()).group(0) for elem in driver.find_elements(By.XPATH, \"//div[contains(@class, 'biGQs _P pZUbB ncFvv osNWb')]\")]\n",
    "    experiences = driver.find_elements(By.XPATH, \"//span[@class='DlAxN']\")\n",
    "    reviews = driver.find_elements(By.XPATH, \"//div[@data-test-target='review-body']//span[@class='JguWG' and not(ancestor::div[contains(@class, 'csNQI')])]\")\n",
    "\n",
    "\n",
    "    for i in range(len(titres)):\n",
    "        avis = {\n",
    "            \"pseudo\": pseudos[i].text if i < len(pseudos) else \"\",\n",
    "            \"titre_review\": titres[i].text if i < len(titres) else \"\",\n",
    "            \"nb_etoiles\": nb_etoiles[i] if i < len(nb_etoiles) else \"\",\n",
    "            \"date\": dates[i] if i < len(dates) else \"\",\n",
    "            \"experience\": experiences[i].text if i < len(experiences) else \"\",\n",
    "            \"review\": reviews[i].text if i < len(reviews) else \"\"\n",
    "        }\n",
    "        data.append(avis)\n",
    "    return data\n",
    "\n",
    "# Fonction pour scraper les avis de toutes les pages\n",
    "def scraper_toutes_pages(driver, nb_pages):\n",
    "    \"\"\"\n",
    "    Scrape les avis de toutes les pages en utilisant la fonction `scraper_page`.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    actions = ActionChains(driver)\n",
    "     \n",
    "    for page in range(1, nb_pages + 1):\n",
    "        print(f\"Scraping de la page {page}...\")\n",
    "        time.sleep(5) \n",
    "        try:\n",
    "            # Recharger les avis dynamiquement pour chaque page\n",
    "            data = scraper_page(driver)\n",
    "            print(f\"Données collectées pour la page {page} : {len(data)} avis\")\n",
    "            all_data.extend(data)\n",
    "\n",
    "            # Navigation vers la page suivante\n",
    "            next_button = WebDriverWait(driver, 50).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//a[@aria-label='Page suivante']\"))\n",
    "            )\n",
    "\n",
    "            # Scroll et clic\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_button)\n",
    "            time.sleep(5)\n",
    "            actions.move_to_element(next_button).click().perform()\n",
    "\n",
    "\n",
    "            print(\"Page suivante chargée.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur rencontrée à la page {page} : {e}\")\n",
    "            break  # Arrêter la boucle, mais conserver les données collectées jusqu'ici\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def test_scraping(driver, nbPages_texte):\n",
    "    \"\"\"\n",
    "    Teste l'ensemble du processus de scraping :\n",
    "    - Extraction d'informations globales sur le restaurant\n",
    "    - Scraping des avis sur toutes les pages\n",
    "    - Regroupement des données\n",
    "    \"\"\"\n",
    "    avis = []  # Init\n",
    "    infos_restaurant = {\n",
    "        \"nom\": \"Non disponible\",\n",
    "        \"adresse\": \"Non disponible\",\n",
    "        \"classement\": \"Non disponible\",\n",
    "        \"horaires\": [],\n",
    "        \"note_globale\": \"Non disponible\",\n",
    "        \"note_cuisine\": \"Non disponible\",\n",
    "        \"note_service\": \"Non disponible\",\n",
    "        \"note_rapportqualiteprix\": \"Non disponible\",\n",
    "        \"note_ambiance\": \"Non disponible\",\n",
    "        \"repas\": [],\n",
    "        \"infos_pratiques\": \"Non disponible\",\n",
    "        \"regimes\": [],\n",
    "        \"fonctionnalites\": \"Non disponible\",\n",
    "        \"fourchette_prix\": \"Non disponible\",\n",
    "        \"types_cuisines\": [],\n",
    "        \"latitude\" : \"Non disponible\",\n",
    "        \"longitude\" : \"Non disponible\",\n",
    "        \"nb_avis\": \"Non disponible\",\n",
    "        \"avis_list\": []\n",
    "\n",
    "    }\n",
    "    try:\n",
    "        # Étape 1 : Extraire les infos globales\n",
    "        infos_restaurant = scraper_infos_restaurant(driver)\n",
    "        # print(f\"Nom : {infos_restaurant['nom']}\")\n",
    "        # print(f\"Adresse : {infos_restaurant['adresse']}\")\n",
    "        # print(f\"Classement : {infos_restaurant['classement']}\")\n",
    "        # print(f\"Horaires : {infos_restaurant['horaires']}\")\n",
    "        # print(f\"Note globale : {infos_restaurant['note_globale']}\")\n",
    "        # print(f\"Note cuisine : {infos_restaurant['note_cuisine']}\")\n",
    "        # print(f\"Note service : {infos_restaurant['note_service']}\")\n",
    "        # print(f\"Note rapport qualité prix : {infos_restaurant['note_rapportqualiteprix']}\")\n",
    "        # print(f\"Note ambiance : {infos_restaurant['note_ambiance']}\")\n",
    "        # print(f\"Infos pratiques : {infos_restaurant['infos_pratiques']}\")\n",
    "        # print(f\"Repas : {infos_restaurant['repas']}\")\n",
    "        # print(f\"Régimes : {infos_restaurant['regimes']}\")\n",
    "        # print(f\"Fourchette de prix : {infos_restaurant['fourchette_prix']}\")\n",
    "        # print(f\"Fonctionnalités : {infos_restaurant['fonctionnalites']}\")\n",
    "        # print(f\"Type de cuisine : {infos_restaurant['types_cuisines']}\")\n",
    "        # print(f\"Latitude : {infos_restaurant['latitude']}\")\n",
    "        # print(f\"Longitude : {infos_restaurant['longitude']}\")\n",
    "        # print(f\"NbAvis : {infos_restaurant['nb_avis']}\")\n",
    "        # print(f\"Liste avis : {infos_restaurant['avis_list']}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Étape 2 : Extraire les infos pour les pages d'avis\n",
    "        nb_commentaires_par_page, nb_total_commentaires, nb_pages = extraire_infos(nbPages_texte)\n",
    "        print(f\"Nombre de pages : {nb_pages}\")\n",
    "\n",
    "        # **Estimation du temps total** :\n",
    "        average_time_per_page = 15  # Temps moyen par page en secondes\n",
    "        estimated_total_time = average_time_per_page * nb_pages\n",
    "        # Arrondir en minutes\n",
    "        estimated_total_time_minutes = math.ceil(estimated_total_time / 60)\n",
    "        print(f\"Temps estimé pour terminer le scraping : {estimated_total_time_minutes} minutes.\\n\")\n",
    "\n",
    "        # Étape 3 : Scraper les avis\n",
    "        avis = scraper_toutes_pages(driver, nb_pages)\n",
    "        print(f\"Scraping terminé. Total d'avis collectés : {len(avis)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur générale : {e}\")\n",
    "\n",
    "    # Étape 4 : Regrouper les données, même partielles\n",
    "    restaurant_data = {\n",
    "        \"nom\": infos_restaurant[\"nom\"],\n",
    "        \"adresse\": infos_restaurant[\"adresse\"],\n",
    "        \"classement\": infos_restaurant[\"classement\"],\n",
    "        \"horaires\": infos_restaurant[\"horaires\"],\n",
    "        \"note_globale\": infos_restaurant[\"note_globale\"],\n",
    "        \"note_cuisine\": infos_restaurant[\"note_cuisine\"],\n",
    "        \"note_service\": infos_restaurant[\"note_service\"],\n",
    "        \"note_rapportqualiteprix\": infos_restaurant[\"note_rapportqualiteprix\"],\n",
    "        \"note_ambiance\": infos_restaurant[\"note_ambiance\"],\n",
    "        \"infos_pratiques\": infos_restaurant[\"infos_pratiques\"],\n",
    "        \"repas\": infos_restaurant[\"repas\"],\n",
    "        \"regimes\": infos_restaurant[\"regimes\"],\n",
    "        \"fourchette_prix\": infos_restaurant[\"fourchette_prix\"],\n",
    "        \"fonctionnalités\": infos_restaurant[\"fonctionnalites\"],\n",
    "        \"type_cuisines\": infos_restaurant[\"types_cuisines\"],\n",
    "        \"latitude\": infos_restaurant[\"latitude\"],\n",
    "        \"longitude\": infos_restaurant[\"longitude\"],\n",
    "        \"nb_avis\": infos_restaurant[\"nb_avis\"],\n",
    "        \"avis_list\": infos_restaurant[\"avis_list\"],\n",
    "        \"avis\": avis  # Liste des avis\n",
    "\n",
    "        \n",
    "    }\n",
    "\n",
    "    return restaurant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour configurer le driver\n",
    "def create_driver():\n",
    "    #changer par le bon chemin\n",
    "    service = Service('C:/Users/Ihnhn/Desktop/M2 SISE/NLP/Projet/chromedriver.exe')\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    ]\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(\"--incognito\")\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    user_agent = random.choice(user_agents)\n",
    "    options.add_argument(f'--user-agent={user_agent}')\n",
    "    return uc.Chrome(options=options, service=service)\n",
    "\n",
    "# URL à visiter\n",
    "#changer avec l'URL du restaurant à scraper\n",
    "url = \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d695217-Reviews-Brasserie_Georges-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"\n",
    "\n",
    "# Boucle pour redémarrer le navigateur jusqu'à ce que l'élément soit trouvé\n",
    "found = False\n",
    "attempts = 0\n",
    "max_attempts = 20\n",
    "\n",
    "while not found and attempts < max_attempts:\n",
    "    driver = create_driver()  # Créer un nouveau navigateur\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Rendre Selenium indétectable\n",
    "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "        time.sleep(3)\n",
    "        # Accepter les cookies\n",
    "        try:\n",
    "            WebDriverWait(driver, 30).until(\n",
    "                EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[id='onetrust-accept-btn-handler']\"))\n",
    "            ).click()\n",
    "        except TimeoutException:\n",
    "            print(\"Pas de bannière cookies trouvée.\")\n",
    "\n",
    "        # Chercher le nom\n",
    "        nom = driver.find_element(By.XPATH, \"//h1[@class='biGQs _P egaXP rRtyp']\")\n",
    "        print(f\"Nom trouvé : {nom.text}\")\n",
    "        found = True  # Si le nom est trouvé, sortir de la boucle\n",
    "    except NoSuchElementException:\n",
    "        print(f\"Nom non trouvé, tentative {attempts + 1}/{max_attempts}. Redémarrage...\")\n",
    "        attempts += 1\n",
    "        driver.quit()  # Fermer le navigateur avant de recommencer\n",
    "        time.sleep(10)  # Attendre avant de redémarrer un nouveau navigateur\n",
    "if not found:\n",
    "    print(\"Échec : le nom n'a pas été trouvé après plusieurs tentatives.\")\n",
    "    time.sleep(2)\n",
    "    driver.quit()  # Fermer le dernier navigateur si le nom n'est pas trouvé\n",
    "else:\n",
    "    print(\"Le nom a été trouvé avec succès. Le navigateur reste ouvert.\")\n",
    "# Le driver reste ouvert si le nom a été trouvé\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code pour executer le scraping\n",
    "#On recupère dabord le nb de pages à scraper et on appelle la fonction de scraping puis ferme le driver\n",
    "nbPages_texte = driver.find_element(\"xpath\", \"//div[@class='Ci']\").text\n",
    "data = test_scraping(driver, nbPages_texte)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Exporter les données en JSON\n",
    "with open(\"nom_restau.json\", \"w\", encoding=\"utf-8\") as f: json.dump(data, f, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
