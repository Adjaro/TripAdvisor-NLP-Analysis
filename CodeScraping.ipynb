{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import re\n",
    "import random\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "import undetected_chromedriver as uc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour extraire les nombres et calculer les pages\n",
    "def extraire_infos(texte):\n",
    "    \"\"\"\n",
    "    Extrait le nombre de commentaires par page, le nombre total de commentaires,\n",
    "    et calcule le nombre de pages à partir d'un texte donné.\n",
    "    \"\"\"\n",
    "    # Nettoyer le texte pour supprimer les espaces insécables\n",
    "    texte = texte.replace(\"\\u202f\", \"\")  # Remplace les espaces insécables par rien\n",
    "\n",
    "    # Extraire les chiffres du texte\n",
    "    chiffres = [int(s) for s in re.findall(r'\\d+', texte)]\n",
    "    \n",
    "    if len(chiffres) >= 2:\n",
    "        nb_commentaires_par_page = chiffres[1]  # Exemple : \"15\" (2e chiffre)\n",
    "        nb_total_commentaires = chiffres[-1]   # Exemple : \"1300\" (dernier chiffre)\n",
    "        nb_pages = math.ceil(nb_total_commentaires / nb_commentaires_par_page)\n",
    "        return nb_commentaires_par_page, nb_total_commentaires, nb_pages\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "def scraper_infos_restaurant(driver):\n",
    "    \"\"\"\n",
    "    Scrape les informations globales sur le restaurant.\n",
    "    \"\"\"\n",
    "    nom = driver.find_element(By.XPATH, \"//h1[@class='biGQs _P egaXP rRtyp']\").text  \n",
    "    adresse = driver.find_element(By.XPATH, \"//div[contains(text(), 'Emplacement et coordonnées')]/following::span[contains(@class, 'biGQs _P pZUbB hmDzD')][1]\").text \n",
    "    classement_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'biGQs _P pZUbB hmDzD')]//b/span\").text.strip()\n",
    "    classement = (re.search(r'\\d+', classement_element).group())\n",
    "    fourchette_prix = driver.find_element(By.XPATH, \"//div[contains(@class, 'biGQs _P pZUbB alXOW oCpZu GzNcM nvOhm UTQMg ZTpaU W hmDzD') and contains(text(), '€')]\").text.strip().replace(\"€\", \"\").replace(\"\\xa0\", \"\")\n",
    "    cuisine_element = driver.find_element(By.XPATH, \"//div[contains(@class, 'biGQs _P pZUbB alXOW oCpZu GzNcM nvOhm UTQMg ZTpaU W hmDzD') and not(contains(text(), '€'))]\")\n",
    "    type_cuisine = [c.strip() for c in cuisine_element.text.split(',')]\n",
    "    return {\"nom\": nom, \"adresse\": adresse, \"classement\": classement, \"fourchette_prix\": fourchette_prix, \"type_cuisine\": type_cuisine}\n",
    "\n",
    "# Fonction pour scraper les avis d'une page\n",
    "def scraper_page(driver):\n",
    "    \"\"\"\n",
    "    Récupère les avis d'une seule page.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    # Récupération des éléments sur la page\n",
    "    pseudos = driver.find_elements(By.XPATH, \"//span[@class='biGQs _P fiohW fOtGX']\")\n",
    "    titres = driver.find_elements(By.XPATH, \"//div[@class='biGQs _P fiohW qWPrE ncFvv fOtGX']\")\n",
    "    etoiles = driver.find_elements(By.XPATH, \"//div[@class='OSBmi J k']\")\n",
    "    nb_etoiles = [re.search(r'(\\d+),', etoile.get_attribute(\"textContent\")).group(1) for etoile in etoiles]\n",
    "    dates = [driver.execute_script(\"return arguments[0].childNodes[0].textContent;\", elem).strip() for elem in driver.find_elements(By.XPATH, \"//div[@class='aVuQn']\")]\n",
    "    experiences = driver.find_elements(By.XPATH, \"//span[@class='DlAxN']\")\n",
    "    reviews = driver.find_elements(By.XPATH, \"//div[@data-test-target='review-body']//span[@class='JguWG' and not(ancestor::div[contains(@class, 'csNQI')])]\")\n",
    "\n",
    "    for i in range(len(titres)):\n",
    "        avis = {\n",
    "            \"pseudo\": pseudos[i].text if i < len(pseudos) else \"\",\n",
    "            \"titre_review\": titres[i].text if i < len(titres) else \"\",\n",
    "            \"nb_etoiles\": nb_etoiles[i] if i < len(nb_etoiles) else \"\",\n",
    "            \"date\": dates[i] if i < len(dates) else \"\",\n",
    "            \"experience\": experiences[i].text if i < len(experiences) else \"\",\n",
    "            \"review\": reviews[i].text if i < len(reviews) else \"\"\n",
    "        }\n",
    "        data.append(avis)\n",
    "    return data\n",
    "\n",
    "# Fonction pour scraper les avis de toutes les pages\n",
    "def scraper_toutes_pages(driver, nb_pages):\n",
    "    \"\"\"\n",
    "    Scrape les avis de toutes les pages en utilisant la fonction `scraper_page`.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    actions = ActionChains(driver)\n",
    "     \n",
    "    for page in range(1, nb_pages + 1):\n",
    "        print(f\"Scraping de la page {page}...\")\n",
    "        time.sleep(5) \n",
    "        try:\n",
    "            # Recharger les avis dynamiquement pour chaque page\n",
    "            data = scraper_page(driver)\n",
    "            print(f\"Données collectées pour la page {page} : {len(data)} avis\")\n",
    "            all_data.extend(data)\n",
    "\n",
    "            # Navigation vers la page suivante\n",
    "            next_button = WebDriverWait(driver, 50).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//a[@aria-label='Page suivante']\"))\n",
    "            )\n",
    "\n",
    "            # Scroll et clic\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_button)\n",
    "            time.sleep(5)\n",
    "            actions.move_to_element(next_button).click().perform()\n",
    "\n",
    "\n",
    "            print(\"Page suivante chargée.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur rencontrée à la page {page} : {e}\")\n",
    "            break  # Arrêter la boucle, mais conserver les données collectées jusqu'ici\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def test_scraping(driver, nbPages_texte):\n",
    "    \"\"\"\n",
    "    Teste l'ensemble du processus de scraping :\n",
    "    - Extraction d'informations globales sur le restaurant\n",
    "    - Scraping des avis sur toutes les pages\n",
    "    - Regroupement des données\n",
    "    \"\"\"\n",
    "    avis = []  # Initialiser pour éviter les erreurs\n",
    "    infos_restaurant = {\n",
    "        \"nom\": \"Non disponible\",\n",
    "        \"adresse\": \"Non disponible\",\n",
    "        \"classement\": \"Non disponible\",\n",
    "        \"fourchette_prix\": \"Non disponible\",\n",
    "        \"type_cuisine\": []\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Étape 1 : Extraire les infos globales\n",
    "        infos_restaurant = scraper_infos_restaurant(driver)\n",
    "        print(f\"Nom : {infos_restaurant['nom']}\")\n",
    "        print(f\"Adresse : {infos_restaurant['adresse']}\")\n",
    "        print(f\"Classement : {infos_restaurant['classement']}\")\n",
    "        print(f\"Fourchette de prix : {infos_restaurant['fourchette_prix']}\")\n",
    "        print(f\"Type de cuisine : {infos_restaurant['type_cuisine']}\")\n",
    "\n",
    "        # Étape 2 : Extraire les infos pour les pages d'avis\n",
    "        nb_commentaires_par_page, nb_total_commentaires, nb_pages = extraire_infos(nbPages_texte)\n",
    "        print(f\"Nombre de pages : {nb_pages}\")\n",
    "\n",
    "        # **Estimation du temps total** :\n",
    "        average_time_per_page = 15  # Temps moyen par page en secondes\n",
    "        estimated_total_time = average_time_per_page * nb_pages\n",
    "        # Arrondir en minutes\n",
    "        estimated_total_time_minutes = math.ceil(estimated_total_time / 60)\n",
    "        print(f\"Temps estimé pour terminer le scraping : {estimated_total_time_minutes} minutes.\\n\")\n",
    "\n",
    "        # Étape 3 : Scraper les avis\n",
    "        avis = scraper_toutes_pages(driver, nb_pages)\n",
    "        print(f\"Scraping terminé. Total d'avis collectés : {len(avis)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur générale : {e}\")\n",
    "\n",
    "    # Étape 4 : Regrouper les données, même partielles\n",
    "    restaurant_data = {\n",
    "        \"nom\": infos_restaurant[\"nom\"],\n",
    "        \"adresse\": infos_restaurant[\"adresse\"],\n",
    "        \"classement\": infos_restaurant[\"classement\"],\n",
    "        \"fourchette_prix\": infos_restaurant[\"fourchette_prix\"],\n",
    "        \"type_cuisine\": infos_restaurant[\"type_cuisine\"],\n",
    "        \"avis\": avis  # Liste des avis\n",
    "    }\n",
    "\n",
    "    return restaurant_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page ouverte avec un User-Agent réaliste\n"
     ]
    }
   ],
   "source": [
    "# Service pour ChromeDriver\n",
    "# Modifier avec le bon chemin\n",
    "service = Service('C:/Users/Ihnhn/Desktop/M2 SISE/NLP/Projet/chromedriver.exe')\n",
    "# Step 3: Rotate user agents \n",
    "user_agents = [\n",
    "    # Add your list of user agents here\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "  \n",
    "]\n",
    "# Configuration du navigateur\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"start-maximized\")\n",
    "\n",
    "# select random user agent\n",
    "user_agent = random.choice(user_agents)\n",
    "# pass in selected user agent as an argument\n",
    "options.add_argument(f'--user-agent={user_agent}')\n",
    "# Lancement du navigateur\n",
    "driver = uc.Chrome(options=options, service=service)\n",
    "\n",
    "# Ouvrez TripAdvisor\n",
    "#driver.get(\"https://www.tripadvisor.fr/Restaurant_Review-g187265-d3727154-Reviews-Les_Terrasses_de_Lyon-Lyon_Rhone_Auvergne_Rhone_Alpes.html\")\n",
    "driver.get(\"https://www.tripadvisor.fr/Restaurant_Review-g187265-d23110895-Reviews-Frazarin-Lyon_Rhone_Auvergne_Rhone_Alpes.html\")\n",
    "#driver.get(\"https://www.tripadvisor.fr/Restaurant_Review-g187265-d2281210-Reviews-Bouchon_Les_Lyonnais-Lyon_Rhone_Auvergne_Rhone_Alpes.html\")\n",
    "# Exécution de JavaScript pour rendre Selenium indétectable\n",
    "driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "time.sleep(3)\n",
    "click_cookies = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"button[id='onetrust-reject-all-handler']\"))).click()\n",
    "\n",
    "print(\"Page ouverte avec un User-Agent réaliste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbPages_texte = driver.find_element(\"xpath\", \"//div[@class='Ci']\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom : Frazarin Bistrot Franco Italien\n",
      "Adresse : 23 Rue De Condé, 69002 Lyon France\n",
      "Classement : 3\n",
      "Fourchette de prix : 16,00 -38,00 \n",
      "Type de cuisine : ['Italienne', 'Française', 'Saine']\n",
      "Nombre de pages : 16\n",
      "Temps estimé pour terminer le scraping : 4 minutes.\n",
      "\n",
      "Scraping de la page 1...\n",
      "Données collectées pour la page 1 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 2...\n",
      "Données collectées pour la page 2 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 3...\n",
      "Données collectées pour la page 3 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 4...\n",
      "Données collectées pour la page 4 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 5...\n",
      "Données collectées pour la page 5 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 6...\n",
      "Données collectées pour la page 6 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 7...\n",
      "Données collectées pour la page 7 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 8...\n",
      "Données collectées pour la page 8 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 9...\n",
      "Données collectées pour la page 9 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 10...\n",
      "Données collectées pour la page 10 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 11...\n",
      "Données collectées pour la page 11 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 12...\n",
      "Données collectées pour la page 12 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 13...\n",
      "Données collectées pour la page 13 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 14...\n",
      "Données collectées pour la page 14 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 15...\n",
      "Données collectées pour la page 15 : 15 avis\n",
      "Page suivante chargée.\n",
      "Scraping de la page 16...\n",
      "Données collectées pour la page 16 : 12 avis\n",
      "Erreur rencontrée à la page 16 : Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x009C34A3+25059]\n",
      "\t(No symbol) [0x0094CEA4]\n",
      "\t(No symbol) [0x0082BEC3]\n",
      "\t(No symbol) [0x0086FD86]\n",
      "\t(No symbol) [0x0086FFCB]\n",
      "\t(No symbol) [0x008AD952]\n",
      "\t(No symbol) [0x00891F44]\n",
      "\t(No symbol) [0x008AB51E]\n",
      "\t(No symbol) [0x00891C96]\n",
      "\t(No symbol) [0x00863FAC]\n",
      "\t(No symbol) [0x00864F3D]\n",
      "\tGetHandleVerifier [0x00CB5613+3113811]\n",
      "\tGetHandleVerifier [0x00CCA2DA+3199002]\n",
      "\tGetHandleVerifier [0x00CC2AB2+3168242]\n",
      "\tGetHandleVerifier [0x00A63310+680016]\n",
      "\t(No symbol) [0x009557ED]\n",
      "\t(No symbol) [0x00952A98]\n",
      "\t(No symbol) [0x00952C35]\n",
      "\t(No symbol) [0x00945890]\n",
      "\tBaseThreadInitThunk [0x76365D49+25]\n",
      "\tRtlInitializeExceptionChain [0x7713CEBB+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x7713CE41+561]\n",
      "\n",
      "Scraping terminé. Total d'avis collectés : 237\n"
     ]
    }
   ],
   "source": [
    "data = test_scraping(driver, nbPages_texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Exporter les données en JSON en une seule ligne\n",
    "with open(\"restaurant_data.json\", \"w\", encoding=\"utf-8\") as f: json.dump(data, f, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
